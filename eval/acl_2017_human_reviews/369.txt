The paper describes a method for improving two-step translation using deep learning. Results are presented for Chinese->Spanish translation, but the approach seems to be largely language-independent.  The setting is fairly typical for two-step MT. The first step translates into a morphologically underspecified version of the target language. The second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms (using a morphological generator). The main novelty of this work is the choice of deep NNs as classifiers in the second step. The authors also propose a rescoring step which uses a LM to select the best variant.  Overall, this is solid work with good empirical results: the classifier models reach a high accuracy (clearly outperforming baselines such as SVMs) and the improvement is apparent even in the final translation quality.  My main problem with the paper is the lack of a comparison with some straightforward deep-learning baselines. Specifically, you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step. (Unless I misunderstood the approach.) But this is a sequence labeling task which RNNs are well suited for. How would e.g. a bidirectional LSTM network do when trained and used in the standard sequence labeling setting? After reading the author response, I still think that baselines (including the standard LSTM) are run in the same framework, i.e. independently for each local label. If that's not the case, it should have been clarified better in the response. This is a problem because you're not using the RNNs in the standard way and yet you don't justify why your way is better or compare the two approaches.  The final re-scoring step is not entirely clear to me. Do you rescore n-best sentences? What features do you use? Or are you searching a weighted graph for the single optimal path? This needs to be explained more clearly in the paper. (My current impression is that you produce a graph, then look for K best paths in it, generate the inflected sentences from these K paths and *then* use a LM -- and nothing else -- to select the best variant. But I'm not sure from reading the paper.) This was not addressed in the response.  You report that larger word embeddings lead to a longer training time. Do they also influence the final results?  Can you attempt to explain why adding information from the source sentence hurts? This seems a bit counter-intuitive -- does e.g. the number information not get entirely lost sometimes because of this? I would appreciate a more thorough discussion on this in the final version, perhaps with a couple of convincing examples.  The paper contains a number of typos and the general level of English may not be sufficient for presentation at ACL.  Minor corrections:  context of the application of MT -> context of application for MT  In this cases, MT is faced in two-steps -> In this case, MT is divided into two steps  markov -> Markov  CFR -> CRF  task was based on a direct translation -> task was based on direct translation  task provided corpus -> task provided corpora  the phrase-based system has dramatically -> the phrase-based approach...  investigated different set of features -> ...sets of features  words as source of information -> words as the source...  correspondant -> corresponding  Classes for gender classifier -> Classes for the...  for number classifier -> for the...  This layer's input consists in -> ...consists of  to extract most relevant -> ...the most...  Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer would produce (-1, 1).  information of a word consists in itself -> ...of itself  this $A$ set -> the set $A$  empty sentences and longer than 50 words -> empty sentences and sentences longer than...  classifier is trained on -> classifier is trained in  aproximately -> approximately  coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)  in descendant order -> in descending order  cuadratic -> quadratic (in multiple places)  but best results -> but the best results  Rescoring step improves -> The rescoring step...  are not be comparable -> are not comparable