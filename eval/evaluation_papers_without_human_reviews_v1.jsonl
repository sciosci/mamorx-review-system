{"paper_id": "acl_2024_s37", "title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s37.pdf", "human_reviewer": "", "barebones": "Here is a list of major feedback comments for this scientific paper, as a peer reviewer would provide:\\n\\n1. The study addresses an important and timely research question about potential biases in large language models for hiring decisions. The methodology of using name substitution to test for discrimination is well-established.\\n\\n2. The large scale of the experiments, with over 2 million generated emails across multiple models, occupations, and demographic groups, is a strength of the study. This allows for robust statistical analysis.\\n\\n3. The finding that Hispanic male names consistently receive the lowest acceptance rates across models is concerning and merits further investigation. The authors should discuss potential implications of this more thoroughly.\\n\\n4. While the differences in acceptance rates between groups are statistically significant, the absolute magnitudes are relatively small (1.6-3.78%). The authors should provide more context on whether these effect sizes are practically meaningful.\\n\\n5. The study design using underspecified prompts helps isolate the effect of names, but limits generalizability to real-world hiring scenarios with more applicant information. This limitation should be discussed more prominently.\\n\\n6. The analysis of how stated qualifications interact with demographic biases is interesting. The finding that white male names are most favored for \"somewhat qualified\" candidates deserves more discussion.\\n\\n7. The comparison across model sizes, showing Llama2-70B has the least variation across groups, is intriguing. More discussion of potential reasons for this would strengthen the paper.\\n\\n8. The ethics statement thoughtfully considers potential risks of the research. However, more discussion of mitigation strategies or recommendations for responsible AI development would be valuable.\\n\\n9. While the study focuses on race/ethnicity and gender, acknowledging other important dimensions of identity that were not examined would provide useful context.\\n\\n10. The paper would benefit from a more in-depth discussion of how these findings relate to and build upon prior work on algorithmic bias in hiring.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n\\n- Limited scope of demographic groups studied\\n  \u2022 Only examines Black, White, and Hispanic names\\n  \u2022 Does not consider other important demographic factors like age, nationality, etc.\\n\\n- Potential oversimplification of hiring process \\n  \u2022 Uses only first names as proxy for demographic identity\\n  \u2022 Lacks other key resume information typically used in hiring\\n\\n- Questions about ecological validity\\n  \u2022 Artificial prompt-based setup may not reflect real-world hiring practices\\n  \u2022 Unclear how results would generalize to actual hiring scenarios\\n\\n- Methodological concerns\\n  \u2022 Reliance on name-based stereotypes could reinforce problematic assumptions\\n  \u2022 Limited number of templates/prompts may not capture full range of hiring contexts\\n\\n4. Suggestions for improvement", "multi_agent_without_knowledge": "Here is the comprehensive critical review of the paper \"Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?\":\\n\\nThis paper presents a novel and significant contribution to the field of AI ethics and LLM research by examining potential biases in hiring decisions made by large language models (LLMs). The study\\\\'s methodology, findings, and implications are both timely and important, given the increasing use of AI in various decision-making processes.\\n\\nStrengths:\\n\\n1. Methodology: The paper demonstrates a rigorous and well-designed experimental approach. The use of 820 templates combining different qualification levels, base templates, and occupational roles provides a comprehensive framework for testing LLM biases. The large-scale analysis of over 2 million generated emails across multiple state-of-the-art LLMs adds statistical power to the findings.\\n\\n2. Novelty: The study\\\\'s focus on name-based discrimination in hiring decisions, using minimal information about candidates, is a unique approach that effectively isolates the impact of perceived demographic characteristics on LLM decision-making.\\n\\n3. Reproducibility: The use of multiple random seeds for open-source models and the detailed documentation of the methodology enhance the study\\\\'s reproducibility, setting a high standard for future research in this area.\\n\\n4. Clarity: The paper is well-structured and clearly written, with a logical flow from the research question through methodology, results, and conclusions. The authors provide thorough explanations of their experimental setup, data generation, and analysis processes.\\n\\n5. Empirical Findings: The study reveals significant biases in LLMs\\\\' hiring decisions, particularly the consistent disadvantage faced by Hispanic male applicants across different models and settings. These findings align with and extend previous research on labor market discrimination.\\n\\n6. Practical Implications: The research has immediate relevance for the use of AI in hiring processes and other high-stakes decision-making contexts, serving as a warning about potential fairness issues in professional settings.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Limited Demographic Scope: While the study focuses on three races/ethnicities and two genders, it could be expanded to include a wider range of demographic identities, including other racial and ethnic groups, gender identities, nationalities, and disability statuses.\\n\\n2. Occupational Coverage: Although 41 occupational roles were examined, this represents only a fraction of existing occupations. A more comprehensive range of job sectors could provide a fuller picture of LLM biases.\\n\\n3. Model Selection: The study focused on large language models (\u22657B parameters). Including smaller models could offer insights into how model size affects discriminatory behaviors.\\n\\n4. Simplified Hiring Process: While the minimal-information approach effectively isolated name-based biases, future research could benefit from simulating more realistic hiring processes with fuller applicant profiles.\\n\\n5. Visual Representation: The paper could benefit from more visual aids, such as graphs or charts, to illustrate the differences in acceptance rates across different groups and models.\\n\\nSuggestions for Future Research:\\n\\n1. Longitudinal Studies: Track changes in LLM discriminatory behaviors over time and across model versions.\\n\\n2. Mitigation Strategies: Explore and propose concrete strategies for mitigating biases in LLMs, especially for hiring-related tasks.\\n\\n3. Intersectionality: Conduct a more in-depth analysis of how multiple identity factors interact in LLM decision-making.\\n\\n4. Comparative Analysis: Directly compare LLM and human hiring biases to provide important context for interpreting the results.\\n\\n5. Cross-cultural Analysis: Expand the research to include how LLM biases may vary across different cultural and linguistic contexts.\\n\\n6. Training Data Impact: Investigate how the training data of these LLMs influences their hiring biases.\\n\\n7. Legal and Regulatory Implications: Explore the legal and regulatory implications of using LLMs in hiring processes.\\n\\nIn conclusion, this paper makes a significant contribution to our understanding of LLM biases in hiring decisions. Its methodological rigor, clear presentation, and important findings provide a solid foundation for future research in this critical area of AI ethics. While there are areas for improvement and expansion, the study effectively highlights the potential risks of using LLMs in high-stakes decision-making processes and underscores the need for ongoing vigilance and research as these models become increasingly integrated into various aspects of society.", "multi_agent_with_knowledge": "Critical Review of \"Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?\"\\n\\nThis paper presents a large-scale study examining whether large language models (LLMs) exhibit discriminatory behavior based on race, ethnicity, and gender when making simulated hiring decisions. The research addresses crucial questions about bias in AI systems as they become increasingly integrated into high-stakes decision-making processes.\\n\\nNovelty Assessment:\\nWhile the paper investigates an important topic, it\\\\'s essential to note that the novelty of this research is somewhat limited. Similar studies have previously explored bias in LLMs\\\\' decision-making processes related to job applications, focusing on race, ethnicity, and gender. However, this study does offer a unique contribution by using email generation as opposed to resume scoring, and by providing a more comprehensive analysis across multiple LLMs and various occupational roles.\\n\\nStrengths:\\n\\n1. Experimental Design and Methodology:\\nThe study\\\\'s primary strength lies in its robust and comprehensive experimental design. The authors generated over 2 million emails across multiple state-of-the-art LLMs, providing a substantial dataset for analysis. The use of 820 templatic prompts, 300 diverse names representing three races/ethnicities and two genders, and 41 occupational roles allows for a nuanced examination of potential biases across various dimensions.\\n\\n2. Consistency of Claims and Evidence:\\nThe paper\\\\'s claims are well-supported by the presented evidence. The extensive quantitative results, clearly presented in tables and figures, consistently demonstrate biases, particularly against Hispanic names and in favor of White applicants.\\n\\n3. Clarity of Writing and Visualizations:\\nThe paper is exceptionally well-written, with a clear structure and logical flow of ideas. The figures and tables, as evidenced by the captions, provide clear and detailed breakdowns of the results, supporting the abstract\\\\'s findings. The use of color coding in the tables (blue for significantly above average, red for significantly below average) helps quickly identify notable trends.\\n\\nLimitations and Suggestions for Improvement:\\n\\n1. Novelty and Differentiation:\\nWhile the study offers valuable insights, it could benefit from more clearly articulating how it differs from and builds upon existing research on bias in LLMs, particularly in hiring contexts. The authors should emphasize the unique aspects of their methodology and findings.\\n\\n2. Abstract Comprehensiveness:\\nThe abstract could be more comprehensive in describing the study\\\\'s scope. It should mention the specific LLMs tested (e.g., Llama2, GPT-3.5), the broader range of racial and gender comparisons made (including Black female names), and the consideration of different occupational roles and educational requirements.\\n\\n3. Demographic Representation:\\nWhile the study includes names representing three races/ethnicities and two genders, it could benefit from expanding to include a broader range of demographic groups, such as other ethnicities, non-binary gender identities, or age groups.\\n\\n4. Real-world Applicability:\\nThe paper could benefit from a more in-depth discussion of how these findings might translate to actual hiring scenarios and suggest potential mitigation strategies for organizations considering the use of LLMs in their hiring processes.\\n\\n5. Ethical Implications:\\nA more extensive exploration of the ethical considerations and potential mitigation strategies would enhance the paper\\\\'s impact and provide valuable guidance for practitioners and policymakers.\\n\\n6. Visual Presentation:\\nWhile the tables provide valuable data, the paper could benefit from including more visualizations (e.g., graphs or charts) to make the trends more immediately apparent. Additionally, improving the image quality for better readability and including a legend explaining abbreviations would enhance clarity.\\n\\nConclusion:\\nThis paper represents a significant contribution to our understanding of bias in large language models, particularly in the context of hiring decisions. While its novelty is somewhat limited by existing research in the field, it offers a unique and comprehensive analysis that provides crucial insights into the challenges of ensuring fairness in AI-assisted hiring processes. The robust methodology, clear presentation of results, and thoughtful discussion of implications make it a valuable resource for researchers, policymakers, and practitioners in the field of AI ethics. By addressing the suggested improvements, particularly in emphasizing its unique contributions and expanding on real-world implications, the authors can further enhance the impact and relevance of this important work."}
{"paper_id": "acl_2024_s62", "title": "Aligning Large Language Models via Fine-grained Supervision", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s62.pdf", "human_reviewer": "", "barebones": "Here is a list of peer review feedback comments for the scientific paper:\\n\\n1. Methodology:\\n- The proposed fine-grained supervision approach using minimal editing is novel and interesting. However, more details are needed on how exactly the editing process works, especially when using LLMs like Claude-2 rather than human annotators. What specific instructions are given? How is consistency ensured across edits?\\n\\n2. Experimental Design:  \\n- The comparison to baseline RLHF methods is appropriate, but the sample size and number of experimental runs should be increased to strengthen the statistical significance of the results.\\n- It would be beneficial to include additional baseline methods beyond just standard PPO-RLHF for a more comprehensive evaluation.\\n\\n3. Results:\\n- The reported 5.1% improvement in win rate is promising, but more analysis is needed on the statistical significance of this result. \\n- Additional metrics beyond just win rate would provide a more holistic evaluation of the model\\\\'s capabilities and alignment.\\n\\n4. Limitations:\\n- The authors acknowledge the lack of rigorous mathematical proof for the reward allocation strategy. This is an important limitation that should be addressed in future work.\\n- The reliance on proprietary LLMs like Claude-2 for editing limits reproducibility. Consider using open-source alternatives.\\n\\n5. Broader Impact:\\n- The paper would benefit from a discussion of potential ethical implications and limitations of this approach to LLM alignment.\\n\\n6. Writing and Organization:  \\n- The abstract and introduction provide a clear overview, but the methods section could be restructured for better flow and clarity.\\n- Some equations and technical details in Section 2.2 could use further explanation for readers less familiar with RL concepts.\\n\\n7. Future Work:\\n- Suggestions for expanding on this work in future research would strengthen the paper\\\\'s conclusion.\\n\\nOverall, this paper presents an interesting and potentially impactful approach to improving LLM alignment through fine-grained supervision. Addressing the above points would further strengthen the work. I recommend minor revisions before publication.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   a. Lack of theoretical foundation\\n      \u2022 No rigorous mathematical proof for effectiveness of reward allocation strategy\\n      \u2022 Reliance on empirical results without strong theoretical justification\\n   \\n   b. Limited experimental validation\\n      \u2022 Experiments conducted only on one model (LLaMA-7B)\\n      \u2022 Lack of comparison with other recent alignment techniques\\n\\n   c. Potential scalability issues\\n      \u2022 No discussion on how the method scales to larger language models\\n      \u2022 Unclear if fine-grained annotation remains feasible for more complex tasks\\n\\n   d. Ethical concerns and potential biases\\n      \u2022 Use of proprietary LLM (Claude-2) for data annotation may introduce biases\\n      \u2022 No discussion on potential negative impacts or misuse of the proposed method\\n\\n4. Suggestions for improvement", "multi_agent_without_knowledge": "Here is the comprehensive review of the paper \"Aligning Large Language Models via Fine-grained Supervision\":\\n\\nComprehensive Review of \"Aligning Large Language Models via Fine-grained Supervision\"\\n\\n1. Clarity and Structure:\\nThe paper is generally well-structured, following a logical flow from introduction to conclusion. The authors clearly state their main contributions in the introduction, which helps set the stage for the rest of the paper. The methodology is explained in detail, with separate sections for fine-grained data collection and token-level reward modeling. However, some improvements could be made:\\n\\n- The abstract could be more concise and focused on the key contributions and results.\\n- Some technical terms and concepts (e.g., RLHF, PPO) are introduced without sufficient explanation for readers who may not be familiar with them.\\n- The figures and tables, while informative, could benefit from more detailed captions to aid in interpretation.\\n\\n2. Methodology:\\nThe paper introduces a novel approach to aligning large language models through fine-grained supervision. Key aspects include:\\n\\na) Fine-grained data collection: Instead of annotating entire responses, the method involves targeted editing by humans or language models. This approach aims to provide more granular feedback on specific parts of a response that need improvement.\\n\\nb) Token-level reward modeling: The authors propose a token-level reward scheme that assigns rewards to individual tokens rather than entire sequences. This allows for more precise feedback during the reinforcement learning process.\\n\\nThe methodology is innovative and well-explained, but could benefit from:\\n- More detailed examples of the editing process and how it differs from traditional annotation methods.\\n- A clearer explanation of how the token-level reward model is integrated into the PPO algorithm.\\n\\n3. Experiments and Results:\\nThe experimental setup is solid, using the AlpacaFarm framework and evaluating against established baselines. Key findings include:\\n\\n- An absolute performance increase of 4.7% compared to traditional methods when using the fine-grained data collection technique.\\n- Further improvements when incorporating the token-level reward model.\\n- A significant boost in training efficiency, with PPO convergence time reduced by half.\\n- The fine-grained dataset enables the reward model to reach an accuracy of approximately 85.2% in assigning rewards to superior responses.\\n\\nThese results are impressive and well-supported by the data presented. However, the paper could benefit from:\\n- More detailed analysis of the types of improvements seen in model outputs.\\n- Discussion of potential limitations or edge cases where the method might not perform as well.\\n\\n4. Impact and Significance:\\nThe paper presents a significant contribution to the field of LLM alignment:\\n\\n- The fine-grained supervision approach offers a more nuanced method for improving model behavior compared to traditional RLHF techniques.\\n- The improved training efficiency could be particularly important as LLMs continue to grow in size.\\n- The method shows promise for enhancing value alignment in LLMs, which is crucial for their safe and beneficial deployment.\\n\\n5. Limitations and Future Work:\\nThe authors acknowledge that they struggle to provide rigorous mathematical proof for the effectiveness of their reward allocation strategy. This honesty is commendable, and it opens up avenues for future research:\\n\\n- Developing theoretical foundations for fine-grained supervision in LLM alignment.\\n- Exploring the scalability of this approach to even larger language models.\\n- Investigating how this method performs on a wider range of tasks and domains.\\n\\n6. Overall Assessment:\\nThis paper presents a novel and promising approach to LLM alignment. The combination of fine-grained data collection and token-level reward modeling shows significant improvements over baseline methods in both performance and training efficiency. While there are areas where the paper could be improved in terms of clarity and depth of analysis, the core contributions are valuable and likely to influence future research in this important area of AI development.\\n\\nRecommendations for improvement:\\n1. Provide more detailed examples of the editing process and resulting improvements in model outputs.\\n2. Expand on the potential limitations and edge cases of the approach.\\n3. Include a more in-depth discussion of the broader implications for AI safety and ethics.\\n4. Strengthen the theoretical foundations of the token-level reward modeling approach.\\n5. Consider conducting user studies to assess the real-world impact of the improved model alignment.\\n\\nIn conclusion, this paper represents a significant step forward in the field of LLM alignment, offering both practical improvements and promising directions for future research.", "multi_agent_with_knowledge": "Critical Review of \"Aligning Large Language Models via Fine-grained Supervision\"\\n\\nThis paper introduces an approach to improving the alignment of large language models (LLMs) with human values and expectations through a fine-grained reinforcement learning from human feedback (RLHF) framework. The authors address a challenge in AI alignment by proposing a method that combines fine-grained data collection and token-level reward modeling to enhance the precision and effectiveness of LLM alignment.\\n\\nStrengths:\\n\\n1. Performance Improvement: The proposed method demonstrates improvements in LLM performance, achieving up to a 5.1% absolute increase in win rate against the reference model compared to traditional PPO-RLHF approaches. This performance boost highlights the potential impact of the fine-grained supervision approach.\\n\\n2. Enhanced Reward Modeling: The token-level reward model shows increased accuracy (85.2%) in assigning rewards to superior responses, outperforming models trained on original datasets. This improvement in reward modeling precision is likely a key factor in the overall performance gains.\\n\\n3. Training Efficiency: A notable strength of the approach is the significant improvement in training efficiency. The fine-grained reward model reduces the time required for Proximal Policy Optimization (PPO) to converge by half, which has important implications for scaling to larger language models and more complex tasks.\\n\\n4. Practical Implementation: The authors provide a clear methodology for implementing their approach, including the use of minimal editing techniques and integration with existing RLHF pipelines. This practical focus enhances the potential for adoption and further development of the method by other researchers and practitioners.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Novelty Concerns: The novelty of the proposed approach is questionable when compared to existing work in the field. Several assessments indicate that the core idea of using more granular supervision to enhance LLM alignment is already present in other papers, such as those focusing on token-level reinforcement learning or hybrid frameworks incorporating token-level policy probabilities. The paper needs to more clearly differentiate its contributions from existing methods and demonstrate significant advancements over prior work.\\n\\n2. Theoretical Foundation: The authors acknowledge a lack of rigorous mathematical proof for the effectiveness of their reward allocation strategy, particularly in Equation 4. Developing a stronger theoretical foundation would enhance the credibility and generalizability of the approach.\\n\\n3. Generalizability: The study primarily uses the AlpacaFarm environment and the LLaMA-7B model. To strengthen claims of broad applicability, further testing on diverse environments and model architectures is necessary. This would help demonstrate the method\\\\'s effectiveness across different contexts and scales.\\n\\n4. Human Evaluation: While the paper uses Claude as a judge for evaluations, incorporating direct human evaluations could provide additional validation of the approach\\\\'s effectiveness in aligning with human values. This would be particularly valuable given the focus on human value alignment.\\n\\n5. Comparative Analysis: A more extensive comparison with other recent advancements in LLM alignment techniques would provide better context for the significance of this approach within the field. This would help readers understand how the proposed method compares to other state-of-the-art techniques, especially those that already incorporate token-level or fine-grained approaches.\\n\\n6. Long-term Effects: The paper could benefit from a discussion on the long-term effects of this fine-grained supervision on model behavior, especially in terms of maintaining coherence and consistency across longer outputs. This would provide insights into the sustainability of the improvements achieved through this method.\\n\\n7. Ethical Considerations: A more in-depth discussion of potential ethical implications of fine-grained LLM alignment, including privacy concerns and the potential for misuse, would enhance the paper\\\\'s comprehensiveness.\\n\\n8. Clarity and Consistency: There are some inconsistencies between the abstract and the figure captions, particularly regarding the specific models used (e.g., Claude, Davinci003) and the types of improvements seen. The abstract could be more specific about the evaluation process and the kinds of enhancements observed in model outputs.\\n\\nSuggestions for Enhancement:\\n\\n1. Clearly articulate the novel aspects of the approach in comparison to existing work on token-level and fine-grained LLM alignment methods. Emphasize any unique aspects of the implementation or results that set this work apart.\\n\\n2. Expand on the theoretical foundations of the token-level reward modeling approach, potentially collaborating with theorists to develop a more robust mathematical framework.\\n\\n3. Conduct additional experiments using a wider range of LLMs and task environments to demonstrate the method\\\\'s generalizability.\\n\\n4. Include ablation studies to isolate the impact of different components of the proposed method (e.g., fine-grained data collection vs. token-level reward modeling).\\n\\n5. Provide more detailed examples of the minimal editing process and its impact on model outputs to illustrate the practical benefits of the approach.\\n\\n6. Discuss potential real-world applications and implications of the fine-grained supervision method.\\n\\n7. Ensure consistency between the abstract and the main content of the paper, particularly regarding the specific models used and the types of improvements observed.\\n\\n8. Include a more comprehensive discussion of the limitations and potential drawbacks of the proposed method, acknowledging areas where further research is needed.\\n\\nIn conclusion, while the paper presents an interesting approach to LLM alignment through fine-grained supervision, it faces significant challenges in terms of novelty and differentiation from existing work. The authors should focus on clearly articulating the unique contributions of their method and providing more robust evidence of its advantages over current state-of-the-art techniques in LLM alignment. Additionally, addressing the identified limitations and expanding on the theoretical foundations would significantly strengthen the paper\\\\'s impact and relevance in the field."}
{"paper_id": "acl_2024_s47", "title": "AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s47.pdf", "human_reviewer": "", "barebones": "Here are my main feedback comments on this scientific paper:\\n\\n1. The abstract provides a good overview of the key contributions and findings, but could be more concise. Consider tightening it up to focus on the core novelty and results.\\n\\n2. The introduction does a nice job of motivating the problem and highlighting limitations of existing approaches. However, it could benefit from a clearer roadmap paragraph outlining the key contributions and structure of the paper.\\n\\n3. The proposed AGR approach seems novel and well-motivated. The use of causal intervention and reinforcement learning to guide rationalization is an interesting direction. However, more justification and intuition could be provided for some of the design choices.\\n\\n4. The theoretical analysis and proof in Section 3.3 and Appendix A help provide some formal grounding, which is good to see. However, the practical implications of the theoretical results could be discussed more.\\n\\n5. The experimental evaluation is fairly comprehensive, with comparisons to multiple baselines on two datasets. The ablation study helps validate the key components. However, some additional analysis of the results and failure cases would strengthen the evaluation.\\n\\n6. The limitations section is good to include, but could be expanded with more discussion of potential drawbacks or failure modes of the approach.\\n\\n7. The paper would benefit from a more thorough discussion of broader implications and future work directions in the conclusion.\\n\\n8. Some of the equations and technical descriptions could use more explanation or intuition to make them more accessible to a broader audience.\\n\\n9. The paper structure and flow are generally good, but some sections (e.g. 3.2) feel a bit dense and could potentially be reorganized for clarity.\\n\\n10. More qualitative examples and analysis of the generated rationales would help illustrate the benefits of the approach.\\n\\nOverall, this appears to be a solid technical contribution with promising results, but could be strengthened in terms of accessibility, analysis, and broader discussion. Let me know if you would like me to elaborate on any of these points.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   \\n   a) Limited theoretical justification\\n      - Lemma 1 provides only a high-level justification for the auxiliary agent\\n      - Lack of rigorous proofs or analysis for other key components\\n   \\n   b) Complexity concerns\\n      - Introduces additional complexity compared to existing methods\\n      - May impact scalability and practical deployment\\n   \\n   c) Empirical evaluation limitations  \\n      - Evaluated on only two datasets\\n      - Lack of ablation studies on some key components\\n   \\n   d) Unclear generalizability\\n      - Focus on specific rationalization tasks\\n      - Not demonstrated on other NLP tasks or modalities\\n\\n4. Suggestions for improvement\\n   a) Strengthen theoretical foundations\\n   b) Conduct more comprehensive empirical evaluation\\n   c) Improve efficiency and reduce complexity  \\n   d) Demonstrate broader applicability", "multi_agent_without_knowledge": "Critical Review of \"AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization\"\\n\\nThis paper introduces AGR (Agent-Guided Rationalization), a novel approach to selective rationalization that addresses the critical issue of degeneration accumulation in existing methods. The authors present a well-structured and innovative solution that combines causal intervention calculus with reinforcement learning to guide the cooperative game optimization during rationale training.\\n\\nStrengths:\\n\\n1. Novelty and Contribution:\\nThe paper\\\\'s primary strength lies in its innovative approach to tackling a significant problem in rationalization methods. By introducing a reinforced causal agent to guide the model\\\\'s learning process, AGR represents a substantial advancement in the field. The integration of causal intervention calculus with reinforcement learning is particularly noteworthy, opening new avenues for research in optimizing cooperative game dynamics in machine learning models.\\n\\n2. Theoretical Foundation:\\nThe authors provide a solid theoretical justification for their approach, including Lemma 1, which supports the creation of an auxiliary agent for rationalization. This theoretical grounding enhances the credibility of the proposed method and provides a foundation for future research in this area.\\n\\n3. Empirical Results:\\nAGR demonstrates impressive performance, consistently outperforming state-of-the-art methods on both BeerAdvocate and HotelReview datasets. The method\\\\'s robustness is evidenced by its consistent performance improvements across different sparsity levels, which is a significant achievement in selective rationalization tasks.\\n\\n4. Comprehensive Evaluation:\\nThe paper includes a thorough evaluation of the proposed method, including ablation studies that demonstrate the importance of key components such as causal effectiveness optimization and the pretrained agent embedding. The inclusion of visualized examples effectively illustrates AGR\\\\'s ability to focus on relevant aspects and mitigate degeneration during training.\\n\\n5. Clarity and Structure:\\nOverall, the paper is well-structured and clearly written. The authors effectively communicate their ideas, methodologies, and results, maintaining consistency in their claims throughout the text.\\n\\nAreas for Improvement:\\n\\n1. Accessibility:\\nSome sections of the paper, particularly in the methodology, could benefit from more intuitive explanations or examples. This would make the content more accessible to a broader audience, especially readers without a strong background in reinforcement learning and causal inference.\\n\\n2. Complexity Analysis:\\nWhile the authors provide a comparison of AGR\\\\'s complexity with other models, showing a slight increase, a more detailed discussion on the practical implications of this increased complexity would be beneficial. This could include an analysis of the trade-offs between improved performance and increased computational requirements.\\n\\n3. Limitations and Future Work:\\nThe authors acknowledge limitations in efficiently searching for meaningful actions within larger search spaces and the challenge of incorporating the cooperative framework with large language models. A more in-depth discussion of these limitations and potential strategies to address them would strengthen the paper.\\n\\n4. Generalizability:\\nWhile the paper demonstrates excellent results on BeerAdvocate and HotelReview datasets, including additional datasets from different domains would provide stronger evidence of the method\\\\'s generalizability.\\n\\n5. Practical Implications:\\nThe paper could benefit from a more extensive discussion on the practical implications and potential real-world applications of AGR. This would help readers better understand the broader impact of the proposed method beyond academic research.\\n\\nSuggestions for Future Work:\\n\\n1. Exploring methods to improve the efficiency of searching for meaningful actions within larger search spaces.\\n2. Investigating the integration of data-level debiasing techniques to further enhance the performance of AGR.\\n3. Extending the work to incorporate the cooperative framework with large language models, as suggested by the authors.\\n4. Conducting user studies to evaluate the interpretability and usefulness of the rationales generated by AGR in real-world scenarios.\\n\\nConclusion:\\n\"AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization\" presents a significant advancement in the field of selective rationalization. The novel approach to addressing degeneration accumulation, combined with strong theoretical foundations and impressive empirical results, positions this work as highly influential. While there are areas for potential improvement, particularly in terms of accessibility and discussing practical implications, the paper\\\\'s contributions are substantial and likely to inspire future research in causal and agent-guided rationalization techniques. The potential impact of this work extends beyond its immediate field, offering valuable insights for developing more robust and interpretable AI systems across various domains.", "multi_agent_with_knowledge": "Comprehensive Review of \"AGR: Reinforced Causal Agent-Guided Self-explaining Rationalization\"\\n\\nThis paper introduces AGR (Agent-Guided Rationalization), an approach to selective rationalization in Natural Language Processing (NLP) that addresses the issue of degeneration accumulation in existing methods. The authors present a solution that combines causal intervention calculus with reinforcement learning to guide the rationalization process.\\n\\nNovelty and Contribution:\\nAfter careful consideration and comparison with existing work in the field, the novelty of this paper is limited. The core concepts of combining causal reasoning with reinforcement learning for improving model performance and explainability have been explored in previous works. While the AGR method introduces some interesting elements, such as the agent-guided approach and the focus on rationalization, these do not represent a significant departure from existing frameworks in the field.\\n\\nThe paper\\\\'s main contributions include:\\n1. Introduction of a reinforced causal agent to guide cooperative game optimization during rationale training.\\n2. Quantification of causal effects in the rationalization process using causal intervention calculus.\\n3. Integration of reinforcement learning to refine learning bias during training.\\n4. Theoretical insights into the necessity of guidance for accurate predictions.\\n\\nHowever, these contributions appear to be incremental advancements rather than fundamentally novel concepts in the field of causal reasoning and reinforcement learning for NLP tasks.\\n\\nMethodology:\\nThe AGR method is built upon a theoretical foundation, introducing several key components:\\n1. Rationale Causal Attribution: Quantifies causal effects using causal intervention calculus.\\n2. Markov Decision Process as RL: Models the training process as a dynamic approach to optimizing rationale selection.\\n3. Pretrained Agent: Introduces a reinforced causal agent to align probability distributions.\\n4. Policy Network Architecture: A network that learns representations of action candidates.\\n\\nWhile the methodology is well-explained, it would benefit from a more explicit discussion of how it differs from and improves upon existing approaches that combine causal reasoning and reinforcement learning.\\n\\nExperimental Validation:\\nThe authors conduct experiments to validate their approach:\\n1. Comparison with multiple baselines on BeerAdvocate and HotelReview datasets.\\n2. Sparsity experiments demonstrating effectiveness across different sparsity levels.\\n3. Ablation studies illustrating the importance of each component.\\n4. Visualization of generated rationales for qualitative assessment.\\n\\nResults and Impact:\\n1. AGR shows improvements over some baseline methods on the tested datasets.\\n2. The approach addresses degeneration accumulation, a limitation of existing methods.\\n3. The combination of causal inference and RL provides a framework for optimizing rationale selection.\\n\\nHowever, the impact of these results is somewhat limited by the lack of significant novelty in the overall approach.\\n\\nClarity and Presentation:\\nThe paper\\\\'s structure is generally logical, but there are several areas for improvement:\\n1. The abstract could better reflect the actual novelty of the work, avoiding overstatement of its contributions.\\n2. The transition between theoretical framework and experimental results could be smoother.\\n3. More visualizations or examples of generated rationales would help readers better understand the qualitative improvements of AGR.\\n4. The figures provided do not fully represent all aspects mentioned in the abstract. For instance, the causal intervention calculus and reinforcement learning process are not clearly visualized.\\n5. The results table (Table 1) could benefit from including statistical significance measures and results from both mentioned datasets.\\n\\nLimitations and Future Work:\\nThe authors discuss some limitations, including:\\n1. The need for more efficient search strategies for meaningful actions in larger search spaces.\\n2. The absence of data-level debiasing techniques.\\n3. Challenges in fine-tuning pretrained language models within the cooperative game framework.\\n\\nFuture research directions could include:\\n1. Investigating ways to improve the efficiency of searching for meaningful actions.\\n2. Exploring the integration of data-level debiasing techniques with AGR.\\n3. Developing methods to incorporate the AGR framework with large pretrained language models.\\n4. Extending the application of AGR to other NLP tasks beyond sentiment analysis.\\n5. Exploring the use of more diverse datasets to demonstrate generalizability across different domains.\\n\\nConclusion:\\nWhile AGR presents an interesting approach to selective rationalization for NLP tasks, its novelty and impact are somewhat limited when compared to existing work in the field. The paper\\\\'s integration of causal inference and reinforcement learning is a positive aspect, but it does not represent a significant departure from previous research. The empirical results show some improvements, but the overall contribution to the field appears incremental rather than transformative.\\n\\nTo strengthen future work, the authors should focus on:\\n1. More clearly differentiating their approach from existing methods that combine causal reasoning and reinforcement learning.\\n2. Providing a more comprehensive set of visualizations that fully represent the proposed method and its advantages.\\n3. Expanding the experimental validation to include a wider range of datasets and tasks to demonstrate broader applicability and generalizability.\\n4. Addressing the limitations in the current approach, particularly in terms of efficiency and scalability to larger language models and more complex NLP tasks.\\n\\nDespite these limitations, the paper provides valuable insights into the challenges of selective rationalization and offers a foundation for future research in this important area of explainable AI in NLP."}
{"paper_id": "acl_2024_s14", "title": "Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s14.pdf", "human_reviewer": "", "barebones": "Here is a list of feedback comments for the scientific paper, focusing on major points as a peer reviewer:\\n\\n1. The proposed bi-directional multi-granularity generation (BDMG) framework is an interesting and novel approach to knowledge graph-to-text generation. The idea of generating sentence-level text first and then aggregating to graph-level is promising.\\n\\n2. The backward relation extraction task is a clever addition that seems to improve performance. More details on how this is implemented would be helpful.\\n\\n3. The experimental results show significant improvements over state-of-the-art baselines on the WebNLG dataset. The ablation study effectively demonstrates the contributions of different components.\\n\\n4. The paper could benefit from a more thorough discussion of limitations and potential drawbacks of the proposed approach. For example, are there cases where sentence-level generation may not work well?\\n\\n5. More details on the implementation, such as hyperparameters, training time, and computational resources used would be useful for reproducibility.\\n\\n6. The introduction and related work sections provide good context, but could be more concise. Some of the background information could potentially be trimmed.\\n\\n7. The methodology section is generally clear, but some of the mathematical formulations and notation could be explained in more detail for better readability.\\n\\n8. It would be interesting to see qualitative examples of the model\\\\'s output compared to baselines, to get a better sense of the improvements in text quality.\\n\\n9. The conclusion is quite brief. Expanding on potential future work directions would strengthen the paper.\\n\\n10. Overall, this appears to be a solid contribution to the field of knowledge graph-to-text generation, with promising results and a novel approach. With some refinements and additional details, it could be a strong paper.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   a. Limited theoretical foundation\\n     - Lacks formal analysis of proposed framework\\\\'s advantages\\n     - No theorems or proofs to support claims of improved performance\\n   \\n   b. Insufficient comparison to recent state-of-the-art methods\\n     - Does not compare against some recent strong baselines (e.g. T5-based models)\\n     - Ablation studies do not isolate impact of each proposed component\\n   \\n   c. Concerns about reproducibility and implementation details\\n     - Hyperparameter settings and training details not fully specified\\n     - Code not made publicly available\\n   \\n   d. Evaluation metrics and analysis could be more comprehensive\\n     - Relies mainly on automatic metrics, lacks human evaluation\\n     - No qualitative analysis or example outputs provided\\n\\n4. Suggestions for improvement\\n   a. Strengthen theoretical justification\\n   b. Expand comparisons to recent state-of-the-art methods\\n   c. Provide more implementation details and release code\\n   d. Conduct human evaluation and qualitative analysis", "multi_agent_without_knowledge": "Critical Review of \"Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model\"\\n\\nThis review provides a comprehensive analysis of the paper, addressing its strengths, limitations, and potential areas for improvement.\\n\\n1. Contribution and Novelty:\\nThe paper introduces a novel bi-directional multi-granularity generation framework (BDMG) for knowledge graph-to-text (KG-to-text) tasks. The main contributions are:\\na) A sentence-level generation approach that constructs text based on corresponding triples, rather than generating the whole text at once.\\nb) A backward relation-extraction task to enhance the correctness of relational information.\\nc) Achieving state-of-the-art results on the WebNLG benchmark dataset.\\n\\nThe BDMG approach addresses a significant limitation in existing methods by focusing on sentence-level generation, which improves the accuracy of incorporating relevant triples for each sentence. This novel approach represents a noteworthy advancement in the field of KG-to-text generation.\\n\\n2. Methodology:\\nThe paper clearly explains the BDMG framework, which consists of two main components:\\na) Forward Sentence-Level Generation: Decomposes the generation of text into a sequential decoding problem, generating semantically complete sentences based on specific subsets of KG triples.\\nb) Backward Relation Extraction: Enhances the model\\\\'s ability to capture correct relational information by prompting the model to infer relations between head and tail entities based on the generated text.\\n\\nThe methodology is well-described, including mathematical formulations for the generation process and loss functions. However, the paper could benefit from more detailed examples or illustrations to enhance understanding, particularly for readers less familiar with the field.\\n\\n3. Experiments and Results:\\nThe experiments demonstrate significant performance improvements over previous state-of-the-art methods. Key findings include:\\n- BDMG-11B outperforms the previous SOTA (Plan Selection) by 8.5 points in BLEU, 3.6 points in METEOR, and 7.4 points in ROUGE on the WebNLG dataset.\\n- Ablation studies effectively demonstrate the importance of each component in the BDMG framework.\\n\\nHowever, there are several areas where the experimental section could be strengthened:\\na) Dataset Diversity: Expand experiments to include additional KG-to-Text datasets to demonstrate generalizability.\\nb) Evaluation Metrics: Incorporate more recent metrics like BERTScore or BLEURT, and include human evaluation scores.\\nc) Comparison with Other Models: Include comparisons with more recent approaches, especially those using similar large language model backbones.\\nd) Scalability and Efficiency Analysis: Provide more information on how the BDMG approach scales with increasing knowledge graph size and complexity, and compare computational requirements with other models.\\ne) Error Analysis: Include a detailed categorization of error types to guide future improvements.\\nf) Robustness Testing: Evaluate performance on noisy or incomplete knowledge graphs.\\n\\n4. Clarity and Structure:\\nThe paper follows a clear and logical structure, which aids in overall clarity. However, there are areas for improvement:\\na) The explanation of the BDMG framework could benefit from more detailed examples or step-by-step illustrations.\\nb) Mathematical formulations could use more accompanying explanations to improve readability for a broader audience.\\nc) The limitations section is brief and could be expanded to provide a more balanced view of the research.\\n\\n5. Impact and Future Work:\\nThe BDMG framework shows promising results in improving KG-to-text generation, with potential applications in various fields such as question-answering systems, dialogue systems, and mitigating hallucination in large language models. To enhance the paper\\\\'s impact:\\na) Discuss potential applications beyond the WebNLG dataset and how the approach might be applied to other domains or larger-scale knowledge graphs.\\nb) Elaborate on the computational requirements and efficiency of the BDMG approach compared to existing methods.\\nc) Consider discussing ethical implications or potential biases in the generated text.\\nd) Expand on future work, such as adapting the method to handle more complex knowledge graphs or multi-modal inputs, investigating scalability to even larger language models, and exploring ways to make the method more effective for simpler, single-sentence KGs.", "multi_agent_with_knowledge": "Critical Review of \"Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model\"\\n\\nThis paper presents a novel approach to knowledge graph-to-text (KG-to-text) generation, introducing a bi-directional multi-granularity generation framework (BDMG) that addresses key challenges in the field. The authors demonstrate significant improvements over existing methods, achieving new state-of-the-art results on the WebNLG benchmark dataset.\\n\\nNovelty:\\nThe paper introduces a novel bi-directional multi-granularity generation framework that operates at both sentence and graph levels. This approach, along with the backward relation-extraction task, offers a more nuanced and potentially more accurate method for generating text from knowledge graphs. The novelty of this work is evident when compared to existing approaches in the field, such as zero-shot generation using pre-trained models or adapter methods for encoding graph structure into pretrained language models.\\n\\nStrengths:\\n\\n1. Novel Approach: The BDMG framework introduces an innovative bi-directional approach to KG-to-text generation. By constructing sentence-level generation based on corresponding triples before generating graph-level text, the method addresses the common issue of incorporating incorrect KG triples for each sentence. This approach allows for more fine-grained control over the generation process and improves the overall accuracy of the generated text.\\n\\n2. State-of-the-Art Performance: The proposed method achieves impressive results, significantly outperforming previous state-of-the-art methods. The BDMG-11B model improves upon the prior SOTA (Plan Selection) by approximately 8.5 BLEU, 3.6 METEOR, and 7.4 ROUGE points. These substantial improvements demonstrate the effectiveness of the proposed framework.\\n\\n3. Backward Relation Extraction: The incorporation of a backward relation-extraction task enhances the correctness of relational information in the generated text. This bi-directional approach is a key innovation that contributes to the overall performance improvement.\\n\\n4. Effective Use of Large Language Models: The study leverages large language models (Flan T5) as the backbone, demonstrating the potential of adapting pre-trained models for specific tasks. This approach shows promise for enhancing performance in various NLP tasks.\\n\\n5. Clear Visualization: Figure 1 provides a clear example from the WebNLG dataset, helping to illustrate the task at hand. Figure 2 effectively visualizes the proposed bi-directional multi-granularity generation framework, enhancing the reader\\\\'s understanding of the method.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Limited Dataset: The study focuses solely on the WebNLG dataset. While this is a standard benchmark, evaluating the method on multiple datasets from different domains would provide stronger evidence of its generalizability.\\n\\n2. Lack of Human Evaluation: The paper relies entirely on automated metrics (BLEU, METEOR, ROUGE) for evaluation. Incorporating human evaluation of the generated text quality would provide valuable insights into factors like fluency, coherence, and factual correctness that automated metrics may not fully capture.\\n\\n3. Scalability Analysis: While the authors mention that their method is more applicable to larger KGs with multiple sentences, there is no detailed analysis of how the method performs as the size and complexity of the knowledge graphs increase. A thorough scalability analysis would strengthen the paper\\\\'s claims.\\n\\n4. Error Analysis: The paper would benefit from a more detailed error analysis, including qualitative examples of both successful and unsuccessful generations. This would provide deeper insights into the model\\\\'s strengths and weaknesses and guide future improvements.\\n\\n5. Abstract Specificity: The abstract could be more specific about the performance metrics used to evaluate the model\\\\'s effectiveness, which are detailed in Table 1. Additionally, briefly mentioning the specific modules or components of the proposed framework in the abstract would provide a more comprehensive overview.\\n\\n6. Baseline Explanation: It would be helpful to include a brief explanation of the \"Plan-Selection\" baseline mentioned in Table 1\\\\'s caption, as it\\\\'s not described in the abstract or main text.\\n\\nSuggestions for Improvement:\\n\\n1. Conduct experiments on additional datasets to demonstrate the method\\\\'s generalizability across different domains and types of knowledge graphs.\\n\\n2. Incorporate human evaluation to assess the quality of generated text beyond automated metrics.\\n\\n3. Perform a detailed scalability analysis to show how the method performs on increasingly large and complex knowledge graphs.\\n\\n4. Provide a more comprehensive error analysis with qualitative examples of the model\\\\'s output.\\n\\n5. Enhance the abstract by including more specific details about the components of the proposed method and the evaluation metrics used.\\n\\n6. Ensure consistency in terminology throughout the paper, such as the hyphenation of \"backward relation-extraction task\" mentioned in the abstract and figures.\\n\\nIn conclusion, this paper presents a novel and promising approach to KG-to-text generation, demonstrating significant improvements over existing methods. While there are areas for further exploration and improvement, the bi-directional multi-granularity generation framework introduces valuable innovations to the field. Addressing the suggested improvements would further strengthen the paper\\\\'s contribution and impact."}
{"paper_id": "acl_2024_s59", "title": "Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s59.pdf", "human_reviewer": "", "barebones": "Here are my main feedback comments as a peer reviewer for this scientific paper:\\n\\n1. The research questions and methodology are clearly stated and appropriate for investigating zero-shot cross-lingual reranking with LLMs for low-resource African languages. The experimental setup using the CIRAL dataset and comparison of different reranking scenarios is well-designed.\\n\\n2. The results provide valuable insights into the effectiveness of LLMs for cross-lingual and monolingual reranking in low-resource settings. The finding that reranking in English via translation is generally most effective is noteworthy.\\n\\n3. The comparison of proprietary (RankGPT) and open-source (RankZephyr) LLMs is informative, showing the growing capabilities of open-source models. However, more analysis on why RankZephyr outperforms RankGPT 3.5 in some scenarios would be beneficial.\\n\\n4. The exploration of using LLMs\\\\' own translations for reranking is novel and produces interesting results, especially for RankGPT 4. Further analysis on why this works well despite lower BLEU scores would strengthen the paper.\\n\\n5. The discussion of limitations is thoughtful, but could be expanded to include potential solutions or future research directions to address these limitations.\\n\\n6. The paper would benefit from a more in-depth error analysis to understand where and why the LLM rerankers fail in certain scenarios. Some qualitative examples of successful and unsuccessful reranking could provide additional insights.\\n\\n7. While the focus on African languages is valuable, discussing how the findings may generalize to other low-resource languages would enhance the paper\\\\'s impact.\\n\\n8. The conclusion could be strengthened by more explicitly stating the key takeaways and practical implications of the research for developing IR systems for low-resource languages.\\n\\nOverall, this is a solid study that makes a meaningful contribution to understanding zero-shot cross-lingual reranking with LLMs. Addressing the above points would further improve an already strong paper.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   - Limited scope of languages studied\\n     \u2022 Only examines 4 African languages\\n     \u2022 May not generalize well to other low-resource languages\\n   - Reliance on translation\\n     \u2022 Effectiveness heavily dependent on translation quality\\n     \u2022 Does not fully address core issue of LLM performance on low-resource languages directly\\n   - Lack of comparison to more recent baselines\\n     \u2022 Only compares to mT5 and AfrimT5\\n     \u2022 Does not evaluate against state-of-the-art multilingual models\\n   - Insufficient analysis of computational costs\\n     \u2022 No discussion of inference time or resource requirements\\n     \u2022 Unclear if approach is practical for real-world deployment\\n\\n4. Suggestions for improvement\\n   - Expand language coverage\\n   - Explore translation-free approaches\\n   - Include comparisons to more recent baselines\\n   - Analyze computational requirements and practicality", "multi_agent_without_knowledge": "Critical Review of \"Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages\"\\n\\nThis paper presents a significant contribution to the field of cross-lingual information retrieval (CLIR) and low-resource language processing. The study investigates the effectiveness of large language models (LLMs) as zero-shot cross-lingual rerankers for four African languages: Hausa, Somali, Swahili, and Yoruba. The research is novel in its comprehensive evaluation of both proprietary (RankGPT 4 and RankGPT 3.5) and open-source (RankZephyr) LLMs in cross-lingual and monolingual retrieval settings.\\n\\nStrengths:\\n\\n1. Methodology: The study employs a robust experimental design, utilizing the CIRAL dataset specifically designed for CLIR in African languages. The use of both document translation (BM25-DT) and query translation (BM25-QT) approaches for first-stage retrieval, followed by listwise reranking using a sliding window technique, demonstrates a comprehensive approach to the problem.\\n\\n2. Novel Contributions: The paper introduces an innovative approach by evaluating the effectiveness of LLMs when leveraging their own generated translations for reranking. This provides valuable insights into the relationship between an LLM\\\\'s language understanding and its reranking capabilities.\\n\\n3. Significant Findings: The results show that cross-lingual reranking with LLMs is generally more effective than reranking in African languages. Notably, reranking entirely in English using document translation achieves the best results across all languages, with substantial improvements in nDCG@20 scores.\\n\\n4. Open-Source Model Performance: The comparable performance of the open-source RankZephyr model to RankGPT 4 in English reranking scenarios is a significant finding, highlighting the potential of open-source LLMs in this domain.\\n\\n5. Implications for Future Research: The study opens up several avenues for future research, including improving LLMs\\\\' multilingual capabilities and developing alternative reranking pipelines less reliant on translation.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Language Scope: While the focus on four African languages is valuable, the study could benefit from a clearer justification for the selection of these specific languages and a discussion on how representative they are of low-resource languages in general.\\n\\n2. Translation Dependency: The reliance on translations for achieving good reranking effectiveness introduces potential biases and dependencies on the quality of translation models. A more detailed analysis of how translation quality affects reranking performance would strengthen the paper.\\n\\n3. Model Selection: The paper could provide a clearer justification for the chosen LLMs and how they compare to other available models. A brief discussion in the background section about the selection criteria would be beneficial.\\n\\n4. Generalizability: The extent to which the findings can be applied to other low-resource languages or LLM types could be more thoroughly discussed. This would enhance the broader impact of the research.\\n\\n5. Alternative Approaches: While the paper mentions potential areas for future work, it could benefit from a more detailed discussion of alternative reranking pipelines that could reduce reliance on translation.\\n\\n6. Long-term Implications: The paper could elaborate further on the broader implications of the findings for IR systems in low-resource languages, providing a more comprehensive perspective on the potential impact of this research.\\n\\nRecommendations:\\n\\n1. Include a brief discussion in the introduction or methodology section about the selection criteria for the African languages used in the study.\\n\\n2. Add a subsection in the results that specifically analyzes the impact of translation quality on reranking performance, perhaps using different translation models for comparison.\\n\\n3. Expand the background section to include a more comprehensive overview of available LLMs and justify the selection of the models used in this study.\\n\\n4. In the conclusion or discussion section, add a paragraph addressing the generalizability of the findings and potential implications for other low-resource languages.\\n\\n5. In the future work section, provide more specific suggestions for alternative reranking pipelines that could reduce translation dependency.\\n\\n6. Consider adding a brief discussion on the long-term implications of this research for the development of IR systems in low-resource languages.\\n\\nIn conclusion, this paper makes a significant contribution to the field of cross-lingual information retrieval", "multi_agent_with_knowledge": "Critical Review of \"Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages\"\\n\\nThis paper presents a novel and comprehensive study on the effectiveness of large language models (LLMs) as zero-shot cross-lingual rerankers for low-resource African languages. The research addresses a significant gap in the existing literature by focusing on Hausa, Somali, Swahili, and Yoruba, languages that have been understudied in the context of cross-lingual information retrieval (CLIR).\\n\\nStrengths:\\n\\n1. Novelty and Contribution: The study makes several important contributions to the field:\\n   - It is one of the first to thoroughly examine LLMs as cross-lingual retrievers and rerankers for low-resource African languages.\\n   - The research introduces a novel approach of using LLMs\\\\' own zero-shot translations for query translation in the reranking process.\\n   - The comparative analysis of proprietary (RankGPT 4, RankGPT 3.5) and open-source (RankZephyr) LLMs provides valuable insights into their relative effectiveness.\\n\\n2. Methodology: The experimental design is comprehensive and well-structured:\\n   - The study explores diverse scenarios, including cross-lingual, monolingual (English), and African language reranking.\\n   - The use of established baselines (BM25, mT5, and AfrimT5) provides context for assessing LLM performance.\\n   - The sliding window technique and varying context sizes for different LLMs demonstrate attention to implementation details.\\n\\n3. Results and Analysis: The paper presents clear and insightful findings:\\n   - The study reports significant performance gains, with up to 7 points improvement in nDCG@20 over cross-lingual reranking.\\n   - The analysis of LLM translation quality and its impact on reranking effectiveness provides valuable insights into the relationship between translation and retrieval performance.\\n   - The finding that reranking in English via document translation is generally more effective has important implications for CLIR system design.\\n\\n4. Impact: The research has substantial potential impact on the field:\\n   - It advances our understanding of LLM capabilities in low-resource language settings.\\n   - The study provides a benchmark for future research in cross-lingual reranking for African languages.\\n   - The findings have practical applications in improving information retrieval systems for low-resource languages.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Scope: While the study\\\\'s focus on four African languages is valuable, expanding to a broader range of low-resource languages from different language families would enhance the generalizability of the findings.\\n\\n2. Model Diversity: The study primarily focuses on GPT-based models. Incorporating a wider range of open-source and multilingual LLMs would provide a more comprehensive comparison.\\n\\n3. Translation Dependency: The heavy reliance on translation quality introduces potential biases. Developing and evaluating translation-free approaches could be a valuable direction for future research.\\n\\n4. Experimental Robustness: The use of single-run experiments for most results may not account for potential variability in LLM performance. Multi-run experiments could provide more robust findings.\\n\\n5. Error Analysis: A more detailed analysis of error patterns across languages and LLMs could offer deeper insights into the strengths and weaknesses of different approaches.\\n\\n6. Clarity and Accessibility: While generally well-written, some sections use highly technical language that might be challenging for readers not familiar with the field. Providing brief explanations or a glossary for key terms could improve accessibility.\\n\\n7. Visual Representation: The paper mentions figures (e.g., Figure 1 and Figure 2) in the text, but these are not included in the provided content. Including these figures would significantly enhance the clarity of the methodology explanation, particularly for the prompt designs. Clear, well-designed figures can greatly aid in conveying complex information and methodologies.\\n\\nSuggestions for Improvement:\\n\\n1. Expand the language coverage to include a wider range of low-resource languages from different language families to enhance generalizability.\\n\\n2. Incorporate a broader range of open-source and multilingual LLMs for a more comprehensive comparison.\\n\\n3. Develop and evaluate translation-free approaches to reduce reliance on translation quality.\\n\\n4. Conduct multi-run experiments to account for potential variability in LLM performance and provide more robust findings.\\n\\n5. Perform a detailed error analysis to identify specific patterns or challenges across languages and LLMs.\\n\\n6. Include a glossary or provide brief explanations for technical terms to improve accessibility for a broader audience.\\n\\n7. Add the mentioned figures (Figure 1 and Figure 2) to visually represent the methodology, especially the prompt designs. Ensure these figures are clear, informative, and well-integrated with the text.\\n\\n8. Consider exploring the long-term implications of using LLMs for cross-lingual reranking, including potential biases and ethical considerations.\\n\\nIn conclusion, this paper represents a significant and novel contribution to the field of cross-lingual information retrieval for low-resource languages. Its focus on African languages and the use of LLMs for zero-shot reranking fills an important gap in the literature. While there are areas for improvement, particularly in expanding the scope and enhancing the robustness of the experiments, the research provides valuable insights and sets a strong foundation for future work in this critical area of natural language processing and information retrieval."}
{"paper_id": "acl_2024_s32", "title": "ATLAS: Improving Lay Summarisation with Attribute-based Control", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s32.pdf", "human_reviewer": "", "barebones": "Here are some key feedback comments on the paper, focusing on major points:\\n\\n1. Novelty and contribution: The paper presents a novel approach (ATLAS) for controllable lay summarization of scientific articles using multiple attribute-based controls. This addresses an important gap in being able to tailor summaries for different audiences. The use of multiple fine-grained controls appears to be an advancement over previous work.\\n\\n2. Methodology: The approach of using multiple control attributes (length, readability, background info, content word entropy) is well-motivated and explained. The method for incorporating these controls into the BART model is described clearly.\\n\\n3. Experimental design: The experiments are comprehensive, using multiple datasets, baselines, and evaluation metrics. The inclusion of human evaluation is a strength. The ablation study helps demonstrate the contribution of different attributes.\\n\\n4. Results: The results show ATLAS outperforming baselines on most metrics across datasets. The human evaluation results further support the effectiveness of the approach. \\n\\n5. Analysis: The analysis of the discriminatory power of the attributes and the controllability analysis provide good insights into how the method works.\\n\\n6. Limitations: The authors acknowledge some limitations, such as potential for additional useful attributes. Consider expanding on other limitations or potential drawbacks of the approach.\\n\\n7. Broader impact: The paper could benefit from a discussion of potential applications and broader impacts of this technology.\\n\\n8. Clarity: Overall the paper is well-written and organized. Some technical details in the methodology section could potentially be clarified further.\\n\\n9. Future work: The conclusion mentions plans for future work, but this could be expanded with more specific directions.\\n\\n10. Reproducibility: Code availability is not mentioned - providing code would aid reproducibility.\\n\\nOverall, this appears to be a strong paper presenting a novel and effective approach to an important problem in scientific communication. The extensive experiments and analysis provide good support for the claims made.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   a. Limited novelty in the overall approach\\n     \u2022 Uses existing BART model as base architecture\\n     \u2022 Control tokens concept similar to prior work in text style transfer\\n   b. Lack of comparison to more recent baselines\\n     \u2022 Does not compare to latest controllable summarization methods\\n     \u2022 Misses comparison to large language models like GPT-3\\n   c. Insufficient ablation studies and analysis\\n     \u2022 Limited analysis of individual attribute contributions\\n     \u2022 No exploration of different attribute combinations\\n   d. Concerns about generalizability\\n     \u2022 Focuses only on biomedical domain\\n     \u2022 Does not test on other scientific fields or general domain texts\\n\\n4. Suggestions for improvement", "multi_agent_without_knowledge": "Critical Review of \"ATLAS: Improving Lay Summarisation with Attribute-based Control\"\\n\\nThis paper introduces ATLAS (ATtribute-controlled LAy Summarization), a novel approach to scientific lay summarization that allows for fine-grained control over the \"layness\" of generated summaries. The authors present a well-structured and clearly written study that makes significant contributions to the field of controllable text generation and scientific communication.\\n\\nStrengths:\\n\\n1. Innovative Approach: ATLAS addresses a critical gap in existing research by introducing four controllable attributes (Length, Readability, Background information, and Content word entropy) to influence the generation of lay summaries. This approach allows for customization of summaries to meet the needs of diverse audiences, from complete non-experts to scientists in related fields.\\n\\n2. Strong Performance: The paper demonstrates that ATLAS outperforms state-of-the-art baselines, including recent GPT-3.5 based approaches, in both automatic and human evaluations. This is particularly evident for more \"lay\" summaries, such as those in the eLife dataset.\\n\\n3. Comprehensive Evaluation: The authors conduct a thorough evaluation of their model, including ablation studies, performance comparisons with various baselines, and human evaluations. This multi-faceted approach provides strong evidence for the effectiveness of ATLAS.\\n\\n4. Empirical Validation: The study provides empirical evidence for the effectiveness of the chosen control attributes in capturing the differences between summary types, contributing to the theoretical understanding of what constitutes \"layness\" in scientific writing.\\n\\n5. Potential Impact: ATLAS has significant potential applications in improving science communication, assisting researchers in understanding work from adjacent fields, enhancing educational materials, and supporting policymakers and journalists in understanding and reporting scientific findings.\\n\\nLimitations and Suggestions for Improvement:\\n\\n1. Limited Human Evaluation: The study conducted a human evaluation with only two experts, which may limit the reliability of the qualitative assessment. A larger-scale human evaluation involving more experts and potentially non-expert readers could provide more robust and comprehensive results.\\n\\n2. Exploration of Attribute Combinations: While the ablation study examined the contribution of individual attributes, a more comprehensive exploration of different attribute combinations could potentially reveal optimal configurations for different summary types or audience needs.\\n\\n3. Comparison with Previous Work: The paper mentions previous work on readability-controlled scientific summarization (Luo et al., 2022) but does not provide a direct comparison. Including such a comparison would help readers better understand the relative improvement offered by ATLAS.\\n\\n4. Technical Term Explanations: Some technical terms (e.g., \"FKGL\", \"ROUGE\") are used without full explanation, which might be challenging for readers less familiar with the field. Providing brief explanations or a glossary could improve the paper\\\\'s accessibility.\\n\\n5. Expanded Limitations Discussion: The limitations section in the conclusion could be expanded to provide a more critical analysis of the work, including potential biases in the training data or limitations in the control attributes chosen.\\n\\n6. Long-term Evaluation: While the paper demonstrates impressive short-term results, it would be valuable to discuss potential long-term implications or challenges of using ATLAS in real-world scientific communication scenarios.\\n\\nConstructive Criticism:\\n\\n1. To address the limited human evaluation, the authors should consider conducting a larger-scale study involving more experts and non-expert readers. This could provide more robust evidence for ATLAS\\\\'s effectiveness across different audience types.\\n\\n2. The paper would benefit from a more comprehensive exploration of attribute combinations. The authors could conduct experiments testing various combinations of attribute settings to identify optimal configurations for different summary types or audience needs.\\n\\n3. To strengthen the paper\\\\'s contribution, the authors should include a direct comparison with previous work on readability-controlled scientific summarization, such as Luo et al. (2022). This would help readers understand the relative improvement offered by ATLAS.\\n\\n4. The authors should consider adding brief explanations or a glossary for technical terms to improve the paper\\\\'s accessibility to a broader audience.\\n\\n5. The limitations section could be expanded to include a more critical analysis of potential biases in the training data, limitations of the chosen control attributes, and challenges in applying ATLAS to different scientific domains.\\n\\nIn conclusion, \"ATLAS: Improving Lay Summarisation with Attribute-based Control\" presents a significant advancement in controllable scientific lay summarization. The proposed approach demonstrates strong performance and has the potential", "multi_agent_with_knowledge": "Here is a revised comprehensive critical review of the ATLAS paper that incorporates the novelty and figure assessments:\\n\\nComprehensive Critical Review of ATLAS: Improving Lay Summarisation with Attribute-based Control\\n\\nThe ATLAS paper presents a novel approach to scientific lay summarization that addresses a significant gap in the field: the need for fine-grained control over the comprehensibility aspects of generated summaries. This review will assess the paper\\\\'s strengths, limitations, and overall contribution to the field of controllable lay summarization.\\n\\nNovelty and Innovation:\\nATLAS introduces a flexible and innovative method for controlling multiple attributes of lay summaries simultaneously. This approach represents a significant advancement over previous work in the field of scientific lay summarization. The novelty of ATLAS lies in its incorporation of controllable attributes to tailor summaries for different audience needs, which is a substantial innovation compared to more general approaches or those using simple binary control tokens.\\n\\nThe use of targeted control attributes to adjust the \"layness\" of summaries offers a more flexible and potentially more effective solution to the lay summarization challenge than previous methods, such as two-stage fine-tuning or general scientific summarization. This targeted control mechanism allows for customizing summaries based on audience expertise and goals, addressing a gap that previous work identified as a limitation for large language models in biomedical lay summarization.\\n\\nMethodology:\\nATLAS identifies four key attributes for control: length, readability, background information, and content word entropy. The method for discretizing attribute values into bins and incorporating them as control tokens is clearly explained and appears to be an effective approach for enabling fine-grained control.\\n\\nThe use of BART-base as the foundation model, combined with the attribute-based control mechanism, is well-justified and builds upon established techniques in the field.\\n\\nExperimental Setup and Evaluation:\\nThe experimental setup is robust, utilizing a combination of biomedical lay summarization datasets (eLife and PLOS) with varying levels of \"layness.\" This choice of datasets allows for a comprehensive evaluation of ATLAS\\\\'s ability to generate summaries across different levels of technicality.\\n\\nThe evaluation metrics used (ROUGE, BERTScore, Dale-Chall Readability Score) are appropriate and widely accepted in the field. The inclusion of human evaluation adds significant value to the results, as it provides insights into the real-world effectiveness of the generated summaries.\\n\\nResults and Performance:\\nATLAS demonstrates impressive performance, outperforming state-of-the-art baselines in both automatic and human evaluations across different summary types. The model\\\\'s ability to exceed the performance of all baseline approaches for eLife lay summaries and abstracts is particularly noteworthy, as it showcases ATLAS\\\\'s versatility in handling varying levels of technicality.\\n\\nThe ablation study provides valuable insights into the contribution of each attribute to the model\\\\'s performance. The finding that length-based control has the highest impact on ROUGE scores for lay summaries is interesting and aligns with the known differences in summary lengths between datasets.\\n\\nVisualization and Clarity:\\nFigure 1 in the paper effectively visualizes the density distributions of controllable attribute values for different summary types. This visualization supports the paper\\\\'s focus on controlling various properties that contribute to the \"layness\" of generated summaries. The multi-modal distribution shown in the graph suggests different summary types have distinct attribute profiles, which is consistent with the paper\\\\'s goal of tailoring summaries for different audiences.\\n\\nHowever, there are some areas where the clarity of the figures could be improved:\\n1. Ensure all axis labels are fully visible and include a y-axis label for clarity.\\n2. Add a legend to explicitly state what each color represents in the graphs.\\n3. Provide more context for each figure and table in the full paper to enhance the reader\\\\'s understanding of the ATLAS approach and its performance.\\n\\nStrengths:\\n1. Novel multi-attribute control mechanism for lay summarization\\n2. Strong performance across different levels of summary technicality\\n3. Comprehensive evaluation using both automatic metrics and human assessment\\n4. Effective demonstration of the model\\\\'s controllability through ablation studies and readability analysis\\n5. Ability to cater to users with different levels of expertise\\n\\nLimitations and Areas for Improvement:\\n1. While the authors acknowledge that there may be additional attributes that could benefit controllable lay summarization, they do not explore this in depth. Future work could investigate other potential attributes to further enhance the model\\\\'s capabilities.\\n2. The paper could benefit from a more detailed error analysis, examining cases where ATLAS underperforms or generates inaccurate summaries.\\n3. The human evaluation, while valuable, is limited to a sample of 10 articles per dataset. A larger-scale human evaluation could provide more robust insights into the model\\\\'s real-world performance.\\n4. The paper does not extensively discuss the potential ethical implications or biases that might arise from using such a controllable summarization system.\\n5. Cross-domain generalization is not explored, as the study focuses solely on biomedical datasets.\\n6. The abstract could be more specific about the methods, datasets, and evaluation metrics used to improve clarity and strengthen the connection to the visual elements.\\n\\nImpact and Contribution:\\nATLAS represents a significant advancement in the field of controllable lay summarization. By enabling fine-grained control over multiple attributes, it addresses the practical need for tailoring scientific summaries to different audience needs. The model\\\\'s strong performance across various summary types demonstrates its potential for real-world applications in making scientific literature more accessible to diverse audiences.\\n\\nThe paper\\\\'s thorough experimental setup and evaluation provide a solid foundation for future research in this area. The insights gained from the ablation studies and readability analysis contribute valuable knowledge to the understanding of what makes an effective lay summary.\\n\\nSuggestions for Authors:\\n1. Consider exploring additional control attributes that could further enhance the model\\\\'s capabilities.\\n2. Provide a more detailed error analysis to help readers understand the current limitations of the approach.\\n3. Conduct a larger-scale human evaluation to strengthen the reliability of the findings.\\n4. Discuss potential ethical implications and biases of the controllable summarization system.\\n5. Explore cross-domain generalization to demonstrate the model\\\\'s applicability beyond biomedical datasets.\\n6. Improve the clarity of figures by ensuring all labels are visible and adding legends where appropriate.\\n7. In the abstract, briefly mention the specific datasets used (PLOS and eLife) and the inclusion of human evaluation to better align with the paper\\\\'s content.\\n\\nIn conclusion, ATLAS presents a novel and promising approach to controllable lay summarization that addresses important challenges in the field. While there are areas for improvement and further exploration, the paper makes a significant contribution to the advancement of scientific communication and accessibility."}
{"paper_id": "acl_2024_s21", "title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s21.pdf", "human_reviewer": "", "barebones": "Here are some major feedback comments on the scientific paper as a peer reviewer:\\n\\n1. Methodology:\\n- The proposed approach of integrating gaze data into pre-trained language models during fine-tuning is novel and interesting. The method is clearly explained and illustrated.\\n- The use of synthetic scanpaths generated by a model is a clever way to address the scarcity of real gaze data. However, more discussion on potential limitations of using synthetic vs. real gaze data would be valuable.\\n\\n2. Experimental design:\\n- The evaluation on the GLUE benchmark provides a comprehensive assessment across multiple NLP tasks.\\n- The low-resource experiments are particularly valuable for demonstrating the benefits of the approach in limited data scenarios.\\n- Comparing to both standard fine-tuning and EDA baselines strengthens the evaluation.\\n\\n3. Results:\\n- The consistent improvements over baselines, especially in low-resource settings, are promising.\\n- The ablation studies provide useful insights into the importance of scanpath ordering and module placement.\\n\\n4. Analysis:\\n- More in-depth analysis of why the approach works better for some tasks than others would be valuable.\\n- Further discussion on why RoBERTa sees smaller gains compared to BERT would be interesting.\\n\\n5. Limitations:\\n- The limitations section is commendable, highlighting important caveats about the scanpath generation model used.\\n- Expanding on potential solutions or future work to address these limitations would strengthen the paper.\\n\\n6. Broader impact:\\n- The ethics statement raises important points about privacy concerns with gaze data.\\n- Some discussion of potential positive societal impacts of this work could be added.\\n\\n7. Writing and organization:\\n- The paper is generally well-written and structured logically.\\n- Some sections (e.g. results) could potentially be tightened to improve readability.\\n\\nOverall, this is a solid paper presenting an innovative approach to integrating cognitive information into language models. The thorough evaluation and analysis support the claims made. Addressing the points above could further strengthen the contribution.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n- Integration of gaze data into pre-trained language models during fine-tuning\\n- Novel approach to incorporating cognitive information without additional inference-time costs\\n\\n2. Potential reasons for acceptance\\n- Improved performance on GLUE benchmark, especially in low-resource settings\\n- Compatibility with existing LM pipelines\\n- Thorough ablation studies and analysis\\n\\n3. Potential reasons for rejection\\n\\n- Limited novelty in overall approach\\n  \u2022 Builds on existing work of integrating gaze data into NLP models\\n  \u2022 Core idea of reordering embeddings based on scanpaths not entirely new\\n\\n- Concerns about generalizability \\n  \u2022 Reliance on synthetic gaze data from a single, limited corpus\\n  \u2022 Performance gains more pronounced for BERT than RoBERTa\\n\\n- Lack of theoretical justification\\n  \u2022 Limited explanation of why gaze information improves language understanding\\n  \u2022 No clear connection drawn between cognitive processes and model improvements\\n\\n- Incomplete comparison to state-of-the-art\\n  \u2022 No comparison to other recent approaches for improving LM fine-tuning\\n  \u2022 Limited exploration of alternative gaze integration methods\\n\\n4. Suggestions for improvement\\n- Expand experiments to include more diverse language models and tasks\\n- Provide deeper analysis of how gaze information influences model behavior\\n- Compare to a wider range of baselines and state-of-the-art fine-tuning methods\\n- Explore potential of using real gaze data in addition to synthetic data", "multi_agent_without_knowledge": "Here is the final comprehensive review of the paper \"Fine-Tuning Pre-Trained Language Models with Gaze Supervision\":\\n\\nTitle: Fine-Tuning Pre-Trained Language Models with Gaze Supervision\\n\\nThis paper presents an innovative approach to enhancing the performance of pre-trained language models by incorporating human gaze data during the fine-tuning process. The authors propose a method that integrates a scanpath module into pre-trained language models, aiming to improve their ability to learn representations grounded in human language processing. The review will cover the paper\\\\'s strengths, limitations, and potential impact on the field of natural language processing.\\n\\nStrengths:\\n\\n1. Novel Approach: The paper introduces a unique method of incorporating gaze data into pre-trained language models during fine-tuning. This approach bridges the gap between human cognitive processes and machine learning models, potentially leading to more human-like language understanding.\\n\\n2. Performance Improvements: The proposed method consistently outperforms standard fine-tuning and Easy Data Augmentation (EDA) baselines across various tasks in the GLUE benchmark. Notably, the improvements are more pronounced in low-resource settings, with performance gains of 2-3% for BERT and 1-2% for RoBERTa over standard fine-tuning.\\n\\n3. Versatility: The method shows improvements across a wide range of NLP tasks, including sentiment analysis, linguistic acceptability, similarity and paraphrase tasks, and natural language inference tasks.\\n\\n4. Efficiency: The scanpath module is only active during training, ensuring compatibility with existing pre-trained LM-based pipelines and maintaining efficiency during deployment.\\n\\n5. Thorough Experimentation: The authors conduct comprehensive experiments, including ablation studies and comparisons with various baselines, providing strong evidence for the effectiveness of their approach.\\n\\n6. Potential for Low-resource Languages: The method shows particular promise for low-resource tasks and languages, potentially addressing a significant challenge in NLP.\\n\\nLimitations and Areas for Improvement:\\n\\n1. Limited Training Data for Scanpath Generation: The Eyettention model used for scanpath generation was pre-trained on a single eye-tracking corpus with a relatively small sample size and limited domain coverage. This may restrict the benefits of integrating simulated gaze data into language models.\\n\\n2. Varying Performance Gains: The improvements achieved by incorporating gaze supervision vary across different NLP tasks and models. For instance, the benefits are smaller for RoBERTa compared to BERT, especially in low-resource settings.\\n\\n3. Dependency on Synthetic Gaze Data: While the use of synthetic gaze data alleviates the need for large-scale human experiments, it may not fully capture the complexity and variability of real human gaze patterns.\\n\\n4. Ethical Considerations: The paper acknowledges potential privacy risks associated with the collection and processing of human gaze data, which could lead to the extraction of sensitive information.\\n\\nImpact and Future Directions:\\n\\nThe paper makes a significant contribution to the field of NLP by demonstrating the potential of integrating cognitive information into language models. The proposed method opens up new avenues for improving language model performance, particularly in low-resource scenarios. Future work could focus on:\\n\\n1. Developing more robust scanpath generation models trained on larger and more diverse eye-tracking datasets.\\n2. Exploring the integration of other types of cognitive data into language models.\\n3. Investigating the specific impact of gaze supervision on different NLP tasks to understand where it is most effective.\\n4. Addressing the ethical concerns surrounding gaze data collection and usage.\\n\\nIn conclusion, this paper presents a promising approach to enhancing language models by grounding them in human cognitive processes. While there are limitations to be addressed, the method shows significant potential for improving NLP task performance, especially in low-resource settings. The work contributes to the broader goal of creating more human-like language understanding in artificial intelligence systems.", "multi_agent_with_knowledge": "Here is a revised comprehensive review of the paper \"Fine-Tuning Pre-Trained Language Models with Gaze Supervision,\" incorporating the novelty and figure assessments:\\n\\nComprehensive Review of \"Fine-Tuning Pre-Trained Language Models with Gaze Supervision\"\\n\\n1. Overview and Novelty:\\nThis paper presents an approach to enhancing pre-trained language models by incorporating gaze data during fine-tuning. The authors propose integrating a gaze module into language models to improve their ability to learn representations grounded in human language processing. While the integration of cognitive signals into language models is not entirely new, the specific application of gaze data in fine-tuning pre-trained models represents a novel direction in this field.\\n\\nHowever, it\\\\'s important to note that the novelty of this approach is somewhat limited when compared to existing work. A previous paper, \"Cognitive Information Bottleneck: Extracting Minimal Sufficient Cognitive Language Processing Signals,\" presents a more comprehensive and generalizable method for integrating cognitive signals, including eye-tracking data, into language models. The current paper\\\\'s contribution, while valuable, appears to be more incremental rather than groundbreaking in the context of this existing work.\\n\\n2. Methodology:\\nThe methodology is clearly explained, detailing the scanpath integration process and the training objective. Key aspects include:\\na) Extension of the standard fine-tuning objective with an auxiliary loss incorporating synthetic scanpaths.\\nb) Use of a scanpath module that is active only during the fine-tuning stage, ensuring compatibility with existing pre-trained language model pipelines.\\nc) Rearrangement of token embeddings based on simulated fixation sequences.\\n\\n3. Experiments and Results:\\nThe experimental setup uses the GLUE benchmark to evaluate the proposed method in both low-resource and high-resource settings. Key findings include:\\na) Consistent performance improvements over baselines, especially in low-resource scenarios.\\nb) Performance gains of 2-3% for BERT and 1-2% for RoBERTa over standard fine-tuning.\\nc) Superior performance on tasks like CoLA and STS-B, where Easy Data Augmentation (EDA) yields inferior results.\\n\\nThe authors also conducted thorough ablation studies, exploring the impact of the scanpath module\\\\'s location within the model architecture and the importance of the fixation order.\\n\\n4. Strengths:\\na) Integration of gaze data into language model fine-tuning, potentially bridging human cognition and machine learning.\\nb) Computational efficiency: the gaze module is only active during training.\\nc) Consistent performance improvements across various NLP tasks, particularly in low-resource scenarios.\\nd) Comprehensive evaluation on the GLUE benchmark and detailed ablation studies.\\ne) Clear and well-structured presentation of the methodology and results.\\n\\n5. Limitations and Areas for Improvement:\\na) The novelty of the approach is somewhat limited compared to existing work on integrating cognitive signals into language models.\\nb) The abstract lacks specific terminology used in the figures/tables, such as \"scanpath,\" \"GLUE benchmark,\" and \"fixation sequences,\" which could improve clarity and connection between the abstract and visual elements.\\nc) There\\\\'s a discrepancy between the abstract\\\\'s focus on human gaze data and the figure\\\\'s mention of \"simulated fixation sequence.\" This should be clarified or reconciled.\\nd) The captions for Tables 3 and 4 are very brief and lack context, making it difficult to understand their relation to the abstract\\\\'s claims.\\ne) The scanpath-generation model (Eyettention) was pre-trained on a limited dataset, potentially restricting its knowledge of human language processing.\\nf) Performance gains vary across different NLP tasks, indicating that the method may not be equally beneficial for all types of language understanding tasks.\\n\\n6. Suggestions for Improvement:\\na) Clarify in the abstract whether the study uses actual human gaze data or simulated data, as this seems to be an important distinction.\\nb) Include more specific terminology from the figures/tables in the abstract to better connect it with the visual elements.\\nc) Briefly mention the comparison strategies (e.g., standard fine-tuning, text augmentation baselines) in the abstract to set up the context for the tables.\\nd) Expand on the captions for Tables 3 and 4 to provide more context about what is being compared and how it relates to the main claims of the paper.\\ne) Consider including a brief mention of the different sample sizes (K = 200, 500, 1000) in the abstract, as this seems to be an important aspect of the experimental design.\\nf) Explore the use of more diverse and larger eye-tracking datasets to improve the scanpath-generation model.\\ng) Conduct a more detailed analysis of the impact of incorporating cognitive information on specific linguistic phenomena or task types.\\nh) Investigate the application of this method to other types of language models, including more recent architectures.\\n\\n7. Ethical Considerations:\\nThe paper addresses potential privacy risks associated with collecting and processing human gaze data, highlighting the possibility of extracting sensitive information from eye movements. The use of synthetic gaze data is proposed as a partial solution to mitigate these risks.\\n\\n8. Impact and Future Directions:\\nWhile the work contributes to the broader effort of grounding NLP models in human cognitive processes, its impact should be considered in the context of existing research in this area. The effectiveness in low-resource settings could be particularly valuable for advancing NLP capabilities in less-resourced languages or specialized domains.\\n\\nFuture research directions could include:\\na) Comparing this approach more extensively with other cognitive-inspired methods to better contextualize its significance.\\nb) Exploring the potential of combining gaze data with other forms of cognitive or behavioral data.\\nc) Investigating specific real-world applications where this approach could provide significant benefits over existing methods.\\n\\nIn conclusion, while this paper presents an interesting approach to integrating gaze data into language model fine-tuning, its novelty and impact are somewhat limited in the context of existing research. However, the work still contributes valuable insights and opens up potential avenues for further research in cognitive-inspired NLP."}
{"paper_id": "acl_2024_s19", "title": "Two Issues with Chinese Spelling Correction and A Refinement Solution", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s19.pdf", "human_reviewer": "", "barebones": "Here is a list of major feedback comments for the scientific paper, as a peer reviewer would provide:\\n\\n1. The paper addresses an important issue in Chinese Spelling Correction (CSC) research by identifying and fixing mistakes in widely-used benchmark datasets. This contribution is significant and valuable to the research community.\\n\\n2. The methodology for fixing the SIGHAN datasets is generally sound, with two authors independently examining and discussing corrections. However, more details on the criteria used to determine fixes would strengthen the paper.\\n\\n3. The re-evaluation of four representative CSC models using the fixed datasets provides important insights. The drastic changes in results, especially for SIGHAN13, highlight the importance of this work.\\n\\n4. The proposed refinement solution using ChineseBERT is innovative and shows promising results. However, the rationale behind some of the thresholds used (e.g., edit distance > 3 for over-corrections) could be better explained or supported with empirical evidence.\\n\\n5. The analysis of common failure types among the models is insightful and well-presented. This information will be valuable for future research in CSC.\\n\\n6. The limitations section is commendable for its honesty and clarity. The authors appropriately acknowledge the need to evaluate more diverse models and improve the accuracy of character masking in future work.\\n\\n7. The paper would benefit from a more detailed discussion of the implications of these findings for the CSC field as a whole. How might this impact future research directions or evaluation practices?\\n\\n8. Some of the tables are missing or incomplete in the provided text. Ensure all tables are properly formatted and included in the final submission.\\n\\n9. The conclusion could be strengthened by more explicitly stating the broader impact of this work and potential next steps for the research community.\\n\\n10. Consider expanding on the related work section to provide a more comprehensive context for this study within the field of CSC research.\\n\\nOverall, this paper makes a valuable contribution to the field of Chinese Spelling Correction by addressing important issues with benchmark datasets and proposing an effective refinement solution. With some minor revisions and clarifications, it should be suitable for publication.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   a. Lack of theoretical contribution\\n      - No new modeling approach or algorithm proposed\\n      - Limited innovation beyond dataset cleaning and simple post-processing\\n   \\n   b. Insufficient experimental evaluation\\n      - Only four existing models evaluated\\n      - No comparison to very recent state-of-the-art approaches\\n   \\n   c. Questionable generalizability of the refinement solution\\n      - Rule-based approach may not generalize well to other datasets/domains\\n      - Relies on specific properties of Chinese characters and existing tools\\n\\n   d. Inadequate analysis of dataset issues and fixes\\n      - No rigorous quantification of dataset quality improvements\\n      - Potential for introducing new biases during manual correction process\\n\\n4. Suggestions for improvement", "multi_agent_without_knowledge": "Comprehensive Critical Review of \"Two Issues with Chinese Spelling Correction and A Refinement Solution\"\\n\\nSummary:\\nThis paper addresses two critical issues in Chinese Spelling Correction (CSC) research: inaccuracies in benchmark datasets and a performance bottleneck in existing models. The authors make two primary contributions:\\n1. Manually fixing errors in the SIGHAN13, SIGHAN14, and SIGHAN15 datasets and re-evaluating four representative CSC models.\\n2. Proposing a simple yet effective refinement solution using ChineseBERT to improve model performance without additional training.\\n\\nExperimental Design and Methodology:\\nThe experimental design is thorough and well-structured:\\n- The authors conducted a two-round process to examine and fix the SIGHAN datasets, ensuring accuracy through multiple reviewers.\\n- They retrained and re-evaluated four representative CSC models (PLOME, REALISE, LEAD, and SCOPE) using the fixed datasets.\\n- Evaluation used widely-adopted sentence-level precision, recall, and F1 scores for both detection and correction sub-tasks.\\n- The proposed refinement solution using ChineseBERT addresses common failure types observed in the models\\\\' outputs.\\n\\nThe methodology is sound, with clear steps for dataset correction, model evaluation, and refinement. Multiple rounds of dataset correction and evaluation of multiple models strengthen the reliability of the results.\\n\\nResults and Significance:\\n- The fixed datasets revealed significant changes in model performance, particularly on SIGHAN13.\\n- Results on SIGHAN14 and SIGHAN15 generally increased after dataset fixes.\\n- The refinement solution improved all four models\\\\' performance across all metrics.\\n- The study highlighted the importance of accurate benchmark datasets in evaluating CSC models.\\n\\nClarity and Writing Quality:\\nThe paper is well-structured and clearly written. The authors effectively communicate their ideas, methodology, and findings. They provide a logical flow from introducing the issues to presenting results and explaining their refinement solution. The use of tables to present results and examples enhances the clarity of the paper.\\n\\nImpact on the Field:\\nThis paper makes significant contributions to CSC research:\\n- It addresses critical issues in widely-used benchmark datasets, potentially affecting the accuracy of previous research findings.\\n- The re-evaluation of existing models on corrected datasets provides a more accurate assessment of their performance.\\n- The proposed refinement solution offers a simple yet effective way to improve model performance without additional training.\\n- By identifying common failure types across multiple models, the paper provides insights for future research directions in CSC.\\n\\nStrengths:\\n1. Thorough examination and correction of benchmark datasets.\\n2. Comprehensive re-evaluation of multiple representative CSC models.\\n3. Novel refinement solution that improves performance across all evaluated models.\\n4. Clear presentation of methodology and results.\\n\\nLimitations:\\n1. The evaluation focuses on models that incorporate phonological and visual features of Chinese characters, potentially limiting the generalizability of findings.\\n2. The refinement solution relies on rule-based strategies for identifying characters to be masked, which may not be optimal for all cases.\\n3. The thresholds used in the refinement process are set empirically and may not be universally applicable.\\n4. The study does not extensively explore the reasons behind the performance bottleneck in existing models.\\n\\nSuggestions for Improvement and Future Work:\\n1. Expand the evaluation to include a more diverse set of CSC models, including those using detection-correction frameworks and incorporating contextual information.\\n2. Develop data-driven methods to improve the accuracy of identifying characters to be masked in the refinement process.\\n3. Investigate the integration of visual distance alongside phonological distance in the refinement solution.\\n4. Explore the applicability of the proposed refinement solution to other languages or related NLP tasks.\\n5. Conduct user studies or real-world evaluations to assess the practical impact of the improved CSC models on end-users or downstream applications.\\n6. Perform ablation studies to quantify the contribution of each component of the refinement solution.\\n7. Analyze the performance on different error types (e.g., phonological, visual, contextual) to identify areas where models still struggle even after refinement.\\n\\nConclusion:\\nThis paper makes valuable contributions to the field of Chinese Spelling Correction by addressing critical issues in benchmark datasets", "multi_agent_with_knowledge": "Review of \"Two Issues with Chinese Spelling Correction and A Refinement Solution\"\\n\\nThis paper addresses two critical issues in Chinese Spelling Correction (CSC): mistakes in widely-used benchmark datasets and performance bottlenecks in existing models. The authors make significant contributions by manually fixing the SIGHAN datasets and proposing a novel refinement solution. This review will discuss the paper\\\\'s strengths, limitations, and potential impact on the field.\\n\\nStrengths:\\n\\n1. Dataset Improvement: The authors\\\\' manual examination and correction of the SIGHAN datasets (SIGHAN13, SIGHAN14, SIGHAN15) address a fundamental issue in CSC research. This contribution is crucial for accurate evaluation of CSC models and sets a new standard for benchmark datasets in the field.\\n\\n2. Re-evaluation of Existing Models: By retraining and re-evaluating four representative CSC models (PLOME, REALISE, LEAD, SCOPE) using the fixed datasets, the paper provides a more accurate assessment of current state-of-the-art performance. This approach offers valuable insights into the true capabilities of existing CSC systems.\\n\\n3. Novel Refinement Solution: The proposed ChineseBERT-based refinement solution demonstrates notable improvements across multiple models without requiring additional training. This simple yet effective approach has potential for wide applicability in the field.\\n\\n4. Clear Presentation: The paper is well-structured and clearly presents its methodology and findings. The use of tables effectively demonstrates the performance improvements achieved through dataset fixing and the proposed refinement solution.\\n\\nNovelty:\\n\\nThe paper presents significant novelty in addressing fundamental issues in CSC research. The focus on improving benchmark datasets and proposing an effective refinement solution represents a meaningful advancement in the field. This approach differs from previous works that primarily focused on developing new model architectures or incorporating specific features. The paper\\\\'s contributions of fixing datasets, re-evaluating existing models, and proposing a refinement method based on error analysis offer a novel perspective on advancing CSC research.\\n\\nLimitations and Suggestions for Improvement:\\n\\n1. Limited Model Diversity: The study focuses on four models that incorporate phonological and visual features of Chinese characters. Future work should include a more diverse range of models, such as those using detection-correction frameworks or incorporating contextual information.\\n\\n2. Rule-based Refinement Solution: The current refinement strategy for identifying characters to be masked is rule-based and not very accurate. Exploring more complex, data-driven methods could enhance the robustness and accuracy of the refinement process.\\n\\n3. Empirical Thresholds: The thresholds used for identifying over-corrections and deciding whether to preserve inferred characters are set empirically. A more systematic approach to determining these thresholds could improve the refinement solution\\\\'s performance and generalizability.\\n\\n4. Visual Distance Consideration: The current method only considers phonological distance when making refinement decisions. Incorporating visual distance could potentially improve the accuracy of the refinement process, especially for visually similar characters.\\n\\n5. Deeper Analysis of Performance Bottleneck: While the paper identifies the performance bottleneck in recent CSC models, a more in-depth analysis of why certain errors persist across all models could provide valuable insights for future research directions.\\n\\n6. Lack of Figures: The paper relies heavily on tables to present results. Including some visual representations, such as graphs or charts, could enhance the reader\\\\'s understanding of the improvements and trends observed. This is particularly important for clearly illustrating the performance gains achieved through dataset fixing and the refinement solution.\\n\\nImpact:\\n\\nThe paper has the potential to significantly impact future CSC research and applications:\\n\\n1. Improved Benchmarking: The fixed SIGHAN datasets offer a more accurate benchmark for evaluating CSC models, potentially leading to more reliable comparisons between different approaches.\\n\\n2. Performance Boost: The proposed refinement solution demonstrates notable improvements across multiple models, which could lead to immediate performance gains in various CSC applications if widely adopted.\\n\\n3. New Research Directions: By highlighting the limitations of current CSC models and datasets, the paper could inspire new research directions in the field, such as developing more robust models or creating new, high-quality datasets.\\n\\n4. Practical Applications: Improved CSC systems could have wide-ranging applications in areas such as education, text processing, and natural language understanding for Chinese language.\\n\\nConclusion:\\n\\n\"Two Issues with Chinese Spelling Correction and A Refinement Solution\" makes valuable contributions to the field of Chinese Spelling Correction. The authors\\\\' work in improving benchmark datasets and proposing an effective refinement solution addresses critical issues in CSC research and represents a novel approach to advancing the field. While there are areas for improvement, such as expanding the diversity of evaluated models, refining the rule-based approach, and enhancing the visual presentation of results, the paper\\\\'s contributions are significant and have the potential to drive future research and improvements in CSC systems. The clear presentation of methodology and results, along with the novel approach to addressing fundamental issues in CSC, make this paper a valuable addition to the field."}
{"paper_id": "acl_2024_s16", "title": "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s16.pdf", "human_reviewer": "", "barebones": "Here are some key feedback comments for this scientific paper as a peer reviewer:\\n\\n1. The paper presents a novel and potentially impactful method (AFLoRA) for parameter-efficient fine-tuning of large language models. The adaptive freezing approach is interesting and seems to offer benefits in terms of reducing trainable parameters while maintaining or improving performance.\\n\\n2. The experimental results are extensive, covering multiple datasets and model types. The comparisons to existing methods like LoRA and ELoRA provide good context for the advantages of AFLoRA.\\n\\n3. The ablation studies help justify some of the design choices and provide insight into which components of the method are most important. The analysis of freezing trends across layers is particularly interesting.\\n\\n4. The paper could benefit from a more thorough discussion of limitations and potential drawbacks of the approach. For example, does the adaptive freezing add significant complexity or hyperparameters that need tuning?\\n\\n5. It would be helpful to include more details on the computational costs of AFLoRA compared to alternatives. While runtime and FLOP comparisons are provided, a more detailed breakdown of training time and memory usage would strengthen the paper.\\n\\n6. The motivation and intuition behind the freezing score could be explained in more depth. Why is gradient magnitude alone used rather than considering weight magnitudes as well?\\n\\n7. Some of the equations and technical details in the methodology section could be clarified further. For example, the reasoning behind the cubic schedule for freezing fraction could be elaborated on.\\n\\n8. The paper would benefit from a more extensive related work section, placing AFLoRA in the broader context of efficient fine-tuning approaches beyond just LoRA variants.\\n\\n9. While results on GLUE and a few other tasks are provided, testing on a wider range of NLP tasks would help demonstrate the general applicability of the method.\\n\\n10. The conclusion could be strengthened by discussing broader implications and potential future work building on AFLoRA.\\n\\nOverall, this appears to be a solid paper presenting a novel and promising approach, with rigorous experiments and analysis. Addressing the above points could further improve its impact and clarity.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n- Novel parameter-efficient fine-tuning method (AFLoRA) \\n- Adaptive freezing approach for low-rank adaptation\\n- Insights on trainability requirements of LoRA paths\\n\\n2. Potential reasons for acceptance\\n- State-of-the-art performance on GLUE benchmark\\n- Significant reduction in trainable parameters\\n- Improved runtime and FLOPs efficiency\\n- Extensive experiments and ablation studies\\n\\n3. Potential reasons for rejection\\n\\na) Limited theoretical justification\\n- Lack of formal analysis for freezing score effectiveness\\n- No proof of convergence or optimality for adaptive freezing approach\\n\\nb) Narrow experimental scope\\n- Focus primarily on GLUE benchmark tasks\\n- Limited evaluation on truly large language models\\n- No experiments on non-NLP tasks or other modalities\\n\\nc) Incremental improvement over existing methods\\n- Builds heavily on LoRA and ELoRA approaches\\n- Improvements in efficiency, but not drastically different conceptually\\n- May not be considered sufficiently novel for a top-tier venue\\n\\nd) Potential practical limitations\\n- Added complexity in implementation and hyperparameter tuning\\n- Unclear how method scales to extremely large models\\n- May require careful tuning of freezing schedule for each task/model\\n\\n4. Suggestions for improvement\\n- Provide stronger theoretical foundations for the approach\\n- Expand experiments to more diverse tasks and larger models\\n- Compare against a wider range of PEFT methods\\n- Investigate potential of combining with other efficiency techniques", "multi_agent_without_knowledge": "Based on the comprehensive review and additional suggestions, here is the final critical review of the AFLoRA paper:\\n\\nTitle: AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models\\n\\n1. Overview and Contributions:\\nAFLoRA introduces a novel parameter-efficient fine-tuning (PEFT) method that combines low-rank adaptation with adaptive freezing. The paper\\\\'s key contributions include:\\na) A new PEFT method that significantly reduces trainable parameters and computation costs.\\nb) Development of a novel \"freezing score\" to determine which projection matrices to freeze during training.\\nc) Extensive evaluation on GLUE benchmark and other tasks, demonstrating state-of-the-art performance with fewer trainable parameters.\\n\\n2. Methodology:\\nAFLoRA builds upon previous PEFT methods, introducing several key innovations:\\na) A trainable low-rank path that includes both projection matrices and feature transformation vectors.\\nb) Adaptive freezing based on a novel freezing score.\\nc) A cubic schedule for thresholding freezing scores.\\n\\nThe freezing score, calculated using smoothed gradients and uncertainty tensors, serves as a proxy for the trainability requirement of a LoRA tensor.\\n\\n3. Experimental Results:\\nThe paper presents comprehensive evaluations on multiple NLP benchmark datasets:\\na) On the GLUE benchmark, AFLoRA achieves state-of-the-art performance on most datasets, with an average improvement of up to 0.85% compared to LoRA while requiring 9.5\u00d7 fewer average trainable parameters.\\nb) AFLoRA yields 1.86\u00d7 and 2.96\u00d7 improvement in runtime and FLOPs, respectively, compared to ELoRA.\\nc) On large language model tasks (GSM8k and CNN/DailyMail), AFLoRA demonstrates improved accuracy with significantly fewer trainable parameters.\\n\\n4. Strengths:\\na) Novel approach combining low-rank adaptation with adaptive freezing.\\nb) Comprehensive evaluation across various benchmarks and tasks.\\nc) Significant efficiency gains in terms of trainable parameters and computation costs.\\nd) Insightful ablation studies providing valuable insights into the method\\\\'s components.\\n\\n5. Limitations and Suggestions for Improvement:\\na) Further exploration of freezing score metrics is needed, as alternative scoring methods outperform the proposed one on certain datasets.\\nb) The paper could benefit from a deeper analysis of layer-specific behaviors, particularly why keeping projection matrices trainable in FFN layers yields better performance.\\nc) A broader applicability study exploring AFLoRA\\\\'s performance on non-NLP tasks (e.g., computer vision, multimodal tasks) would strengthen the paper.\\nd) More detailed analysis of hyperparameter sensitivity, especially for the freezing schedule parameters, would be valuable for practitioners.\\ne) Long-term stability analysis comparing AFLoRA-fine-tuned models with full fine-tuning and other PEFT methods could provide additional insights.\\n\\n6. Future Research Directions:\\na) Investigate the potential of combining AFLoRA with other PEFT techniques or adaptive optimization methods.\\nb) Explore the applicability of the adaptive freezing concept to other model components beyond projection matrices.\\nc) Study the impact of AFLoRA on model interpretability and preserved knowledge from pre-training.\\nd) Consider integrating AFLoRA with adaptive rank evaluation frameworks to potentially open new directions for PEFT research.\\n\\n7. Conclusion:\\nAFLoRA represents a significant advancement in parameter-efficient fine-tuning of large language models. By combining low-rank adaptation with adaptive freezing, it achieves impressive performance improvements while substantially reducing computational requirements. The method\\\\'s effectiveness across various tasks and model sizes suggests broad applicability in the field of NLP.\\n\\nWhile the paper provides a strong foundation for this new approach, there are opportunities for further refinement and exploration. Addressing the suggested improvements and pursuing the proposed future research directions could further enhance AFLoRA\\\\'s impact and solidify its position as a valuable tool in the NLP toolkit.\\n\\nOverall, this paper makes a substantial contribution to the field of parameter-efficient fine-tuning and is likely to inspire future research in this important area of machine learning.", "multi_agent_with_knowledge": "Comprehensive Review of \"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models\"\\n\\n1. Summary and Contributions:\\n\\nThis paper introduces AFLoRA (Adaptive Freezing of Low Rank Adaptation), a novel parameter-efficient fine-tuning (PEFT) method for large language models. The key contributions include:\\n\\na) A new approach that starts with trainable low-rank matrices and feature transformation vectors, then gradually freezes projection matrices based on a novel freezing score.\\nb) Improved performance on GLUE and GSM8k benchmarks compared to existing PEFT methods.\\nc) Significant reduction in trainable parameters (up to 9.5\u00d7 fewer) while maintaining or improving performance.\\nd) Runtime improvements of up to 1.86\u00d7 and FLOPs improvements of up to 2.96\u00d7 compared to similar PEFT alternatives.\\n\\n2. Novelty and Impact:\\n\\nThe paper demonstrates moderate novelty through:\\n- Introduction of the \"freezing score\" concept as a proxy for trainability requirements.\\n- An adaptive freezing mechanism that alleviates over-fitting and improves computation efficiency.\\n- A balanced approach to performance, efficiency, and parameter count that addresses limitations of previous PEFT methods like LoRA and ELoRA.\\n\\nHowever, it\\\\'s important to note that while AFLoRA presents an innovative approach, it builds upon existing LoRA-based methods. The incremental nature of the improvements, while significant, may not constitute a fundamentally new paradigm in PEFT.\\n\\nThe impact of this work is potentially substantial, as it addresses critical challenges in fine-tuning large language models, reducing computational costs and memory requirements while maintaining or enhancing performance. This could have far-reaching implications for both research and practical applications of large language models.\\n\\n3. Methodology and Experimental Design:\\n\\nStrengths:\\n- Comprehensive evaluation on multiple NLP benchmark datasets, including GLUE and GSM8k.\\n- Thorough comparison with state-of-the-art PEFT methods.\\n- Use of popular large language models (LLaMA-7B and BART-Large) for additional experiments.\\n- Detailed ablation studies to validate design choices and analyze component impacts.\\n\\nLimitations:\\n- Focus primarily on NLP tasks; generalizability to other domains not explored.\\n- Lack of discussion on potential negative societal impacts or ethical considerations.\\n- The abstract claims improvements on both GLUE and GSM8k, but the provided results primarily focus on GLUE. More explicit results for GSM8k should be included to fully support the claims.\\n\\n4. Clarity and Organization:\\n\\nThe paper is well-structured, following a logical flow from introduction to conclusion. The methodology is explained in detail, and the experimental results are presented comprehensively. However, there are areas for improvement:\\n- Some mathematical notations could benefit from more explanation.\\n- Certain figures lack detailed captions, which could enhance reader interpretation.\\n- The abstract could be more aligned with the actual content of the paper, particularly in mentioning comparisons with LoRA and ELoRA, which feature prominently in the results.\\n\\n5. Figures and Visual Presentation:\\n\\nThe paper effectively uses figures to support its claims and methodology:\\n- Figure 1 clearly compares LoRA, ELoRA, and AFLoRA.\\n- Figures 2-5 effectively illustrate performance comparisons, freezing score methodologies, and freezing trends across layers.\\n\\nThese figures are well-integrated with the text, enhancing the overall clarity and impact of the research presentation. However, some improvements could be made:\\n- Include a visual representation of the adaptive freezing process and the novel freezing score calculation.\\n- Add a figure or table specifically showing the claimed 1.09% improvement, 9.5\u00d7 parameter reduction, and 1.86\u00d7 runtime improvement.\\n- Enhance labeling in figures, particularly Figure 5, by adding layer numbers and a color scale legend.\\n\\n6. Constructive Feedback and Suggestions:\\n\\na) Expand the evaluation to include tasks beyond NLP to demonstrate broader applicability.\\nb) Provide more detailed analysis of trade-offs between performance, efficiency, and model size.\\nc) Include discussion on potential limitations or failure cases of the AFLoRA method.\\nd) Enhance figure captions and explanations to improve overall clarity.\\ne) Consider discussing the environmental impact compared to full fine-tuning and other PEFT approaches.\\nf) Explore potential combinations of AFLoRA with other PEFT techniques for further improvements.\\ng) Address potential ethical considerations and societal impacts of the proposed method.\\nh) Ensure that all claims made in the abstract are fully supported by explicit results in the paper, particularly for the GSM8k benchmark.\\ni) Revise the abstract to better reflect the comparative nature of the study, mentioning LoRA and ELoRA to provide a more accurate preview of the paper\\\\'s content.\\nj) Consider adding a brief explanation of the \"freezing score\" concept in the abstract, as it seems to be a central part of the AFLoRA method.\\n\\nIn conclusion, \"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models\" presents a promising approach to parameter-efficient fine-tuning of large language models. The method demonstrates significant improvements in efficiency and performance, making it a valuable contribution to the field. However, the incremental nature of the innovation should be acknowledged. The paper is well-structured and supported by effective figures, but there is room for improvement in clarity and alignment between the abstract and the main content. With the suggested improvements, particularly in broadening the evaluation, addressing ethical considerations, and enhancing the presentation of results, this work has the potential to significantly impact both research and practical applications of large language models. The authors should be commended for their thorough experimental validation and the potential practical benefits of their approach."}
{"paper_id": "acl_2024_s67", "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation", "pdf_path": "eval/acl_2024_pdfs/acl_2024_s67.pdf", "human_reviewer": "", "barebones": "Here is a list of feedback comments for the scientific paper, focusing on major points:\\n\\n1. Methodology:\\n- The proposed SAIL method is innovative and well-motivated. The iterative approach to deriving high-confidence translation pairs is a clever way to address the lack of seed pairs in unsupervised BLI.\\n- The use of word back-translation to filter high-confidence pairs is a good idea, but more analysis on its effectiveness would be valuable.\\n\\n2. Experimental Design:\\n- The experiments are comprehensive, covering multiple language pairs and LLMs. \\n- The comparison to strong baselines like VECMAP and CONTRASTIVEBLI is appropriate.\\n- Including results on both higher-resource (XLING) and lower-resource (PanLex-BLI) languages strengthens the evaluation.\\n\\n3. Results:\\n- The consistent improvements over zero-shot baselines and mapping-based methods are impressive and well-demonstrated.\\n- Statistical significance testing adds rigor to the results.\\n- The analysis of high-confidence dictionaries and impact of parameters like N_it and N_f provides good insights into the method.\\n\\n4. Discussion:\\n- The comparison to GPT models provides an interesting upper bound, though the caveats about instruction tuning are important to note.\\n- The limitations section is thoughtful and identifies key areas for future work.\\n\\n5. Areas for Improvement:\\n- More analysis on the quality of the derived high-confidence dictionaries would be valuable. The small-scale comparison to Google Translate is a good start, but a more systematic evaluation would strengthen the paper.\\n- Additional discussion on computational costs and efficiency compared to mapping-based methods would be useful.\\n- Exploring how SAIL could be adapted to weakly supervised BLI setups, as mentioned in limitations, could be an interesting direction.\\n\\n6. Presentation:\\n- The paper is generally well-written and structured logically.\\n- Some of the tables and figures could be better integrated into the main text, with more discussion of specific results.\\n\\nOverall, this appears to be a strong paper presenting a novel and effective approach to unsupervised bilingual lexicon induction. The comprehensive experiments and analyses support the main claims well.", "liang_etal": "Review outline:\\n\\n1. Significance and novelty\\n\\n2. Potential reasons for acceptance\\n\\n3. Potential reasons for rejection\\n   \\n   a) Limited scope and applicability\\n   \u2022 Focus solely on bilingual lexicon induction task\\n   \u2022 Experiments only on a small set of languages\\n   \\n   b) Lack of theoretical grounding\\n   \u2022 No formal analysis of why the method works\\n   \u2022 Limited discussion of underlying linguistic principles\\n   \\n   c) Computational cost concerns\\n   \u2022 Significantly higher runtime compared to zero-shot baseline\\n   \u2022 Potential scalability issues for larger language sets\\n   \\n   d) Insufficient comparison to state-of-the-art methods\\n   \u2022 Missing comparisons to recent BLI-specific models\\n   \u2022 Limited analysis against other self-learning approaches\\n\\n4. Suggestions for improvement", "multi_agent_without_knowledge": "Here is a comprehensive review of the paper \"Self-Augmented In-Context Learning for Unsupervised Word Translation\":\\n\\nTitle: Self-Augmented In-Context Learning for Unsupervised Word Translation\\n\\n1. Introduction and Motivation:\\nThe paper addresses the task of unsupervised bilingual lexicon induction (BLI), which aims to automatically generate lexicons of words with similar meanings across different languages without using parallel data. The authors identify a gap in current approaches: while large language models (LLMs) perform well on few-shot BLI tasks, they still lag behind traditional mapping-based methods in fully unsupervised scenarios, particularly for lower-resource languages. To address this, the paper proposes a novel method called Self-Augmented In-Context Learning (SAIL) for unsupervised BLI using LLMs.\\n\\n2. Methodology:\\nThe SAIL method consists of three main steps:\\na) Zero-shot prompting to generate initial word translations.\\nb) High-confidence pair extraction using a back-translation mechanism.\\nc) Iterative refinement using the high-confidence pairs as in-context examples for few-shot prompting.\\n\\nKey innovations include the use of back-translation, an iterative approach leveraging the LLM\\\\'s in-context learning capabilities, and a method for selecting relevant in-context examples based on word embedding similarity.\\n\\n3. Experimental Setup:\\nThe authors evaluate SAIL on two standard BLI benchmarks:\\n- XLING: 5 languages (German, English, French, Italian, Russian) with 20 BLI directions\\n- PanLex-BLI: 3 lower-resource languages (Bulgarian, Catalan, Hungarian) with 6 BLI directions\\n\\nThey experiment with four open-source LLMs: LLAMA 7B, LLAMA-2 7B, LLAMA 13B, and LLAMA-2 13B. The evaluation metric used is the standard top-1 accuracy.\\n\\nBaselines include VECMAP, CONTRASTIVEBLI, and zero-shot prompting with each LLM.\\n\\n4. Results and Discussion:\\nMain findings:\\na) SAIL consistently outperforms zero-shot prompting across all tested LLMs.\\nb) SAIL outperforms mapping-based baselines on both benchmarks, with the exception of CONTRASTIVEBLI (C2) slightly edging out SAIL with LLAMA 7B.\\nc) LLAMA-2 13B demonstrates the strongest BLI capability among the tested LLMs.\\n\\nStatistical significance tests using \u03c7\u00b2 tests show that the improvements achieved by SAIL are statistically significant.\\n\\nFurther analyses include:\\n- Inspection of high-confidence dictionaries, revealing larger dictionaries for higher-resource languages.\\n- Impact of the number of iterations (N_it), showing that one iteration is often sufficient.\\n- Effect of the frequency threshold (N_f), demonstrating substantial improvements even with a small number of high-frequency words.\\n- Usefulness of the word back-translation mechanism in improving BLI performance.\\n- Comparison with GPT models, showing that SAIL with LLAMA-2 13B outperforms zero-shot prompting with both GPT-3.5 and GPT-4, even for higher-resource languages.\\n\\n5. Conclusion:\\nThe paper concludes that SAIL substantially improves unsupervised BLI performance with LLMs, outperforming both established mapping-based and zero-shot BLI baselines. The authors highlight the effectiveness of their approach in leveraging LLMs for this task.\\n\\n6. Limitations:\\n- The scope of languages is constrained to those supported by the underlying LLMs (e.g., LLAMA-2 supports only around 27 languages).\\n- SAIL requires more computational time and budget compared to zero-shot baselines.\\n- The paper focuses solely on the BLI task and does not explore potential applications to other NLP tasks.\\n\\nThis comprehensive review provides a detailed analysis of the paper\\\\'s methodology, experimental setup, results, and limitations. It highlights the significant contribution of the SAIL method to the field of unsupervised bilingual lexicon induction and its potential impact on cross-lingual NLP tasks.", "multi_agent_with_knowledge": "Here is a revised comprehensive review of the paper \"Self-Augmented In-Context Learning for Unsupervised Word Translation\" that incorporates the novelty assessment and figure critique:\\n\\nTitle: Self-Augmented In-Context Learning for Unsupervised Word Translation\\n\\nSummary:\\nThis paper introduces a novel approach called Self-Augmented In-Context Learning (SAIL) for improving unsupervised bilingual lexicon induction (BLI) using large language models (LLMs). The authors address the challenge of performing word translation or BLI in scenarios where no seed translation pairs are available, especially for lower-resource languages. SAIL demonstrates significant improvements over existing methods and shows promise for advancing unsupervised machine translation and cross-lingual NLP tasks.\\n\\nNovelty:\\nThe paper presents a novel contribution to the field of unsupervised bilingual lexicon induction. The SAIL method represents a significant departure from traditional mapping-based approaches and offers an innovative application of LLMs to this task. The self-augmented in-context learning approach, which iteratively improves translation performance by leveraging the LLM\\\\'s own outputs, is a unique and valuable contribution to the field.\\n\\nKey Contributions:\\n1. Introduction of the SAIL method, which iteratively induces high-confidence word translation pairs from LLMs and reapplies them for in-context learning.\\n2. Demonstration of substantial performance gains over zero-shot prompting and traditional mapping-based approaches on two established BLI benchmarks.\\n3. Comprehensive analysis of the SAIL method, including its limitations and potential applications.\\n\\nMethodology:\\nThe SAIL method consists of three main steps:\\n1. Using zero-shot prompting to retrieve a set of high-confidence translation pairs.\\n2. Leveraging these pairs as \"self-augmented\" in-context examples for few-shot prompting to refine the high-confidence dictionary iteratively.\\n3. Conducting few-shot learning with the final self-created seed lexicon for BLI inference on the test set.\\n\\nThe authors introduce a word back-translation mechanism to improve the quality of the high-confidence dictionary, which proves to be effective in their experiments.\\n\\nExperimental Setup:\\n- Datasets: XLING (5 languages, 20 BLI directions) and PanLex-BLI (3 lower-resource languages, 6 BLI directions)\\n- LLMs: LLAMA 7B, LLAMA-2 7B, LLAMA 13B, and LLAMA-2 13B\\n- Baselines: Zero-shot prompting, VECMAP, CONTRASTIVEBLI, GPT-3.5, and GPT-4\\n- Evaluation metric: Top-1 accuracy\\n\\nResults and Analysis:\\n1. SAIL consistently outperforms zero-shot prompting across all tested LLMs and language pairs.\\n2. SAIL achieves state-of-the-art performance on both XLING and PanLex-BLI benchmarks, surpassing mapping-based approaches.\\n3. The method shows particularly strong improvements for lower-resource languages.\\n4. Ablation studies demonstrate the effectiveness of the word back-translation mechanism.\\n5. Analysis of hyperparameters reveals that SAIL approaches optimal performance with just one iteration and a relatively small number of frequent words (1,000).\\n\\nStrengths:\\n1. Novel and effective methodology for unsupervised BLI using LLMs.\\n2. Comprehensive evaluation and analysis, including statistical significance tests.\\n3. Strong performance across various language pairs and resource levels.\\n4. Potential applications in machine translation and other cross-lingual NLP tasks.\\n5. Clear and consistent presentation of ideas and results.\\n\\nLimitations and Suggestions for Improvement:\\n1. Language Coverage: The approach is limited to languages supported by the underlying LLMs. Future work could explore methods to adapt SAIL to languages unseen during LLM pretraining.\\n2. Computational Cost: SAIL requires more computational resources compared to zero-shot approaches. Investigating optimization techniques to reduce this overhead would be beneficial.\\n3. Generalizability: The paper focuses solely on the BLI task. Exploring how SAIL or its principles could be adapted to other NLP tasks would strengthen the broader impact of this work.\\n4. Weakly Supervised Scenarios: Investigating the applicability of SAIL to weakly supervised BLI setups could provide additional valuable insights.\\n5. Error Analysis: A more detailed analysis of the types of errors made by SAIL compared to baselines could provide further understanding of its strengths and weaknesses.\\n6. Expanded LLM Comparison: Including LLMs from other families beyond LLAMA would provide a more comprehensive evaluation.\\n7. Dictionary Quality Analysis: A more rigorous evaluation of the quality of the high-confidence dictionaries would be beneficial.\\n\\nFigure Analysis:\\nThe paper includes a graph (Figure 1) showing the Top-1 accuracy averaged over 20 XLING BLI directions with respect to the number of iterations (Nit). This figure effectively demonstrates the improvement of SAIL over iterations and across different LLM models. However, there are several areas where the figure could be improved:\\n\\n1. Axis Labels: The y-axis label should be more specific, e.g., \"Top-1 Accuracy (%)\" to match the caption. The x-axis could benefit from a clearer label, such as \"Number of Iterations (Nit)\".\\n2. Legend: The legend could be more informative by spelling out \"LLAMA\" instead of using abbreviations and providing brief explanations of what each model represents (e.g., model sizes).\\n3. Baseline Comparison: Including a comparison to mapping-based baselines in the graph would better illustrate the claims made in the abstract.\\n4. Error Bars: Adding error bars or confidence intervals would provide more information about the reliability of the results across different language pairs.\\n5. Zero-shot Baseline: Marking the zero-shot baseline more prominently would emphasize the improvement over this starting point.\\n\\nWhile the figure is generally consistent with the abstract and supports the paper\\\\'s claims, these improvements would enhance its clarity and informativeness.\\n\\nConclusion:\\nThis paper presents a novel and effective approach to unsupervised bilingual lexicon induction using large language models. The SAIL method demonstrates significant improvements over existing techniques and shows promise for advancing cross-lingual NLP tasks. While there are areas for improvement in both the methodology and presentation, the work represents a valuable contribution to the field and opens up new avenues for research in leveraging LLMs for unsupervised language tasks."}
