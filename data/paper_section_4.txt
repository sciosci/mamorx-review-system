Conclusion\n\nIn this paper, we develop a method for optimization-based meta-learning that removes the need\nfor differentiating through the inner optimization path, allowing us to decouple the outer meta-\ngradient computation from the choice of inner optimization algorithm. We showed how this gives us\nsigniﬁcant gains in compute and memory efﬁciency, and also conceptually allows us to use a variety\nof inner optimization methods. While we focused on developing the foundations and theoretical\nanalysis of this method, we believe that this work opens up a number of interesting avenues for\nfuture study.\nBroader classes of inner loop procedures. While we studied different gradient-based optimization\nmethods in the inner loop, iMAML can in principle be used with a variety of inner loop algorithms,\nincluding dynamic programming methods such as Q-learning, two-player adversarial games such\nas GANs, energy-based models [39], and actor-critic RL methods, and higher-order model-based\ntrajectory optimization methods. This signiﬁcantly expands the kinds of problems that optimization-\nbased meta-learning can be applied to.\nMore ﬂexible regularizers. We explored one very simple regularization, (cid:96)2 regularization to the pa-\nrameter initialization, which already increases the expressive power over the implicit regularization\nthat MAML provides through truncated gradient descent. To further allow the model to ﬂexibly reg-\nularize the inner optimization, a simple extension of iMAML is to learn a vector- or matrix-valued λ,\nwhich would enable the meta-learner model to co-adapt and co-regularize various parameters of the\nmodel. Regularizers that act on parameterized density functions would also enable meta-learning to\nbe effective for few-shot density estimation.\n\n9\n\n\u000cAcknowledgements\n\nAravind Rajeswaran thanks Emo Todorov for valuable discussions about implicit gradients and po-\ntential application domains; Aravind Rajeswaran also thanks Igor Mordatch and Rahul Kidambi\nfor helpful discussions and feedback. Sham Kakade acknowledges funding from the Washington\nResearch Foundation for innovation in Data-intensive Discovery; Sham Kakade also graciously ac-\nknowledges support from ONR award N00014-18-1-2247, NSF Award CCF-1703574, and NSF\nCCF 1740551 award.\n\nReferences\n[1] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter\nAbbeel. Continuous adaptation via meta-learning in nonstationary and competitive environ-\nments. CoRR, abs/1710.03641, 2017.\n\n[2] Ferran Alet, Tom´as Lozano-P´erez, and Leslie P Kaelbling. Modular meta-learning. arXiv\n\npreprint arXiv:1806.10166, 2018.\n\n[3] Kelsey R Allen, Evan Shelhamer, Hanul Shin, and Joshua B Tenenbaum.\n\nprototypes for few-shot learning. arXiv preprint arXiv:1902.04552, 2019.\n\nInﬁnite mixture\n\n[4] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 136–145. JMLR. org, 2017.\n\n[5] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom\nSchaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by\ngradient descent. In Advances in Neural Information Processing Systems, pages 3981–3989,\n2016.\n\n[6] Walter Baur and Volker Strassen. The complexity of partial derivatives. Theoretical Computer\n\nScience, 22:317–330, 1983.\n\n[7] Atilim Gunes Baydin, Barak A. Pearlmutter, and Alexey Radul. Automatic differentiation in\n\nmachine learning: a survey. CoRR, abs/1502.05767, 2015.\n\n[8] Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with\n\ndifferentiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.\n\n[9] Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends\n\nin Machine Learning, 2015.\n\n[10] Chuong B. Do, Chuan-Sheng Foo, and Andrew Y. Ng. Efﬁcient multiple hyperparameter\n\nlearning for log-linear models. In NIPS, 2007.\n\n[11] Justin Domke. Generic methods for optimization-based modeling. In AISTATS, 2012.\n[12] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2:\n\nFast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016.\n\n[13] Chelsea Finn. Learning to Learn with Gradients. PhD thesis, UC Berkeley, 2018.\n[14] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and\n\ngradient descent can approximate any learning algorithm. arXiv:1710.11622, 2017.\n\n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\n\ntation of deep networks. International Conference on Machine Learning (ICML), 2017.\n\n[16] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual\n\nimitation learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017.\n\n[17] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In\n\nAdvances in Neural Information Processing Systems, pages 9516–9527, 2018.\n\n[18] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.\n\nInternational Conference on Machine Learning (ICML), 2019.\n\n[19] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and\nreverse gradient-based hyperparameter optimization. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70, pages 1165–1173. JMLR. org, 2017.\n\n10\n\n\u000c[20] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting\nInternational Conference on Learning\n\ngradient-based meta-learning as hierarchical bayes.\nRepresentations (ICLR), 2018.\n\n[21] Andreas Griewank. Some bounds on the complexity of gradients, jacobians, and hessians.\n\n1993.\n\n[22] Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques\nof Algorithmic Differentiation. Society for Industrial and Applied Mathematics, Philadelphia,\nPA, USA, second edition, 2008.\n\n[23] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efﬁcient online\n\nbayesian regression. arXiv preprint arXiv:1807.08912, 2018.\n\n[24] Laurent Hasco¨et and Mauricio Araya-Polo. Enabling user-driven checkpointing strategies in\n\nreverse-mode automatic differentiation. CoRR, abs/cs/0606042, 2006.\n\n[25] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient\n\ndescent. In International Conference on Artiﬁcial Neural Networks, 2001.\n\n[26] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to\n\nescape saddle points efﬁciently. In ICML, 2017.\n\n[27] Jaehong Kim, Youngduck Choi, Moonsu Cha, Jung Kwon Lee, Sangyeul Lee, Sungwan Kim,\nYongseok Choi, and Jiwon Kim. Auto-meta: Automated gradient based meta learner search.\narXiv preprint arXiv:1806.06927, 2018.\n\n[28] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International\n\nConference on Learning Representations (ICLR), 2015.\n\n[29] Gregory Koch. Siamese neural networks for one-shot image recognition. ICML Deep Learning\n\nWorkshop, 2015.\n\n[30] Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot\nlearning of simple visual concepts. In Conference of the Cognitive Science Society (CogSci),\n2011.\n\n[31] Benoit Landry, Zachary Manchester, and Marco Pavone. A differentiable augmented la-\ngrangian method for bilevel nonlinear optimization. arXiv preprint arXiv:1902.03319, 2019.\n[32] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning\n\nwith differentiable convex optimization. arXiv preprint arXiv:1904.03758, 2019.\n\n[33] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\n[34] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for\n\nfew-shot learning. arXiv preprint arXiv:1707.09835, 2017.\n\n[35] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-\nmization through reversible learning. In International Conference on Machine Learning, pages\n2113–2122, 2015.\n\n[36] James Martens. Deep learning via hessian-free optimization. In ICML, 2010.\n[37] Fei Mi, Minlie Huang, Jiyong Zhang, and Boi Faltings. Meta-learning for low-resource natu-\nral language generation in task-oriented dialogue systems. arXiv preprint arXiv:1905.05644,\n2019.\n\n[38] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive\n\nmeta-learner. arXiv preprint arXiv:1707.03141, 2017.\n\n[39] Igor Mordatch. Concept learning with energy-based models. CoRR, abs/1811.02486, 2018.\n[40] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the 34th Interna-\n\ntional Conference on Machine Learning-Volume 70, pages 2554–2563. JMLR. org, 2017.\n\n[41] Devang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In Interna-\n\ntional Joint Conference on Neural Netowrks (IJCNN), 1992.\n\n[42] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global\n\nperformance. Math. Program., 108:177–205, 2006.\n\n[43] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms.\n\narXiv preprint arXiv:1803.02999, 2018.\n\n11\n\n\u000c[44] Jorge Nocedal and Stephen J. Wright. Numerical optimization (springer series in operations\n\nresearch and ﬁnancial engineering). 2000.\n\n[45] Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adap-\nIn Advances in Neural Information Processing\n\ntive metric for improved few-shot learning.\nSystems, pages 721–731, 2018.\n\n[46] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. arXiv preprint\n\narXiv:1602.02355, 2016.\n\n[47] Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards Gener-\n\nalization and Simplicity in Continuous Control. In NIPS, 2017.\n\n[48] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.\n[49] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon\nOsindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint\narXiv:1807.05960, 2018.\n\n[50] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.\nMeta-learning with memory-augmented neural networks. In International Conference on Ma-\nchine Learning (ICML), 2016.\n\n[51] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. Diploma thesis,\n\nInstitut f. Informatik, Tech. Univ. Munich, 1987.\n\n[52] John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust\nregion policy optimization. In International Conference on Machine Learning (ICML), 2015.\n[53] Amirreza Shaban, Ching-An Cheng, Olivia Hirschey, and Byron Boots. Truncated back-\n\npropagation for bilevel optimization. CoRR, abs/1810.10667, 2018.\n\n[54] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.\n\nIn Advances in Neural Information Processing Systems, pages 4077–4087, 2017.\n\n[55] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,\n\n1998.\n\n[56] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin,\nCarles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-\narXiv preprint\ndataset: A dataset of datasets for learning to learn from few examples.\narXiv:1903.03096, 2019.\n\n[57] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for\n\none shot learning. In Neural Information Processing Systems (NIPS), 2016.\n\n[58] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\nCharles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.\narXiv:1611.05763, 2016.\n\n[59] Fengwei Zhou, Bin Wu, and Zhenguo Li. Deep meta-learning: Learning to learn in the concept\n\nspace. arXiv preprint arXiv:1802.03596, 2018.\n\n[60] Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast\n\ncontext adaptation via meta-learning. arXiv preprint arXiv:1810.03642, 2018.\n\n12\n\n\u000c"
