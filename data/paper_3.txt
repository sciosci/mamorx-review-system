Standard optimization schedules use only the current parameters λt to\nregularize the noisy coordinate update, and these methods require tuning to balance bias and variance\nin the update. In our setting, Bayes rule automatically makes this trade-off.\nTo illustrate this perspective we consider a small problem. We ﬁt a variational distribution for latent\nDirichlet allocation on a small corpus of 2.5k documents from the ArXiv. For this problem we can\ncompute the full parallel coordinate update and thus compute the tracking error ||λVB\n2 and\nthe observation noise ||λVB\n2 for various algorithms. We emphasize that ˆλt is unbiased, and\nso the observation noise is completely due to variance.In our experimental evaluation, we aim to answer the following questions empirically: (1) Does\nthe iMAML algorithm asymptotically compute the exact meta-gradient? (2) With ﬁnite iterations,\ndoes iMAML approximate the meta-gradient more accurately compared to MAML? (3) How does\nthe computation and memory requirements of iMAML compare with MAML? (4) Does iMAML\nlead to better results in realistic meta-learning problems? We have answered (1) - (3) through our\ntheoretical analysis, and now attempt to validate it through numerical simulations. For (1) and (2),\nwe will use a simple synthetic example for which we can compute the exact meta-gradient and\ncompare against it (exact-solve error, see deﬁnition 3). For (3) and (4), we will use the common\nfew-shot image recognition domains of Omniglot and Mini-ImageNet.\nTo study the question of meta-gradient accuracy, Figure 2 considers a synthetic regression example,\nwhere the predictions are linear in parameters. This provides an analytical expression for Alg(cid:63)\ni al-\nlowing us to compute the true meta-gradient. We ﬁx gradient descent (GD) to be the inner optimizer\nfor both MAML and iMAML. The problem is constructed so that the condition number (κ) is large,\nthereby necessitating many GD steps. We ﬁnd that both iMAML and MAML asymptotically match\nthe exact meta-gradient, but iMAML computes a better approximation in ﬁnite iterations. We ob-\nserve that with 2 CG iterations, iMAML incurs a small terminal error. This is consistent with our\ntheoretical analysis. In Algorithm 2, δ is dominated by δ(cid:48) when only a small number of CG steps\nare used. However, the terminal error vanishes with just 5 CG steps. The computational cost of 1\nCG step is comparable to 1 inner GD step with the MAML algorithm, since both require 1 hessian-\nvector product (see section C for discussion). Thus, the computational cost as well as memory of\niMAML with 100 inner GD steps is signiﬁcantly smaller than MAML with 100 GD steps.\nTo study (3), we turn to the Omniglot dataset [30] which is a popular few-shot image recognition\ndomain. Figure 2 presents compute and memory trade-off for MAML and iMAML (on 20-way,\n5-shot Omniglot). Memory for iMAML is based on Hessian-vector products and is independent\nof the number of GD steps in the inner loop. The memory use is also independent of the number\nof CG iterations, since the intermediate computations need not be stored in memory. On the other\nhand, memory for MAML grows linearly in grad steps, reaching the capacity of a 12 GB GPU in\napproximately 16 steps. First-order MAML (FOMAML) does not back-propagate through the opti-\nmization process, and thus the computational cost is only that of performing gradient descent, which\nis needed for all the algorithms. The computational cost for iMAML is also similar to FOMAML\nalong with a constant overhead for CG that depends on the number of CG steps. Note however, that\nFOMAML does not compute an accurate meta-gradient, since it ignores the Jacobian. Compared\nto FOMAML, the compute cost of MAML grows at a faster rate. FOMAML requires only gradient\ncomputations, while backpropagating through GD (as done in MAML) requires a Hessian-vector\nproducts at each iteration, which are more expensive.\nFinally, we study empirical performance of iMAML on the Omniglot and Mini-ImageNet domains.\nFollowing the few-shot learning protocol in prior work [57], we run the iMAML algorithm on the\n\n(a)\n\n(b)\n\nFigure 2: Accuracy, Computation, and Memory tradeoffs of iMAML, MAML, and FOMAML. (a) Meta-\ngradient accuracy level in synthetic example. Computed gradients are compared against the exact meta-gradient\nper Def 3. (b) Computation and memory trade-offs with 4 layer CNN on 20-way-5-shot Omniglot task. We\nimplemented iMAML in PyTorch, and for an apples-to-apples comparison, we use a PyTorch implementation\nof MAML from: https://github.com/dragen1860/MAML-Pytorch\n\n7\n\n\u000cTable 2: Omniglot results. MAML results are taken from the original work of Finn et al. [15], and ﬁrst-order\nMAML and Reptile results are from Nichol et al. [43]. iMAML with gradient descent (GD) uses 16 and 25 steps\nfor 5-way and 20-way tasks respectively. iMAML with Hessian-free uses 5 CG steps to compute the search\ndirection and performs line-search to pick step size. Both versions of iMAML use λ = 2.0 for regularization,\nand 5 CG steps to compute the task meta-gradient.\n\nAlgorithm\nMAML [15]\nﬁrst-order MAML [15]\nReptile [43]\niMAML, GD (ours)\niMAML, Hessian-Free (ours)\n\n5-way 5-shot\n99.9 ± 0.1%\n99.2 ± 0.2%\n\n20-way 1-shot\n20-way 5-shot\n5-way 1-shot\n98.9 ± 0.2%\n95.8 ± 0.3%\n98.7 ± 0.4%\n89.4 ± 0.5%\n97.9 ± 0.1%\n98.3 ± 0.5%\n97.68 ± 0.04% 99.48 ± 0.06% 89.43 ± 0.14% 97.12 ± 0.32%\n99.16 ± 0.35% 99.67 ± 0.12% 94.46 ± 0.42%\n98.69 ± 0.1%\n99.50 ± 0.26% 99.74 ± 0.11% 96.18 ± 0.36% 99.14 ± 0.1%\n\ndataset for different numbers of class labels and shots (in the N-way, K-shot setting), and compare\ntwo variants of iMAML with published results of the most closely related algorithms: MAML,\nFOMAML, and Reptile. While these methods are not state-of-the-art on this benchmark, they pro-\nvide an apples-to-apples comparison for studying the use of implicit gradients in optimization-based\nmeta-learning. For a fair comparison, we use the identical convolutional architecture as these prior\nworks. Note however that architecture tuning can lead to better results for all algorithms [27].\nThe ﬁrst variant of iMAML we consider involves solving the inner level problem (the regularized\nobjective function in Eq. 4) using gradient descent. The meta-gradient is computed using conjugate\ngradient, and the meta-parameters are updated using Adam. This presents the most straightforward\ncomparison with MAML, which would follow a similar procedure, but backpropagate through the\npath of optimization as opposed to invoking implicit differentiation. The second variant of iMAML\nuses a second order method for the inner level problem. In particular, we consider the Hessian-free\nor Newton-CG [44, 36] method. This method makes a local quadratic approximation to the objective\nfunction (in our case, G(φ(cid:48), θ) and approximately computes the Newton search direction using CG.\nSince CG requires only Hessian-vector products, this way of approximating the Newton search di-\nrection is scalable to large deep neural networks. The step size can be computed using regularization,\ndamping, trust-region, or linesearch. We use a linesearch on the training loss in our experiments to\nalso illustrate how our method can handle non-differentiable inner optimization loops. We refer the\nreaders to Nocedal & Wright [44] and Martens [36] for a more detailed exposition of this optimiza-\ntion algorithm. Similar approaches have also gained prominence in reinforcement learning [52, 47].\nTables 2 and 3 present the results on Omniglot\nand Mini-ImageNet, respectively. On the Om-\nniglot domain, we ﬁnd that\nthe GD version of\niMAML is competitive with the full MAML algo-\nrithm, and substatially better than its approximations\n(i.e., ﬁrst-order MAML and Reptile), especially for\nthe harder 20-way tasks. We also ﬁnd that iMAML\nwith Hessian-free optimization performs substan-\ntially better than the other methods, suggesting that\npowerful optimizers in the inner loop can offer bene-\nﬁts to meta-learning. In the Mini-ImageNet domain,\nwe ﬁnd that iMAML performs better than MAML and FOMAML. We used λ = 0.5 and 10 gra-\ndient steps in the inner loop. We did not perform an extensive hyperparameter sweep, and expect\nthat the results can improve with better hyperparameters. 5 CG steps were used to compute the\nmeta-gradient. The Hessian-free version also uses 5 CG steps for the search direction. Additional\nexperimental details are Appendix F.\n\nAlgorithm\nMAML\nﬁrst-order MAML\nReptile\niMAML GD (ours)\niMAML HF (ours)\n\nTable 3: Mini-ImageNet 5-way-1-shot accuracy\n\n5-way 1-shot\n48.70 ± 1.84 %\n48.07 ± 1.75 %\n49.97 ± 0.32 %\n48.96 ± 1.84 %\n49.30 ± 1.88 %\n\n5 Related Work\n\nOur work considers the general meta-learning problem [51, 55, 41], including few-shot learning [30,\n57]. Meta-learning approaches can generally be categorized into metric-learning approaches that\nlearn an embedding space where non-parametric nearest neighbors works well [29, 57, 54, 45, 3],\nblack-box approaches that train a recurrent or recursive neural network to take datapoints as input\n\n8\n\n\u000cand produce weight updates [25, 5, 33, 48] or predictions for new inputs [50, 12, 58, 40, 38], and\noptimization-based approaches that use bi-level optimization to embed learning procedures, such\nas gradient descent, into the meta-optimization problem [15, 13, 8, 60, 34, 17, 59, 23]. Hybrid\napproaches have also been considered to combine the beneﬁts of different approaches [49, 56]. We\nbuild upon optimization-based approaches, particularly the MAML algorithm [15], which meta-\nlearns an initial set of parameters such that gradient-based ﬁne-tuning leads to good generalization.\nPrior work has considered a number of inner loops, ranging from a very general setting where all\nparameters are adapted using gradient descent [15], to more structured and specialized settings,\nsuch as ridge regression [8], Bayesian linear regression [23], and simulated annealing [2]. The main\ndifference between our work and these approaches is that we show how to analytically derive the\ngradient of the outer objective without differentiating through the inner learning procedure.\nMathematically, we view optimization-based meta-learning as a bi-level optimization problem.\nSuch problems have been studied in the context of few-shot meta-learning (as discussed previ-\nously), gradient-based hyperparameter optimization [35, 46, 19, 11, 10], and a range of other set-\ntings [4, 31]. Some prior works have derived implicit gradients for related problems [46, 11, 4]\nwhile others propose innovations to aid back-propagation through the optimization path for speciﬁc\nalgorithms [35, 19, 24], or approximations like truncation [53]. While the broad idea of implicit\ndifferentiation is well known, it has not been empirically demonstrated in the past for learning more\nthan a few parameters (e.g., hyperparameters), or highly structured settings such as quadratic pro-\ngrams [4]. In contrast, our method meta-trains deep neural networks with thousands of parameters.\nClosest to our setting is the recent work of Lee et al. [32], which uses implicit differentiation for\nquadratic programs in a ﬁnal SVM layer. In contrast, our formulation allows for adapting the full\nnetwork for generic objectives (beyond hinge-loss), thereby allowing for wider applications.\nWe also note that prior works involving implicit differentiation make a strong assumption of an exact\nsolution in the inner level, thereby providing only asymptotic guarantees. In contrast, we provide\nﬁnite time guarantees which allows us to analyze the case where the inner level is solved approxi-\nmately. In practice, the inner level is likely to be solved using iterative optimization algorithms like\ngradient descent, which only return approximate solutions with ﬁnite iterations. Thus, this paper\nplaces implicit gradient methods under a strong theoretical footing for practically use.