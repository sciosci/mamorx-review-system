Artificial Intelligence (AI) has seen a dramatic surge in development, applicability, and perceived value over the last ten years (1,2). There are promising applications in almost all aspects of society, from healthcare to education to finance (3,4). Understanding the source of this impact and innovation is important for the healthy progress of AI. Historically, most ideas originated in academia, but recent striking results in generative AI (e.g., ChatGPT and Midjourney) seem to overshadow academic AI research in favor of industry progress (e.g., see (5)(6)(7)). But how accurate are these perceptions? Are the advances from AI industry research so dominant that academics should change their agendas? Or are advances from industry simply getting more attention while academia publishes underappreciated novel work? And finally, are there academic-industry team arrangements that would bring the "best of both worlds"? These questions motivated us to investigate how industry and academic teams differ in the publications and AI models they produce.
There is no doubt that academics were the leaders of early advancements in AI. For example, the groundbreaking implementation of the perceptron was carried out by Frank Rosenblatt at the Cornell Aeronautical Laboratory (8); backpropagation was formally introduced by Rumelhart, Hinton, and Williams while working at UC, San Diego, and CMU (9). More recently, the initial deep learning advances also came from academia. Hinton and Salakhutdinov (10) (University of Toronto) introduced efficient methods for training deep networks, and Fei-Fei Li (Stanford University) introduced the ImageNet dataset (11,12). Academics seem to have the freedom and flexibility to explore those problems that the private sector has yet to value.
In recent years, however, industry research teams have carried out increasingly more breakthroughs. For example, Microsoft Research's ResNet (13), Google AI's BERT (14), and OpenAI's GPT (15)(16)(17) are all examples of groundbreaking work that originated in industry. There are obvious advantages that industry teams hold over academics. Deep learning models rely heavily on computational power and large datasets available to industry (18). Due to this disadvantage, academic researchers sometimes prefer to focus on different problems instead (19,20). Additionally, how AI research is conducted and motivated varies significantly between industry and academia. In the private sector, scientific discoveries have historically been transformed into marketable products (21). Academic discoveries usually do not have such clear mandates (22). Thus, it is uncertain whether and how these environments contribute to impact and novelty outcomes.
The availability of science research metadata offers a remarkable opportunity to study the differences in the research produced by academia and industry. In the present study, we investigate the impact and novelty of hundreds of thousands of articles from a dataset of AI conferences between 1995 and 2020. We also analyzed hundreds of models from records of state-of-the-art results for natural language processing and computer vision tasks. We categorize co-authoring teams into industry, academic, and academic-industry teams. We show that industry teams tend to be more cited and citation-disruptive than academic teams and produce more stateof-the-art models. On the other hand, academic teams produce significantly more novel work (highly atypical and less conventional) than industry teams. After controlling for field, team size, seniority, and team prestige, we further show that these differences are accentuated. Surprisingly, the novelty disadvantage of industry does not improve when collaborating with academics; the impact disadvantage of academic teams does not improve by collaborating with industry researchers either. Our findings suggest that industry and academic research contributions are different and cannot replace one another: to advance AI forward, we need the nurturing and growth of both environments.
There is limited investigation on the effects of academic versus industry research, and the results are inconclusive. In the work of (23), the authors examined the citation impact of Italian publications indexed in the Web of Science from 2010 to 2017. They found that most publications in the private sector collaborate with academic researchers. Moreover, publications from teams with only industry researchers have a lower impact than those with only academic researchers. However, publications resulting from collaborations between industry and academia have a greater impact. In (24), by investigating Danish authors from the Web of Science between 1996 to 2013, the authors found no significant differences in citations between private-public collaborative publications and solely public (academic) collaborations. They found that such a trend is the opposite for international collaborations, with public-private publications having a higher impact than public-only publications. In (25), the authors investigated the case in Canada, showing that academic-industry collaborations have more impact than research work with only industry authors and academic authors. ( 26) examined collaborative industry-academic research in artificial intelligence (AI). They explored various factors, including the research context, collaborator experience, and strategy. The study found that the industry is more inclined toward conducting applied research. In contrast, academic institutions focus on enhancing basic research, opening new doors for applied research.
There are even fewer publications at the intersection of academic vs. industry work and AI. In (27), the authors studied the opportunities of future AI research, and they proposed that industry researchers are more likely to push new AI techniques because investments are now increasingly accumulating in this area: there is still an order of magnitude difference between government and private investments in the area (28). ( 29) introduced spatially explicit bibliometric analysis to study early-stage AI research. They applied social network analysis to explore the topics, preferences, collaboration, and models in the AI area from 1990-2014. Their analysis revealed that, while academy-industry collaboration is common in many computer science areas, there needs to be more collaboration between industry and academia in AI. More recently, Ahmed et al. (2023) examined how much AI industry research is growing in influence. They examined a number of factors, including Ph.D. and research faculty hires, the computing needs of academia vs. industry models, and the growing number of publications by industry. To the best of our knowledge, no studies analyze the novelty of research in academics vs. industry. Fig. 1. Academic articles and AI models emerge from collaborative efforts that are exclusively academic, exclusively industry, or a fusion of both, each presenting a distinct blend of impact and novelty. A. Authors are classified as being academics or industry researchers. B. Papers are published in a venue (conference) and that venue belongs to a field. The papers published in such venues have teams that are exclusively academic (blue circles), exclusively industry (red circles), and academic-industry teams (green circles). Some of these papers published models for specific tasks (e.g., text summarization). Papers cite each other (continuous line with arrow) C. Papers can be cited by papers in the future (gray circles). D. Sever l metrics of a paper or a model produced by a paper are computed along the impact and novelty dimensions. Such metrics are analyzed across time, field, and team types. In general, we find that no team composition dominates both novelty and impact simultaneously.
Our work analyzes the differences in novelty and impact between academic, industry, and academic-industry teams. We analyzed the affiliations of the researchers (Fig. 1A) publishing in conferences-the most common place to publish AI research. We grouped conferences into fields (Fig. 1B) and analyzed how the papers published by academic, industry, and academicindustry teams cite each other and how other publications cite them (Fig. 1C). We also analyzed the state-of-the-art models published by these types of teams. Finally, we analyze several metrics related to novelty and impact (Fig 1D). One of the ideas is to understand whether there are academic-industry arrangements that would dominate either industry or academia alone in the impact-novelty spectrum (Fig. 1D)  
We start by analyzing the size of research in AI. Similar to general trends in science (31)(32)(33)(34)(35), AI has been increasing. After dividing publications into academic teams, industry teams, and academic-industry teams, the number of publications by different kinds of teams has been steadily increasing since 1995 (Fig. 2A). In particular, there has been a marked increase since 2008. In terms of percentage (Fig. 2B), academic teams produce most of the work, but the other kinds of teams are starting to increase. When looking at the team size (Fig. 1C), there has been a steady growth in the mean team size across different categories of co-authorship teams, which aligns with the idea that science has become a team sport (36). Different subfields show a similar growing trend across team types (Figs 2D, 2E, and 2F are academic teams, industry teams, and academic-industry teams, respectively). We observe that the number of papers in computer vision, machine learning, and natural language processing has increased the most compared to the other sub-fields of robotics and data mining, regardless of the type of co-authorship teams.
As expected, the field of NLP has seen a dramatic recent expansion across the board (Figs 2D-F), but computer vision is still the largest subfield.
While the number of publications in AI has been increasing over the years, some publications have attracted more attention from the community, being cited more often and disrupting the citation network. To better understand how teams might contribute to the disparity of research impact, we compared the likelihood of publishing a high-impact paper across publication years for different types of teams. Similar to previous work measuring impact (37), we calculated the citation each paper received within the five years after publication as a measure of impact. We further ranked these citation counts and identified papers among the top 10 percent most cited AI papers each year. We find that academic-industry teams (Fig. 3A) are more likely to produce high-impact publications than industry teams and academic teams-when we later control for team size and field, the advantage of these collaborative teams goes away. We observe that in the year 2020, citation counts for AI publications published by research teams with exclusively industry authors in 2015 were 73.78 percent more likely to be highly cited compared to research teams with exclusively academic authors (t(116,536) = 3.41, p < 0.001).
The high impact of a publication could result from many factors, including the nature of authorship affiliation and whether authors themselves are influential in their field. We break all co-authorship teams into teams with high average h-index (teams within the top 10% highest team average h-index, Fig 3B ), medium average h-index (teams with the top 50% highest team average h-index, Fig. 3C), and relatively low average h-index (teams with the bottom 25% highest team average h-index, Fig 3D). We found a consistent trend across these subgroups, suggesting that controlling for the team type still makes an industry team dominant.
The impact of research publications can also be influenced by the research field or the venue where the paper was published. We break down the field of AI into sub-fields. We use the categorization of AI conferences provided by the AI deadline website1 (see Materials and Methods), grouping conferences into natural language processing (Fig. 3E), computer vision (Fig. 3F), data mining (Fig. 3G), machine learning (Fig. 3H), and robotics (Fig. 3I). Across fields, we observe the same trend that academic-industry teams are more likely to produce highly cited papers, followed by industry teams. (Later in our analysis, we will see that the advantage of academic-industry teams disappears once we control for other factors) Finally, we estimated the (citation) disruptive index of AI articles from academic, industry, and academic-industry teams (see Methods for more details). Overall, we see a dramatic increase in disruptiveness across team types after 2010 (Fig. 4E). We also observed that the average citation disruptive index for industry publications is 12.80 percent higher than academic publications (t(21,852) = 2.75, p < 0.001). These results suggest that articles from academic teams are more likely to consolidate citations, but articles from industry or academic-industry teams are more disruptive. This might be because industry teams usually propose new neural network architectures, the predecessors of which need to be updated. In contrast, industry or academicindustry teams publish on interdisciplinary subfields, which are more likely to be cited by articles from another field that propose ideas based on other fields. 
AI models with better performance are more likely to be adopted due to their state-of-the-art status, gaining more impact in the research community. We selected 173 notable AI models in popular language and vision areas to analyze the gap between industry and academia. We use the models examined in (38), which satisfy the following criteria: 1) they have received at least 1,000 citations, 2) the research community highly accepts them, 3) they received state-of-the-art performance at the time of publication, and 4) they have been deployed in large projects. Our model dataset covers seven decades, but almost 80% have been proposed during the last decade.
In our analysis, at the beginning of the deep learning era (circa 2012), most SOTA-performing AI models were still produced by academia (Fig. 4A). Most of these models had under 1 billion parameters in size (Fig. 4B). As an example, in 2013, Google published their Word2Vec (large) model with 6.92 billion parameters. After that, AI models proposed by the industry dominated the field of natural language processing and computer vision regarding model size. We observed that in 2022, industry teams proposed 25 state-of-the-art models. In contrast, academic teams only proposed 2, showing that industry teams proposed 11.5 times more state-of-the-art models than academic teams (exact binomial test, N = 27, K = 2, proportion = 0.08, p < 0.001).
A trend in many AI subfields is that models with better performance tend to have more parameters. For instance, GPT-1 has 0.12 billion parameters, GPT-2 has 1.5 billion parameters, and GPT-3 has about 175 billion parameters (15)(16)(17). We noticed a significant growth in the size of AI models proposed by industry and academic teams. Academics propose larger models than the other types of co-authorship teams before 2010, which is the pre-deep-learning era. However, after 2010, industry teams and academic-industry teams produced larger models. At the same time, the size of AI models increased immensely within a decade, as shown in Figure 4B.
A second fundamental function of science is to advance innovation by publishing novel research (40). Which kind of team produces the most novel research? To investigate this question, we estimated the atypicality and conventionality of AI articles from academic teams, industry teams, and academic-industry teams (see Methods for more details). These have been proposed as proxies for novelty. Briefly, atypicality is how unusual the most usual citations of an article are; conventionality is how unusual the most common citations are. Intuitively, if atypicality and conventionality are high, it indicates novel combinations of knowledge.
Our results show that after 2010, AI articles from academic teams were more novel and less conventional (Fig 4D andFig 4E, respectively) compared to AI articles from industry teams or academic-industry teams. Because atypicality is a highly skewed metric, if we look at the median of each type of team, we find that in 2020, the median atypicality of academic teams was 2.8 times higher than the median atypicality of industry teams (t(11,959) = 6.69, p < 0.001). These results suggest that academic teams are likely to pursue novel research. Although previous literature (39) shows that novel research is more likely to receive more citations than conventional research, the discipline of AI might have a different tradition, i.e., state-of-the-art AI models can attract more citations than other types of AI research. 
To understand how research proposed by different types of teams in AI would impact other types of teams, we investigate how they self-cite, as a group, their type of team or cross-cite other types of teams. Figure 4 shows excess self-citations between academic teams and industry teams. It shows that industry teams are more likely to cite papers by other industry teams, creating an "industry bubble." On the other hand, academic teams have less excess self-citation, meaning they are more likely to be familiar with literature from both groups.
This article investigated the relationship between paper impact, disruptiveness, novelty, and coauthorship types. Our previous analysis shows that academic-industry teams are more likely to produce high-impact papers, and academic teams are more likely to contribute novel ideas. While the results are straight-forward, they might obscure factors that might affect the impact and novelty of a paper, such as paper sub-fields, author seniority, co-authorship team size, average academic age (the year since the author first published a paper at the time, they published a newer paper), and the publishing year of the paper. For example, it could be possible that industry researchers are already highly accomplished. Still, researchers in academia are constantly renewing and more recent, creating a correlation between team type and impact. In this section, we fit mixed effect models to investigate the simultaneous effect of these confounders on the impact and novelty of work. 
In this model, we set co-authorship team size, co-authorship team average academic age and hindex, and the year of publishing as independent variables. We then set research sub-field and co-authorship team types as mixed effects. We calculated each paper's citation count five years after publishing as the dependent variable to measure impact. Aside from the full model considering all variables, we fitted three nested models considering a subset of variables to understand how each variable affects the impact of papers. In model 1, we set the co-authorship team type as a mixed effect, showing that without considering other factors, academic-industry teams are most likely to produce high-impact papers, followed by industry teams. Model 2 was created by adding co-authorship team size as an independent variable. The intercept of the fitted model 2 shows that after considering team size, academic-industry teams are less likely to produce high-impact papers compared to industry teams with the same team size. In model 3, we added variables, such as average academic age and h-index of the co-authorship teams, for each paper to represent co-authorship team seniority. The results show that for teams with similar team sizes and co-authorship seniority, industry teams are more likely to produce high-impact papers compared with academic-industry teams and academic teams. In the final model, we added publishing venues (i.e., conferences) representing sub-fields as a mixed effect. The result shows that papers in computer vision are comparatively more impactful. After considering publishing venues and other factors of co-authorship teams, we may conclude from the intercept that papers from industry teams are most likely to produce high-impact papers, followed by academic-industry teams.
In this model, we set co-authorship team size, co-authorship team average academic age and hindex, and the year of publishing as independent variables. We then set different publishing venues representing research sub-fields in AI and co-authorship team types as mixed effects. We use the negative atypicality for each paper as the dependent variable to represent novelty. Like the mixed effect model for impact, we fitted three models using different subsets of variables aside from fitting a model with all the variables. Model 1 is fitted using only the co-authorship team types as mixed effects. The intercept for each group shows that without considering coauthorship team size, co-author seniority, and publishing venue, academic-industry teams are the most novel, followed by academic teams. After considering the effect of co-authorship team size, in model 2, the intercepts show that academic teams are the most novel, surpassing academicindustry teams, consistent with the result from the mixed effect models for impact that shows academic-industry teams are more prominent. After considering all factors, in the full model, the intercepts show that academic teams have produced research work with the most novelty compared with other co-authorship teams.
We see that team sizes matter for publication impact and novelty: publications with more coauthors possess more influence outlets since each co-author will advocate for the paper (Tables 1 and2). Teams with more co-authors are likelier to include researchers with different backgrounds, proposing more atypical ideas with such collaboration. Our result showed that collaboration between industry and academic researchers is among teams with bigger publishing team sizes. Our statistical analysis also shows that different fields have different citing cultures, with some communities citing other work more often and others citing other work less often, causing the disparity in citation count between different subfields in AI. 
The previous results show that academic-industry teams are more likely to publish top-cited papers across different types of co-authorship teams and in different sub-fields in AI. We also observed that those collaborations produce more disruptive but less novel and more conventional publications. To understand what makes those collaborations stand out in AI, for papers produced by academic-industry teams, we further break them down into different groups based on whether their first and last authors are in the industry or academia. We set those authors as academic authors for papers with first or last authors affiliated with both industry and academic entities. While there are 1,277 cases out of 29,131 cases in the dataset, we found that results excluding those 1,277 cases are still similar to those in Fig. 5. Fig. 5A shows the number of papers published each year from 1995 to 2020 by different academic-industry collaborations. It shows that while the number of papers published by all types of teams has increased over the years, teams with first and last authors from the industry and teams with both academic first and last authors are increasing faster. When academics drive the projects in academic-industry teams, we see the same characteristics as in academic teams: they have a lower impact but higher novelty. Similarly, when industry teams drive the projects in mixed teams, they have high impact and low novelty.
For papers produced by academic-industry teams, with the same categorization we mentioned above, we broke this class into four different classes based on first and last author affiliation. We examined the percentage of high-impact papers for each type of collaboration. Among the four types of collaborations, we assume that the paper's first and last authors are the leading authors for those publishing co-authorship teams. With all types of collaborations, teams with both first and last authors from the industry are more likely to produce high-impact papers (Figure 4). In contrast, teams with first and last authors from academia are less and less likely to produce highimpact papers. Papers with the first author from industry but the last author from academia and papers with the last author from academia but the last author from industry are cases where both industry and academia authors are leading the project, with relatively lower impact compared to papers with both first and last authors from the industry.
We investigated disruptiveness, novelty, and conventionality indexes to understand better how those indexes change over time for different types of academic-industry teams. We calculated those indexes for those teams and plotted the average index by year. Fig. 5C shows the disruptiveness index of papers published by different types of academic-industry teams. Consistent with Fig. 4C, teams with both the first and last authors from industry shared traits with teams that are exclusively industry and have the most disruptiveness among the four groups. In contrast, teams with both first and last authors from academia have relatively low disruptiveness. Fig. 5D and5E show the conventionality and atypicality of different types of academic-industry collaborations. Fig. 5E shows that teams with first authors from academia and last authors from industry, along with teams with both first and last authors from industry, are more conventional. Finally, teams with the first author from industry and the last author from academia are more atypical, followed by teams with both the first and last authors from academia (Fig. 5D).
Our analysis shows that among different types of collaboration between industry and academia, teams led by industry researchers, with both first and last authors from the industry, have produced more impactful and disruptive publications, while teams led by academic researchers, with first and last authors from academia, are producing less impactful papers, indicating that teams led by academic researchers might not be benefitted by their collaboration with industry researchers. However, our analysis also shows that teams with both first and last authors from academia and teams with academic last authors and industry first authors have published papers that are less conventional and more atypical, similar to teams of exclusively academic authors. A qualitative analysis of the top highly cited papers, citation-disruptive, and novel papers (Supplementary material) shows that highly atypical work usually combines disparate fields (e.g., "Learn To Be Uncertain Leveraging Uncertain Labels In Chest X Rays With Bayesian Neural Networks" published in a CVPR workshop combines Bayesian statistics with neural works with an application to radiology), and highly impactful are articles (e.g., "Adam: A Method for Stochastic Optimization" published in ICLR introduced one of the most common optimizers used for training neural networks today).
Motivated by the striking new advances in AI research by industry over the last years, we wanted to investigate whether and how AI research differs when produced by academic vs. industry teams. In particular, we used publication, citation, and state-of-the-art models published by these types of teams to investigate their work's impact, citation disruptiveness, novelty, and state-ofthe-art status. Our study indicates that research publications from industry-affiliated teams are generally more impactful, which is consistent with the results shown in ( 25) and ( 23) (but see (24)). We also see that publications from exclusively academic teams are less conventional and more atypical, suggesting that they are more novel and likely to introduce new, exploratory ideas. Our anal sis reveals that the unique strengths of academic and industry teams are difficult to replace by academic-industry partnerships; collaborations behave similarly to industry teams.
We speculate that industry research is becoming increasingly more impactful and citationdisruptive because they have more access to data, computational power, and people. In the era of deep learning and big models, having large training datasets and modern computing hardware is crucial for advancing AI. Many technology companies rely on data and computing to drive their business, giving them a competitive advantage (18). In recent years, more and more Ph.D. students and professors have been influenced by industry research due to their abundant computational power, large datasets, and higher pay (30). Thus, the result of industry AI work is more prominent. Academic teams are producing the most novel research. This is consistent with a view in which academia addresses problems that industry does not consider (40). However, we also found that academic teams produced less impactful research. This paradoxical, high-novelty-low-impact combination is rare in the literature. For example, in the original atypicality and conventionality metrics by (39), novelty predicts impact. Similarly, in the work of (41), they found that highly novel papers are less cited early but reach equal or higher citations than more conventional work. We found that this citation disadvantage by academic teams did not diminish over time (Supplementary materials, Fig. S1.D). We entertain some hypotheses. One is that the AI field moves too fast and that publications that are not "discovered" in time miss out on citations due to fast obsolescence rates (42). Another hypothesis is that the influence of these publications is not perceived through direct citations-our metric of choice-but rather through network effects, which may be captured by network-based impact metrics such as the Eigenfactor (43). In the future, we will investigate these other explanations for the apparent divergence between impact and novelty.
We discovered that academic-industry teams are more impactful than other co-authorship teams. However, these teams have markedly different sizes, which could contribute to the impact differential of their papers. When controlling for team size, academic age, prestige, year, and field, we found that this academic-industry advantage largely disappears. Additionally, we found that the most impactful academic-industry teams are those where the first and last authors are from industry, while the most novel ones are those where the first and last authors are from academia. Because first and last authors often carry the bulk of the research guidance, this indicates that academic and industry researchers carry over their research cultures. A similar pattern is observed in the novelty space. In sum, teams that consist of a mix of both do not benefit from the competitive advantages of each culture.
Our study has some limitations. We apply citation-based metrics such as citation-disruptiveness, atypicality, and conventionally to understand industry and academic research. Even though those metrics are widely used in the field of science of science (44), they might not truly capture the disruptiveness and novelty in the colloquial sense of the word-we only care about citations and not other forms of disruption or novelty such as fundamental technological advances (40). Other limitations include our controls for the mixed effect models (see Results). For example, the international composition of a co-authorship team or whether the team is formed through crossinstitutional collaboration can influence impact and novelty. Measuring those factors is beyond the scope of this study and should be considered for future research. Finally, we are, in part also analyzing the effect of mobility on scientists. After all, most industry research is done by scientists who first trained in academia. The mentorship and research culture they enjoyed while in academia is not captured by citation information (45).
Despite its limitations, our study investigates a critical missing piece from the discussions about AI in academia and industry. While others have suggested that students in academia should move away from problems studied in industry (7), our results suggest that both environments are pursuing important work and need each other. We would suggest that students in academia continue their research and intensify their efforts to pursue novel and exciting work. In some fields, industry does not have a culture of publishing results because it opens them up because of intellectual property issues (46,47). Our study thus contributes to the understanding of the relationship between industry and academia for research in non-AI fields as well.
This article aimed to understand the impact and novelty characteristics of different academic and industry team compositions. We found that academic teams have less impact than industry teams but produce significantly more atypical and, therefore, novel work. By analyzing academic-industry collaboration teams, we found that while they produce the highest impact compared to purely industry or academic teams, they cannot seem to enjoy the benefits of pure teams alone.
In future work, we will investigate how the individuals who are part of the teams might have an outsized effect on the impact and novelty of the articles. In our work, we only analyzed individuals as part of teams, and all our performance measures were team-related. Also, we plan to understand whether there are significant variations across nations. Given the giant push for AI across the US, China, and Europe, there might be significant differences due to the disparate incentives in academics, industry, and academic-industry teams across these populations.
To identify AI papers, we selected representative AI conferences according to a website called "AI Conference Deadlines"4 , which lists popular AI conferences and their deadlines for paper submission. The website categorizes different AI conferences, which we used to identify different sub-fields in AI. The mapping between subfields and conferences is shown in (Table 4).
To keep track of the trend of state-of-the-art models in AI, we selected 173 notable AI models from (38) to analyze the gap between industries and academia. The following criteria select all models: 1. They have at least 1000 citations. 2. The research community highly accepts them. 3. They received state-of-the-art performance at the time of publication. 4. They have been deployed in large projects. It is a curated collection of the SOTA AI models (38), which has comprehensive features of models from the 1950s to 2022. The dataset consists of models belonging to two domains, vision, and language, and is categorized into three organization types: academia, industry, and academic-industry collaboration.

While there are several popular definitions of originality, disruption is one definition of originality that means "making previous state-of-the-art obsolete." The disruptiveness of a research article could be measured by what the paper cites and what papers are citing this paper: If a paper is cited by many papers that are not also citing what the paper cites, it shows that this paper makes what it cites obsolete. Thus, (4 ) proposed a network-based metric, the CD index, to measure the disruptiveness of a research paper, ranging from -1 to 1. If a res arch paper has a CD index of 1, then this paper is highly disruptive, and all papers citing this paper don't cite their predecessors. If a res arch paper has a CD index of -1, then this paper is not disruptive but consolidating, and all papers citing this paper also cite its predecessors. In this work, we compute these metrics to estimate the disruptiveness of AI papers to better understand the differences between research work done in academia and research work done in industry. 
To better understand how research work from the industry might be different from the work from academia, it is important to also look at how novel and conventional their work is, and in general, whether work from the industry is more novel compared to the work from academia, or is it the opposite? In this work, we used the method proposed by (49) to estimate the novelty and conventionality of research work from the field of AI. This method quantifies the novelty and conventionality of a research paper through the novelty of combinations of cited journal pairs. If a paper's top 10% novel combination of publishing venues is novel across all papers in the previous year, then this paper can be considered a novel. If a paper's top 50% novel combination of publishing venues is conventional across all papers in the previous year, then this paper can be considered conventional. For each venue combination, we compute the likelihood of its appearance compared to a random co-reference network using a z-score. We measure the conventionality of a research paper by using the median of z-scores of all co-reference pairs of a paper; we then measure the novelty of a paper by looking at the top 10 percentile of z-scores of all co-reference pairs. If a co-reference has a negative z-score, then this pair of co-reference is novel; otherwise, it is conventional.
While the types of different teams would affect how impactful a work would be, for instance, people might cite papers affiliated with large companies because they seem more reliable, different natures linked to a research paper might also affect how likely a paper will be impactful, such as the size of the co-authorship team, the targeting field of the research paper. We fitted a mixed-effect linear regression to show how decisive team type might be linked to how likely a paper will be impactful and how team type can be compared with other factors such as paper field, team size, and co-authorship team seniority. Like linear regression, mixed-effect linear regression shows the relationship between the dependent and independent variables and the relationship between independent variables. Unlike linear regression models, mixed effect linear regression models assume that the dataset for fitting the model could be broken down into different groups, with relationships between variables differing in different groups. In the mixedeffect linear regression model, we fit for this work, the field of the papers and the types of coauthorship teams are set as fixed effects.
To further understand how publications could shape the research field by contributing ideas to teams from different environments, we measure the outsize self-citation using a difference-indifference metric. We define excess self-citation as where ð¸ð¶ð¶ ! (ð¼) refers to the outsize self-citation difference in difference percentage at time t of papers by exclusively industry teams citing papers by exclusively academic teams, while ð¸ð¶ð¶ ! (ð´) refers to outsize self-citation difference in difference percentage of papers by exclusively academic teams citing papers by exclusively industry teams. ð‘¡ # is the time of publication of the papers considered. ð‘ƒ(ð›¼ | ð›½, ð‘¡ â‰¤ ð‘¡ # ) refers to the probability of paper in team type ð›½ to cite a paper from team type ð›¼ while ð‘ƒ(ð›¼ | ð‘¡ â‰¤ ð‘¡ # ) refers to the probability of any paper by any team type to cite a paper by team type ð›¼. Intuitively, these expressions correct for differences in cross-citation and self-citation cultures across team types.


Microsoft Academic Graph (MAG) includes rich information about many entities in the scientific process, including research papers, publishing venues, authors, affiliations, and fields. Using a copy of MAG we retrieved in late 2021, we analyze the metadata, including reference, authorship, and author affiliations, for research publications from AI publishing venues. We were able to identify conferences mentioned in the website "AI Conference Deadlines" 2 , which includes the most active and popular conferences in the fields of AI and the website also includes categories for conferences, such as natural language processing (NLP), computer vision (CV), and data mining (DM).
To identify researchers affiliated with industry entities and differentiate them from researchers from academia, we first match affiliations from MAG with Research Organization Registry 3 to match affiliations from the industry. On top of that, we compiled a list of words that indicate the academic status of an affiliation, such as "university," "academy," "school," "college," and "faculty." We were able to identify the status (whether it is from the industry or academia) of an author's affiliation at the time the author participated in the authorship of an AI paper.
Lizhen Liang, Han Zhuang, James Zou, Daniel E. Acuna
Corresponding author: daniel.acuna@colorado.edu
The PDF file includes:


Tables S1
Table S1. Mean and SEM of H-index and academic age for junior and senior team members. The h-index and academic age of junior researchers in industry are higher than in academia. To have an overview of the researcher population, we plot the histogram of researchers' h-index and academic age at the time they published a paper after (including) the year 2000, shown in Fig. S1 A-B. For h-indexes, we show the distribution of h-indexes at the time each author published an AI paper, with h-indexes less than 100. We show that across different types of teams, the distribution of h-indexes generally follows the same trend. We also show the distribution of academic age at the time each researcher published an AI paper for researchers with less than 50 years of academic age. We found that for authors published in industry-academic collaboration teams, there is a peak around 4-6 years of academic age. While for authors published in all industry teams, there is a peak at 7-8 years of academic age. To further compare the seniority of authors publishing in different types of co-authorship teams, we train Gaussian mixture clustering models on different types of teams with h-index and academic age at the time of publishing. We set the number of components to two and compared the centroid of the less senior cluster and the more senior cluster across different types of co-authorship teams. For authors published in academic teams, the less senior cluster has a centroid of 3    ). In Fig. S2. B, we show that while robotics is the most atypical field, natural language processing is less atypical, and as shown in C, the field of natural language processing is most conventional. Such finding is consistent with what we show in the mixed effect regression model. On the other hand, in Fig. S2. A, we show that machine learning is the most citation-disruptive field, while data mining and natural language processing is less disruptive.

In this section, we ranked the top papers by different metrics we used for analysis (bottom papers in the case of atypicality and conventionality), including citation count within five years of publication (C5), citation disruptive index (dc), atypicality, and conventionality. Then, I rank publications by combining the scaled variables I used in the previous lists. For each of the variables, we scaled them with a min-max scaler, keeping them between 0 and 1 before combining them to create a standardized index. We subtract the scaled atypicality and conventionality to get the standardized value, since for these two measures, the smaller the better. We first show the ranking with a standardized index with scaled C5, dc, atypicality, and conventionality. Then, we show the ranking with scaled C5, atypicality, and conventionality. 



1. Methodology and Data:

a) Author Classification:
- The paper uses a combination of Microsoft Academic Graph (MAG) data, Research Organization Registry, and keyword-based identification to classify authors as industry or academic.
- For ambiguous cases with dual affiliations (1,277 out of 29,131), the authors defaulted to classifying them as academic.
- Recommendation: Provide a more detailed explanation of how various types of ambiguous affiliations were handled beyond the dual affiliation case mentioned.

b) 5-Year Citation Window:
- The paper uses a â€œcitation count within five years of publication (C5)â€ metric for impact assessment.
- While not explicitly stated, this choice likely allows for fair comparison between papers and aligns with previous work in the field.
- Recommendation: Explain the specific rationale for choosing a 5-year window and conduct sensitivity analyses with different time windows (e.g., 3-year, 7-year) to demonstrate the robustness of findings.

c) AI Conference Deadlines Website:
- The website was used to categorize AI conferences into subfields.
- Potential biases include limited comprehensiveness, oversimplified categorization, selection bias, and temporal bias.
- Recommendation: Use multiple sources for conference selection, review the categorization thoroughly, and regularly update the conference list to address these biases.

2. Analysis and Results:

a) Academic-Industry Collaborations:
- The study found that industry teams tend to be more cited and citation-disruptive, while academic teams produce more novel work.
- Surprisingly, collaboration between industry and academia does not seem to combine these advantages effectively.
- Teams led by industry researchers produce more impactful and disruptive publications, while academic-led teams collaborating with industry produce less impactful papers.
- Recommendation: Conduct a deeper investigation into why these collaborations donâ€™t seem to benefit from the advantages of pure teams, possibly exploring factors such as team dynamics, resource allocation, or research objectives.

b) Citation Disruptiveness Metric:
- The paper uses citation-based metrics to measure research impact and novelty.
- Recommendation: Discuss the limitations of these metrics and consider incorporating alternative measures to provide a more holistic view of research impact and novelty.

3. Limitations and Future Work:

- The study relies heavily on citation-based metrics, which may not capture all forms of disruption or novelty.
- Factors such as international composition of teams and cross-institutional collaborations are not fully accounted for.
- The analysis does not capture the effect of scientist mobility between academia and industry.
- Recommendation: Address these limitations in the paper and propose specific avenues for future research to overcome them, such as incorporating qualitative analysis or exploring additional metrics for research impact and novelty.

4. Presentation and Structure:

- Recommendation: Enhance data visualization to better illustrate key findings, particularly the differences between team types across various metrics.
- Provide a more detailed discussion of the mixed-effects models used, including assumptions and potential limitations.

By addressing these points, the paper can significantly strengthen its methodology, analysis, and overall contribution to understanding the dynamics of AI research across academia and industry.
