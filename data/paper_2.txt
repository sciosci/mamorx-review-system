Our aim is to solve the bi-level meta-learning problem in Eq. 4 using an iterative gradient based\nalgorithm of the form θ ← θ − η dθF (θ). Although we derive our method based on standard\ngradient descent for simplicity, any other optimization method, such as quasi-Newton or Newton\nmethods, Adam [28], or gradient descent with momentum can also be used without modiﬁcation.\nThe gradient descent update be expanded using the chain rule as\n\ni (θ)\n\n∇φLi(Alg(cid:63)\n\ni (θ)).\n\n(5)\n\nM(cid:88)\n\nθ ← θ − η\n\ndAlg(cid:63)\ndθ\ni (θ)) is simply ∇φLi(φ) |φ=Alg(cid:63)\n\n1\nM\n\ni=1\n\nHere, ∇φLi(Alg(cid:63)\ni (θ) which can be easily obtained in practice via\nautomatic differentiation. For this update rule, we must compute dAlg(cid:63)\ni is implicitly\ndeﬁned as an optimization problem (Eq. 4), which presents the primary challenge. We now present\nan efﬁcient algorithm (in compute and memory) to compute the meta-gradient..\n\n, where Alg(cid:63)\n\ni (θ)\n\ndθ\n\ndθ\n\ni (θ)\n\n3.1 Meta-Gradient Computation\nIf Alg(cid:63)\ni (θ) is implemented as an iterative algorithm, such as gradient descent, then one way to com-\npute dAlg(cid:63)\nis to propagate derivatives through the iterative process, either in forward mode or\nreverse mode. However, this has the drawback of depending explicitly on the path of the optimiza-\ntion, which has to be fully stored in memory, quickly becoming intractable when the number of\ngradient steps needed is large. Furthermore, for second order optimization methods, such as New-\nton’s method, third derivatives are needed which are difﬁcult to obtain. Furthermore, this approach\nbecomes impossible when non-differentiable operations, such as line-searches, are used. However,\nby recognizing that Alg(cid:63)\ni is implicitly deﬁned as the solution to an optimization problem, we may\nemploy a different strategy that does not need to consider the path of the optimization but only the\nﬁnal result. This is derived in the following Lemma.\nLemma 1. (Implicit Jacobian) Consider Alg(cid:63)\nˆLi(φi)\nbe the result of Alg(cid:63)\n\n(cid:17)\ni (θ) as deﬁned in Eq. 4 for task Ti. Let φi = Alg(cid:63)\n(cid:18)\n\nis invertible, then the derivative Jacobian is\n\n(cid:19)−1\n\ni (θ). If\n\ni (θ)\n\n(cid:16)\n\nφ\n\n=\n\nI +\n\n∇2\n\nφ\n\nˆLi(φi)\n\n1\nλ\n\n.\n\n(6)\n\nλ∇2\nI + 1\ndAlg(cid:63)\ndθ\n\ni (θ)\n\ni (θ), thereby decoupling meta-gradient computation from choice of inner level optimizer.\n\nNote that the derivative (Jacobian) depends only on the ﬁnal result of the algorithm, and not the\npath taken by the algorithm. Thus, in principle any approach of algorithm can be used to compute\nAlg(cid:63)\nPractical Algorithm: While Lemma 1 provides an idealized way to compute the Alg(cid:63)\ni Jacobians\nand thus by extension the meta-gradient, it may be difﬁcult to directly use it in practice. Two\nissues are particularly relevant. First, the meta-gradients require computation of Alg(cid:63)\ni (θ), which is\nthe exact solution to the inner optimization problem. In practice, we may be able to obtain only\napproximate solutions. Second, explicitly forming and inverting the matrix in Eq. 6 for computing\n\n4\n\n\u000cAlgorithm 1 Implicit Model-Agnostic Meta-Learning (iMAML)\n1: Require: Distribution over tasks P (T ), outer step size η, regularization strength λ,\n2: while not converged do\nSample mini-batch of tasks {Ti}B\n3:\nfor Each task Ti do\n4:\n5:\nend for\n6:\n7:\nUpdate meta-parameters with gradient descent: θ ← θ − η ˆ∇F (θ) // (or Adam)\n8:\n9: end while\n\nAverage above gradients to get ˆ∇F (θ) = (1/B)(cid:80)B\n\nCompute task meta-gradient gi = Implicit-Meta-Gradient(Ti, θ, λ)\n\ni=1 ∼ P (T )\n\ni=1 gi\n\nAlgorithm 2 Implicit Meta-Gradient Computation\n1: Input: Task Ti, meta-parameters θ, regularization strength λ\n2: Hyperparameters: Optimization accuracy thresholds δ and δ(cid:48)\n3: Obtain task parameters φi using iterative optimization solver such that: (cid:107)φi − Alg(cid:63)\ni (θ)(cid:107) ≤ δ\n4: Compute partial outer-level gradient vi = ∇φLT (φi)\n5: Use an iterative solver (e.g. CG) along with reverse mode differentiation (to compute Hessian\n\nvector products) to compute gi such that: (cid:107)gi −(cid:0)I + 1\n\nλ∇2 ˆLi(φi)(cid:1)−1\n\nvi(cid:107) ≤ δ(cid:48)\n\n6: Return: gi\n\nthe Jacobian may be intractable for large deep neural networks. To address these difﬁculties, we\nconsider approximations to the idealized approach that enable a practical algorithm.\nFirst, we consider an approximate solution to the inner optimization problem, that can be obtained\nwith iterative optimization algorithms like gradient descent.\nDeﬁnition 1. (δ–approx. algorithm) Let Algi(θ) be a δ–accurate approximation of Alg(cid:63)\n\ni (θ), i.e.\n\n(cid:107)Algi(θ) − Alg(cid:63)\n\ni (θ)(cid:107) ≤ δ\n\nSecond, we will perform a partial or approximate matrix inversion given by:\nDeﬁnition 2. (δ(cid:48)–approximate Jacobian-vector product) Let gi be a vector such that\n\n(cid:19)−1∇φLi(φi)(cid:107) ≤ δ(cid:48)\n\n(cid:107)gi −\n\nI +\n\n1\nλ\n\n∇2\n\nφ\n\nˆLi(φi)\n\n(cid:18)\n\nw(cid:62)(cid:18)\n\n(cid:19)\n\nwhere φi = Algi(θ) and Algi is based on deﬁnition 1.\nNote that gi in deﬁnition 2 is an approximation of the meta-gradient for task Ti. Observe that gi can\nbe obtained as an approximate solution to the optimization problem:\n\nmin\nw\n\n1\n2\n\nI +\n\n∇2\n\nφ\n\nˆLi(φi)\n\n1\nλ\n\nw − w(cid:62)∇φLi(φi)\n\n(7)\n\nThe conjugate gradient (CG) algorithm is particularly well suited for this problem due to its excel-\nlent iteration complexity and requirement of only Hessian-vector products of the form ∇2 ˆLi(φi)v.\nSuch hessian-vector products can be obtained cheaply without explicitly forming or storing the Hes-\nsian matrix (as we discuss in Appendix C). This CG based inversion has been successfully deployed\nin Hessian-free or Newton-CG methods for deep learning [36, 44] and trust region methods in re-\ninforcement learning [52, 47]. Algorithm 1 presents the full practical algorithm. Note that these\napproximations to develop a practical algorithm introduce errors in the meta-gradient computation.\nWe analyze the impact of these errors in Section 3.2 and show that they are controllable. See Ap-\npendix A for how iMAML generalizes prior gradient optimization based meta-learning algorithms.\n\n3.2 Theory\n\nIn Section 3.1, we outlined a practical algorithm that makes approximations to the idealized update\nrule of Eq. 5. Here, we attempt to analyze the impact of these approximations, and also under-\nstand the computation and memory requirements of iMAML. We ﬁnd that iMAML can match the\n\n5\n\n\u000cTable 1: Compute and memory for computing the meta-gradient when using a δ–accurate Algi, and the cor-\nresponding approximation error. Our compute time is measured in terms of the number of ∇ ˆLi computations.\nAll results are in ˜O(·) notation, which hide additional log factors; the error bound hides additional problem\ndependent Lipshitz and smoothness parameters (see the respective Theorem statements). κ ≥ 1 is the condi-\ntion number for inner objective Gi (see Equation 4), and D is the diameter of the search space. The notions\nof error are subtly different: we assume all methods solve the inner optimization to error level of δ (as per\ndeﬁnition 1). For our algorithm, the error refers to the (cid:96)2 error in the computation of dθLi(Alg(cid:63)\ni (θ)). For\nthe other algorithms, the error refers to the (cid:96)2 error in the computation of dθLi(Algi(θ)). We use Prop 3.1 of\nShaban et al. [53] to provide the guarantee we use. See Appendix D for additional discussion.\n\nAlgorithm\n\nMAML (GD + full back-prop)\n\nMAML (Nesterov’s AGD + full back-prop)\n\nTruncated back-prop [53] (GD)\n\nImplicit MAML (this work)\n\nδ\n\nδ\n\nMemory\n\nCompute\n\nκ log(cid:0) D\n(cid:1)\nκ log(cid:0) D\n(cid:1) Mem(∇ ˆLi) · √\nκ log(cid:0) D\n(cid:1)\n(cid:1)\nκ log(cid:0) D\n\nMem(∇ ˆLi) · κ log(cid:0) D\n(cid:1)\nκ log(cid:0) D\n(cid:1)\nMem(∇ ˆLi) · κ log(cid:0) 1\n(cid:1)\n\nMem(∇ ˆLi)\n\nδ\n\nδ\n\nδ\n\nδ\n\n\u0001\n\n√\n\n√\n\nError\n\n0\n\n0\n\n\u0001\n\nδ\n\nminimax computational complexity of backpropagating through the path of the inner optimizer, but\nis substantially better in terms of memory usage. This work to our knowledge also provides the\nﬁrst non-asymptotic result that analyzes approximation error due to implicit gradients. Theorem 1\nprovides the computational and memory complexity for obtaining an \u0001–approximate meta-gradient.\nWe assume Li is smooth but do not require it to be convex. We assume that Gi in Eq. 4 is strongly\nconvex, which can be made possible by appropriate choice of λ. The key to our analysis is a second\norder Lipshitz assumption, i.e. ˆLi(·) is ρ-Lipshitz Hessian. This assumption and setting has received\nconsiderable attention in recent optimization and deep learning literature [26, 42].\nTable 1 summarizes our complexity results and compares with MAML and truncated backpropa-\ngation [53] through the path of the inner optimizer. We use κ to denote the condition number of\nthe inner problem induced by Gi (see Equation 4), which can be viewed as a measure of hardness\nof the inner optimization problem. Mem(∇ ˆLi) is the memory taken to compute a single derivative\n∇ ˆLi. Under the assumption that Hessian vector products are computed with the reverse mode of\nautodifferentiation, we will have that both: the compute time and memory used for computing a\nHessian vector product are with a (universal) constant factor of the compute time and memory used\nfor computing ∇ ˆLi itself (see Appendix C). This allows us to measure the compute time in terms of\nthe number of ∇ ˆLi computations. We refer readers to Appendix D for additional discussion about\nthe algorithms and their trade-offs.\nOur main theorem is as follows:\nTheorem 1. (Informal Statement; Approximation error in Algorithm 2) Suppose that: Li(·) is B\nLipshitz and L smooth function; that Gi(·, θ) (in Eq. 4) is a µ-strongly convex function with condi-\ntion number κ; that D is the diameter of search space for φ in the inner optimization problem (i.e.\n(cid:107)Alg(cid:63)\nLet gi be the task meta-gradient returned by Algorithm 2. For any task i and desired accuracy level\n\u0001, Algorithm 2 computes an approximate task-speciﬁc meta-gradient with the following guarantee:\n\ni (θ)(cid:107) ≤ D); and ˆLi(·) is ρ-Lipshitz Hessian.\n\n||gi − dθLi(Alg(cid:63)\n\ni (θ))|| ≤ \u0001 .\n\n(cid:16)√\n\nFurthermore, under the assumption that the Hessian vector products are computed by the re-\nverse mode of autodifferentiation (Assumption 1), Algorithm 2 can be implemented using at most\n˜O\n\ngradient computations of ˆLi(·) and 2 · Mem(∇ ˆLi) memory.\n\n(cid:16) poly(κ,D,B,L,ρ,µ,λ)\n\n(cid:17)(cid:17)\n\nκ log\n\n\u0001\n\nThe formal statement of the theorem and the proof are provided the appendix.\nImportantly, the\nalgorithm’s memory requirement is equivalent to the memory needed for Hessian-vector products\nwhich is a small constant factor over the memory required for gradient computations, assuming the\nreverse mode of auto-differentiation is used. Finally, based on the above, we also present corollary 1\nin the appendix which shows that iMAML efﬁciently ﬁnds a stationary point of F (·), due to iMAML\nhaving controllable exact-solve error