Standard optimization schedules use only the current parameters \(\lambda_t\) to regularize the noisy coordinate update, requiring tuning to balance bias and variance. In contrast, Bayes rule automatically handles this trade-off. A small problem fitting a variational distribution for latent Dirichlet allocation on 2.5k ArXiv documents illustrates this perspective. The full parallel coordinate update allows computation of tracking error \(\|\lambda_{VB}\|_2\) and observation noise \(\|\lambda_{VB}\|_2\) for various algorithms. The unbiased nature of \(\hat{\lambda_t}\) means observation noise is solely due to variance.

Empirical evaluation aims to answer: (1) Does iMAML asymptotically compute the exact meta-gradient? (2) Does iMAML approximate the meta-gradient more accurately compared to MAML with finite iterations? (3) How do the computation and memory requirements of iMAML compare with MAML? (4) Does iMAML lead to better results in realistic meta-learning problems? Questions (1) - (3) are addressed through theoretical analysis, now validated through numerical simulations. For (1) and (2), a synthetic example allows computation of the exact meta-gradient for comparison. For (3) and (4), common few-shot image recognition domains Omniglot and Mini-ImageNet are used.

Meta-gradient accuracy is studied using a synthetic regression example, with linear parameter predictions allowing an analytical expression for \(\text{Alg}^*\). Gradient descent (GD) is fixed as the inner optimizer for both MAML and iMAML. Both algorithms asymptotally match the exact meta-gradient, but iMAML approximates it better with finite iterations. With 2 CG iterations, iMAML incurs a small terminal error, consistent with theoretical analysis. Terminal error vanishes with 5 CG steps. Computational cost of 1 CG step matches 1 inner GD step in MAML, making iMAML with 100 GD steps significantly less computationally and memory-intensive than MAML with 100 GD steps.

On the Omniglot dataset, Figure 2 shows computation and memory trade-offs for MAML and iMAML (20-way, 5-shot). Memory for iMAML, based on Hessian-vector products, is independent of GD steps in the inner loop and CG iterations. MAML memory grows linearly with grad steps, hitting a 12 GB GPU's capacity at around 16 steps. First-order MAML (FOMAML) skips back-propagation through optimization, reducing computational cost to gradient descent. iMAML's computational cost is similar to FOMAML with a constant CG overhead. FOMAML, however, misses accurate meta-gradients due to ignoring the Jacobian. MAML's compute cost grows faster than FOMAML, requiring Hessian-vector products at each iteration.

Empirical performance of iMAML is studied on Omniglot and Mini-ImageNet domains. Following a few-shot learning protocol, iMAML runs on datasets with varying class labels and shots, comparing two iMAML variants with published results of MAML, FOMAML, and Reptile. The first iMAML variant solves the inner level problem using gradient descent, with meta-gradients computed using conjugate gradient and updated using Adam. The second variant employs a Hessian-free or Newton-CG method for the inner level problem, making local quadratic approximations and using CG for Newton search direction. The step size is determined using linesearch on training loss.

Results on Omniglot (Tables 2 and 3) show that the GD version of iMAML is competitive with full MAML and superior to approximations like first-order MAML and Reptile, especially in harder 20-way tasks. iMAML with Hessian-free optimization performs better than other methods, highlighting the benefits of powerful inner loop optimizers. In Mini-ImageNet, iMAML outperforms MAML and FOMAML. Using \(\lambda = 0.5\) and 10 inner loop gradient steps, the results can improve with better hyperparameters. Both iMAML variants use 5 CG steps for meta-gradient computation.