Meta-Learning with Implicit Gradients\n\nAravind Rajeswaran∗\nUniversity of Washington\n\nChelsea Finn∗\n\nUniversity of California Berkeley\n\naravraj@cs.washington.edu\n\ncbfinn@cs.stanford.edu\n\nSham M. Kakade\n\nUniversity of Washington\nsham@cs.washington.edu\n\nSergey Levine\n\nUniversity of California Berkeley\nsvlevine@eecs.berkeley.edu\n\nAbstract\n\nA core capability of intelligent systems is the ability to quickly learn new tasks by\ndrawing on prior experience. Gradient (or optimization) based meta-learning has\nrecently emerged as an effective approach for few-shot learning. In this formu-\nlation, meta-parameters are learned in the outer loop, while task-speciﬁc models\nare learned in the inner-loop, by using only a small amount of data from the cur-\nrent task. A key challenge in scaling these approaches is the need to differentiate\nthrough the inner loop learning process, which can impose considerable computa-\ntional and memory burdens. By drawing upon implicit differentiation, we develop\nthe implicit MAML algorithm, which depends only on the solution to the inner\nlevel optimization and not the path taken by the inner loop optimizer. This ef-\nfectively decouples the meta-gradient computation from the choice of inner loop\noptimizer. As a result, our approach is agnostic to the choice of inner loop opti-\nmizer and can gracefully handle many gradient steps without vanishing gradients\nor memory constraints. Theoretically, we prove that implicit MAML can compute\naccurate meta-gradients with a memory footprint no more than that which is re-\nquired to compute a single inner loop gradient and at no overall increase in the\ntotal computational cost. Experimentally, we show that these beneﬁts of implicit\nMAML translate into empirical gains on few-shot image recognition benchmarks.\n\n1\n\nIntroduction\n\nA core aspect of intelligence is the ability to quickly learn new tasks by drawing upon prior expe-\nrience from related tasks. Recent work has studied how meta-learning algorithms [51, 55, 41] can\nacquire such a capability by learning to efﬁciently learn a range of tasks, thereby enabling learn-\ning of a new task with as little as a single example [50, 57, 15]. Meta-learning algorithms can be\nframed in terms of recurrent [25, 50, 48] or attention-based [57, 38] models that are trained via a\nmeta-learning objective, to essentially encapsulate the learned learning procedure in the parameters\nof a neural network. An alternative formulation is to frame meta-learning as a bi-level optimization\nprocedure [35, 15], where the “inner” optimization represents adaptation to a given task, and the\n“outer” objective is the meta-training objective. Such a formulation can be used to learn the initial\nparameters of a model such that optimizing from this initialization leads to fast adaptation and gen-\neralization. In this work, we focus on this class of optimization-based methods, and in particular\nthe model-agnostic meta-learning (MAML) formulation [15]. MAML has been shown to be as ex-\npressive as black-box approaches [14], is applicable to a broad range of settings [16, 37, 1, 18], and\nrecovers a convergent and consistent optimization procedure [13].\n\n∗Equal Contributions. Project page: http://sites.google.com/view/imaml\n\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\n\n\u000cFigure 1: To compute the meta-gradient(cid:80)\n\ndLi(φi)\n\n, the MAML algorithm differentiates through\nthe optimization path, as shown in green, while ﬁrst-order MAML computes the meta-gradient by\napproximating dφi\ndθ as I. Our implicit MAML approach derives an analytic expression for the exact\nmeta-gradient without differentiating through the optimization path by estimating local curvature.\n\ndθ\n\ni\n\nDespite its appealing properties, meta-learning an initialization requires backpropagation through\nthe inner optimization process. As a result, the meta-learning process requires higher-order deriva-\ntives, imposes a non-trivial computational and memory burden, and can suffer from vanishing gra-\ndients. These limitations make it harder to scale optimization-based meta learning methods to tasks\ninvolving medium or large datasets, or those that require many inner-loop optimization steps. Our\ngoal is to develop an algorithm that addresses these limitations.\nThe main contribution of our work is the development of the implicit MAML (iMAML) algorithm,\nan approach for optimization-based meta-learning with deep neural networks that removes the need\nfor differentiating through the optimization path. Our algorithm aims to learn a set of parameters\nsuch that an optimization algorithm that is initialized at and regularized to this parameter vector\nleads to good generalization for a variety of learning tasks. By leveraging the implicit differentiation\napproach, we derive an analytical expression for the meta (or outer level) gradient that depends only\non the solution to the inner optimization and not the path taken by the inner optimization algorithm,\nas depicted in Figure 1. This decoupling of meta-gradient computation and choice of inner level\noptimizer has a number of appealing properties.\nFirst, the inner optimization path need not be stored nor differentiated through, thereby making\nimplicit MAML memory efﬁcient and scalable to a large number of inner optimization steps. Sec-\nond, implicit MAML is agnostic to the inner optimization method used, as long as it can ﬁnd an\napproximate solution to the inner-level optimization problem. This permits the use of higher-order\nmethods, and in principle even non-differentiable optimization methods or components like sample-\nbased optimization, line-search, or those provided by proprietary software (e.g. Gurobi). Finally, we\nalso provide the ﬁrst (to our knowledge) non-asymptotic theoretical analysis of bi-level optimiza-\ntion. We show that an \u0001–approximate meta-gradient can be computed via implicit MAML using\n˜O(log(1/\u0001)) gradient evaluations and ˜O(1) memory, meaning the memory required does not grow\nwith number of gradient steps.\n\n2 Problem Formulation and Notations\nWe ﬁrst present the meta-learning problem in the context of few-shot supervised learning, and then\ngeneralize the notation to aid the rest of the exposition in the paper.\n\n2.1 Review of Few-Shot Supervised Learning and MAML\nIn this setting, we have a collection of meta-training tasks {Ti}M\ni=1 drawn from P (T ). Each task Ti\nis associated with a dataset Di, from which we can sample two disjoint sets: Dtr\n. These\ndatasets each consist of K input-output pairs. Let x ∈ X and y ∈ Y denote inputs and outputs,\nrespectively. The datasets take the form Dtr\n. We are\ninterested in learning models of the form hφ(x) : X → Y, parameterized by φ ∈ Φ ≡ Rd.\nPerformance on a task is speciﬁed by a loss function, such as the cross entropy or squared error loss.\nWe will write the loss function in the form L(φ,D), as a function of a parameter vector and dataset.\nThe goal for task Ti is to learn task-speciﬁc parameters φi using Dtr\ni such that we can minimize the\npopulation or test loss of the task, L(φi,Dtest\n\ni and Dtest\nk=1, and similarly for Dtest\n\ni = {(xk\n\ni )}K\n\ni , yk\n\n).\n\ni\n\ni\n\ni\n\n2\n\n\u000cL\n\ni=1\n\n(cid:122)\n\ni\n\ni\n\nθ∈Θ\n\n1\nM\n\n(cid:125)(cid:124)\n\nIn the general bi-level meta-learning setup, we consider a space of algorithms that compute task-\nspeciﬁc parameters using a set of meta-parameters θ ∈ Θ ≡ Rd and the training dataset from the\ni ) for task Ti. The goal of meta-learning is to learn meta-parameters\ntask, such that φi = Alg(θ,Dtr\n(cid:123)\nthat produce good task speciﬁc parameters after adaptation, as speciﬁed below:\n\n(cid:19)\n(cid:123)\n(cid:1), Dtest\nWe view this as a bi-level optimization problem since we typically interpret Alg(cid:0)θ,Dtr\n\n(cid:18) inner−level\n(cid:122)\n(cid:125)(cid:124)\nAlg(cid:0)θ,Dtr\n\n(cid:1) as either\n\nθ∗\nML := argmin\n\nF (θ) , where F (θ) =\n\nM(cid:88)\n\nouter−level\n\nexplicitly or implicitly solving an underlying optimization problem. At meta-test (deployment) time,\nj corresponding to a new task Tj ∼ P (T ), we can achieve good\nwhen presented with a dataset Dtr\ngeneralization performance (i.e., low test error) by using the adaptation procedure with the meta-\nlearned parameters as φj = Alg(θ∗\nML,Dtr\nj ).\nIn the case of MAML [15], Alg(θ,D) corresponds to one or multiple steps of gradient descent\ninitialized at θ. For example, if one step of gradient descent is used, we have:\n\nφi ≡ Alg(θ,Dtr\n\ni ) = θ − α∇θL(θ,Dtr\ni ).\n\n(2)\nTypically, α is a scalar hyperparameter, but can also be a learned vector [34]. Hence, for MAML, the\nmeta-learned parameter (θ∗\nML) has a learned inductive bias that is particularly well-suited for ﬁne-\ntuning on tasks from P (T ) using K samples. To solve the outer-level problem with gradient-based\nmethods, we require a way to differentiate through Alg. In the case of MAML, this corresponds to\nbackpropagating through the dynamics of gradient descent.\n\n(inner-level of MAML)\n\n.\n\ni\n\n(1)\n\n2.2 Proximal Regularization in the Inner Level\nTo have sufﬁcient learning in the inner level while also avoiding over-ﬁtting, Alg needs to incorpo-\nrate some form of regularization. Since MAML uses a small number of gradient steps, this corre-\nsponds to early stopping and can be interpreted as a form of regularization and Bayesian prior [20].\nIn cases like ill-conditioned optimization landscapes and medium-shot learning, we may want to\ntake many gradient steps, which poses two challenges for MAML. First, we need to store and differ-\nentiate through the long optimization path of Alg, which imposes a considerable computation and\nmemory burden. Second, the dependence of the model-parameters {φi} on the meta-parameters (θ)\nshrinks and vanishes as the number of gradient steps in Alg grows, making meta-learning difﬁcult.\nTo overcome these limitations, we consider a more explicitly regularized algorithm:\n\nAlg(cid:63)(θ,Dtr\n\ni ) = argmin\nφ(cid:48)∈Φ\n\nL(φ(cid:48),Dtr\n\ni ) +\n\nλ\n2\n\n||φ(cid:48) − θ||2.\n\n(3)\n\nThe proximal regularization term in Eq. 3 encourages φi to remain close to θ, thereby retaining a\nstrong dependence throughout. The regularization strength (λ) plays a role similar to the learning\nrate (α) in MAML, controlling the strength of the prior (θ) relative to the data (DtrT ). Like α, the\nregularization strength λ may also be learned. Furthermore, both α and λ can be scalars, vectors, or\nfull matrices. For simplicity, we treat λ as a scalar hyperparameter. In Eq. 3, we use (cid:63) to denote that\nthe optimization problem is solved exactly. In practice, we use iterative algorithms (denoted by Alg)\nfor ﬁnite iterations, which return approximate minimizers. We explicitly consider the discrepancy\nbetween approximate and exact solutions in our analysis.\n\n2.3 The Bi-Level Optimization Problem\nFor notation convenience, we will sometimes express the dependence on task Ti using a subscript\ninstead of arguments, e.g. we write:\n\nLi(φ) := L(cid:0)φ, Dtest\n\n(cid:1),\n\ni\n\n(cid:1), Algi\n(cid:0)θ(cid:1) := Alg(cid:0)θ,Dtr\nˆLi(φ) := L(cid:0)φ,Dtr\nM(cid:88)\ni (θ)(cid:1), and\n(cid:0)Alg(cid:63)\n\nLi\n\ni\n\ni\n\n(cid:1).\n\n1\nM\n\ni=1\n\n(4)\n\nWith this notation, the bi-level meta-learning problem can be written more generally as:\n\nθ∗\nML := argmin\n\nθ∈Θ\n\nF (θ) , where F (θ) =\n\nAlg(cid:63)\n\ni (θ) := argmin\nφ(cid:48)∈Φ\n\nGi(φ(cid:48), θ), where Gi(φ(cid:48), θ) = ˆLi(φ(cid:48)) +\n\n||φ(cid:48) − θ||2.\n\nλ\n2\n\n3\n\n\u000c2.4 Total and Partial Derivatives\nWe use d to denote the total derivative and ∇ to denote partial derivative. For nested function of the\nform Li(φi) where φi = Algi(θ), we have from chain rule\n\ndθLi(Algi(θ)) =\n\n∇φLi(φ) |φ=Algi(θ) =\n\ndAlgi(θ)\n\ndθ\n\ndAlgi(θ)\n\ndθ\n\n∇φLi(Algi(θ))\n\nNote the important distinction between dθLi(Algi(θ)) and ∇φLi(Algi(θ)). The former passes\nderivatives through Algi(θ) while the latter does not. ∇φLi(Algi(θ)) is simply the gradient func-\ntion, i.e. ∇φLi(φ), evaluated at φ = Algi(θ). Also note that dθLi(Algi(θ)) and ∇φLi(Algi(θ))\nare d–dimensional vectors, while dAlgi(θ)\nis a (d × d)–size Jacobian matrix. Throughout this text,\ndθ interchangeably.