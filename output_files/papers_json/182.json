{
    "paper_id": "182",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-06T18:06:53.929127Z"
    },
    "title": "Modeling Contextual Relationships Among Utterances for Multimodal Sentiment Analysis",
    "authors": [],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Multimodal sentiment analysis is a developing area of research, which involves identification of emotions and sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among utterances of a video. In this paper, we propose an LSTM based model which enables these utterances to capture contextual information from its surroundings in the same video, thus aiding the classification process. Our model shows 5 -10% improvement over the state of the art and high robustness to generalizability.",
    "pdf_parse": {
        "paper_id": "182",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Multimodal sentiment analysis is a developing area of research, which involves identification of emotions and sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among utterances of a video. In this paper, we propose an LSTM based model which enables these utterances to capture contextual information from its surroundings in the same video, thus aiding the classification process. Our model shows 5 -10% improvement over the state of the art and high robustness to generalizability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Emotion recognition and sentiment analysis have become a new trend in social media, helping users to automatically extract the opinions expressed in user-generated content, especially videos. Thanks to the high availability of computers and smartphones, and the rapid rise of social media, consumers tend to record their reviews and opinions about products or films and upload them on social media platforms, such as YouTube or Facebook. Such videos often contain comparisons, which can aid prospective buyers make an informed decision.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities. The vocal modulations and facial expressions in the visual data, along with textual data, provide important cues to better identify affective states of the opinion holder. Thus, a combination of text and video data helps to create a better emotion and sentiment analysis model (Poria et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 403,
                        "end": 423,
                        "text": "(Poria et al., 2017)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recently, a number of approaches to multimodal sentiment analysis, producing interesting results, have been proposed (P\u00e9rez-Rosas et al., 2013; Wollmer et al., 2013; Poria et al., 2015) . However, there are major issues that remain unaddressed, such as the role of speaker-dependent versus speaker-independent models, the impact of each modality across the dataset, and generalization ability of a multimodal sentiment classifier. Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 143,
                        "text": "(P\u00e9rez-Rosas et al., 2013;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 144,
                        "end": 165,
                        "text": "Wollmer et al., 2013;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 166,
                        "end": 185,
                        "text": "Poria et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "An utterance is a unit of speech bound by breathes or pauses. Utterance-level sentiment analysis focuses on tagging every utterance of a video with a sentiment label (instead of assigning a unique label to the whole video). In particular, utterance-level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his/her speech. The true meaning of an utterance is relative to its surrounding utterances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we consider such surrounding utterances to be the context, as the consideration of temporal relation and dependency among utterances is key in human-human communication. For example, the MOSI dataset (Zadeh et al., 2016 ) contains a video, in which a girl reviews the movie 'Green Hornet'. At one point, she says \"The Green Hornet did something similar\". Normally, doing something similar, i.e., monotonous or repetitive might be perceived as negative. However, the nearby utterances \"It engages the audience more\", \"they took a new spin on it\", \"and I just loved it\" indicate a positive context.",
                "cite_spans": [
                    {
                        "start": 215,
                        "end": 234,
                        "text": "(Zadeh et al., 2016",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we discard the oversimplifying hypothesis on the independence of utterances and develop a framework based on long short-term memory (LSTM) to extract utterance features that also consider surrounding utterances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model enables consecutive utterances to share information, thus providing contextual information in the classification process. Experimental results show that the proposed framework has outperformed the state of the art on benchmark datasets by 5-10%. The paper is organized as follows: Section 2 provides a brief literature review on multimodal sentiment analysis; Section 3 describes the proposed method in detail; experimental results and discussion are shown in Section 4; finally, Section 5 concludes the paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Text-based sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based systems (Cambria, 2016) . While the use of knowledge bases was initially more popular for the identification of emotions and polarity in text, sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods (Pang et al., 2002; Socher et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 130,
                        "text": "(Cambria, 2016)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 390,
                        "end": 409,
                        "text": "(Pang et al., 2002;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 410,
                        "end": 430,
                        "text": "Socher et al., 2013)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In 1970, Ekman (Ekman, 1974) carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions. Recent studies on speech-based emotion analysis (Datcu and Rothkrantz, 2008) have focused on identifying relevant acoustic features, such as fundamental frequency (pitch), intensity of utterance, bandwidth, and duration.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 28,
                        "text": "(Ekman, 1974)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 233,
                        "end": 261,
                        "text": "(Datcu and Rothkrantz, 2008)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "As for fusing audio and visual modalities for emotion recognition, two of the early works were done by De Silva et al. (De Silva et al., 1997) and Chen et al. (Chen et al., 1998) . Both works showed that a bimodal system yielded a higher accuracy than any unimodal system. More recent research on audio-visual fusion for emotion recognition has been conducted at either feature level (Kessous et al., 2010) or decision level (Schuller, 2011) .",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 142,
                        "text": "De Silva et al. (De Silva et al., 1997)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 147,
                        "end": 178,
                        "text": "Chen et al. (Chen et al., 1998)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 384,
                        "end": 406,
                        "text": "(Kessous et al., 2010)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 425,
                        "end": 441,
                        "text": "(Schuller, 2011)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "While there are many research papers on audiovisual fusion for emotion recognition, only a few have been devoted to multimodal emotion or sentiment analysis using textual clues along with visual and audio modalities. Wollmer et al. (Wollmer et al., 2013) and Rozgic et al. (Rozgic et al., 2012a,b) fused information from audio, visual, and textual modalities to extract emotion and sentiment. Metallinou et al. (Metallinou et al., 2008) and Eyben et al. (Eyben et al., 2010a) fused audio and textual modalities for emotion recognition.",
                "cite_spans": [
                    {
                        "start": 232,
                        "end": 254,
                        "text": "(Wollmer et al., 2013)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 259,
                        "end": 297,
                        "text": "Rozgic et al. (Rozgic et al., 2012a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 393,
                        "end": 436,
                        "text": "Metallinou et al. (Metallinou et al., 2008)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 441,
                        "end": 475,
                        "text": "Eyben et al. (Eyben et al., 2010a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Both approaches relied on a feature-level fusion. Wu et al. (Wu and Liang, 2011) fused audio and textual clues at decision level.",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 80,
                        "text": "Wu et al. (Wu and Liang, 2011)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this work, we propose a LSTM network that takes as input all utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances. Below, we propose an overview of the method -",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "1. Context-Independent Unimodal Utterance- Level Feature Extraction",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "First, the unimodal features are extracted without considering the contextual information of the utterances (Section 3.1). Table 1 presents the feature extraction methods used for each modality.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 129,
                        "end": 130,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "The context-independent unimodal features (from Step 1) are then fed into a LSTM network (termed contextual LSTM) that allows consecutive utterances in a video to share semantic information in the feature extraction process (which provides context-dependent unimodal and multimodal classification of the utterances). We experimentally show that this proposed framework improves the performance of utterance-level sentiment classification over traditional frameworks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Unimodal and Multimodal Classification",
                "sec_num": "2."
            },
            {
                "text": "Videos, comprising of its constituent utterances, serve as the input. We represent the dataset as U:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Unimodal and Multimodal Classification",
                "sec_num": "2."
            },
            {
                "text": "U = \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 u1,1 u1,2 u1,3 ... u 1,L 1 u2,1 u2,2 u2,3 ... u 2,L 2 . . . ... . u M,1 u M,2 u M,3 ... u M,L M \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Unimodal and Multimodal Classification",
                "sec_num": "2."
            },
            {
                "text": "Here, u i,j denotes the j th utterance of the i th video and L = [L 1 , L 2 , ..., L M ] represents the number of utterances per video in the dataset set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Unimodal and Multimodal Classification",
                "sec_num": "2."
            },
            {
                "text": "Initially, the unimodal features are extracted from each utterance separately, i.e., we do not consider the contextual relation and dependency among the utterances (Table 1 ). Below, we explain the textual, audio, and visual feature extraction methods.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 171,
                        "end": 172,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Extracting Context-Independent Unimodal Features",
                "sec_num": "3.1"
            },
            {
                "text": "For feature extraction from textual data, we use a convolutional neural network (CNN).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "The idea behind convolution is to take the dot product of a vector of k weights, w k , known as kernel vector, with each k-gram in the sentence s(t) to obtain another sequence of features c(t) = (c 1 (t), c 2 (t), . . . , c L (t)):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "c j = w T k \u22c5 x i\u2236i+k-1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "We then apply a max pooling operation over the feature map and take the maximum value \u0109(t) = max{c(t)} as the feature corresponding to this particular kernel vector. We use varying kernel vectors and window sizes to obtain multiple features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "The process of extracting textual features is as follows -First, we represent each sentence as the concatenation of vectors of the constituent words. These vectors are the publicly available 300-dimensional word2vec vectors trained on 100 billion words from Google News (Mikolov et al., 2013) . The convolution kernels are thus applied to these word vectors instead of individual words. Each sentence is wrapped to a window of 50 words which serves as the input to the CNN.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 292,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "The CNN has two convolutional layers -the first layer having a kernel size of 3 and 4, with 50 feature maps each and a kernel size 2 with 100 feature maps for the second. The convolution layers are interleaved with pooling layers of dimension 2. We use ReLU as the activation function. The convolution of the CNN over the sentence learns abstract representations of the phrases equipped with implicit semantic information, which with each successive layer spans over increasing number of words and ultimately the entire sentence. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "text-CNN: Textual Features Extraction",
                "sec_num": "3.1.1"
            },
            {
                "text": "Audio features are extracted in 30 Hz frame-rate; we use a sliding window of 100 ms. To compute the features, we use the open-source software openSMILE (Eyben et al., 2010b) which automatically extracts pitch and voice intensity. Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice. Z-standardization is used to perform voice normalization.",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 173,
                        "text": "(Eyben et al., 2010b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio Feature Extraction",
                "sec_num": "3.1.2"
            },
            {
                "text": "The features extracted by openSMILE consist of several low-level descriptors (LLD) and their statistical functionals. Some of the functionals are amplitude mean, arithmetic mean, root quadratic mean, etc. Taking into account all functionals of each LLD, we obtained 6373 features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio Feature Extraction",
                "sec_num": "3.1.2"
            },
            {
                "text": "We use 3D-CNN to obtain visual features from the video. We hypothesize that 3D-CNN will not only be able to learn relevant features from each frame, but will also be able to learn the changes among given number of consecutive frames.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": "In the past, 3D-CNN has been successfully applied to object classification on 3D data (Ji et al., 2013) . Its ability to achieve state-of-the-art results motivated us to use it.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 103,
                        "text": "(Ji et al., 2013)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": "Let vid \u2208 R c\u00d7f \u00d7h\u00d7w be a video, where c = number of channels in an image (in our case c = 3, since we consider only RGB images), f = number of frames, h = height of the frames, and w = width of the frames. Again, we consider the 3D convolutional filter f ilt \u2208 R f m\u00d7c\u00d7f l\u00d7f h\u00d7f w , where f m = number of feature maps, c = number of channels, f d = number of frames (in other words depth of the filter), f h = height of the filter, and f w = width of the filter. Similar to 2D-CNN, f ilt slides across video vid and generates output convout \u2208 R f m\u00d7c\u00d7(f -f d+1)\u00d7(h-f h+1)\u00d7(w-f w+1) . Next, we apply max pooling to convout to select only relevant features. The pooling will be applied only to the last three dimensions of the array convout.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": "In our experiments, we obtained best results with 32 feature maps (f m) with the filter-size of",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": "5 \u00d7 5 \u00d7 5 (or f d \u00d7 f h \u00d7 f w). In other words, the dimension of the filter is 32 \u00d7 3 \u00d7 5 \u00d7 5 \u00d7 5 (or f m \u00d7 c \u00d7 f d \u00d7 f h \u00d7 f w)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": ". Subsequently, we apply max pooling on the output of convolution operation, with window-size being 3 \u00d7 3 \u00d7 3. This is followed by a dense layer of size 300 and softmax. The activations of this dense layer are finally used as the video features for each utterance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Feature Extraction",
                "sec_num": "3.1.3"
            },
            {
                "text": "We hypothesize that, within a video, there is a high probability of utterance relatedness with respect to their sentimental and emotional clues. Since most videos tend to be about a single topic, the utterances within each video are correlated, e.g., due to the development of the speaker's idea, coreferences, etc. This calls for a model which takes into account such inter-dependencies and the effect these might have on the current utterance. To capture this flow of informational triggers across utterances, we use a LSTM-based recurrent network scheme (Gers, 2001) .",
                "cite_spans": [
                    {
                        "start": 557,
                        "end": 569,
                        "text": "(Gers, 2001)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context-Dependent Feature Extraction",
                "sec_num": "3.2"
            },
            {
                "text": "LSTM is a kind of recurrent neural network (RNN), an extension of conventional feed-forward neural network. Specifically, LSTM cells are capable of modeling long-range dependencies, which other traditional RNNs fail to do given the vanishing gradient issue. Each LSTM cell consists of an input gate i, an output gate o, and a forget gate f , which enables it to remember the error during the error propagation. Current research (Zhou et al., 2016) indicates the benefit of using such networks to incorporate contextual information in the classification process.",
                "cite_spans": [
                    {
                        "start": 428,
                        "end": 447,
                        "text": "(Zhou et al., 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Long Short-Term Memory",
                "sec_num": "3.2.1"
            },
            {
                "text": "In our case, the LSTM network serves the purpose of context-dependent feature extraction by modeling relations among utterances. We term our architecture 'contextual LSTM'. We propose several architectural variants of it later in the paper. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Long Short-Term Memory",
                "sec_num": "3.2.1"
            },
            {
                "text": "Let unimodal features have dimension k, each utterance is thus represented by a feature vector x i,t \u2208 R k , where t represents the t th utterance of the video i. For a video, we collect the vectors for all the utterances in it, to get",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual LSTM Architecture",
                "sec_num": "3.2.2"
            },
            {
                "text": "X i = [x i,1 , x i,2 , ..., x i,L i ] \u2208 R L i \u00d7k",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual LSTM Architecture",
                "sec_num": "3.2.2"
            },
            {
                "text": ", where L i represents the number of utterances in the video. This matrix X i serves as the input to the LSTM. Figure 1 demonstrates the functioning of this LSTM module.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 118,
                        "end": 119,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Contextual LSTM Architecture",
                "sec_num": "3.2.2"
            },
            {
                "text": "In the procedure getLstmFeatures(X i ) of Algorithm 1, each of these utterance x i,t is passed through a LSTM cell using the equations mentioned in line 32 to 37. The output of the LSTM cell h i,t is then fed into a dense layer and finally into a softmax layer (line 38 to 39). The activations of the dense layer z i,t are used as the contextdependent features of contextual LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual LSTM Architecture",
                "sec_num": "3.2.2"
            },
            {
                "text": "The training of the LSTM network is performed using categorical cross entropy on each utterance's softmax output per video, i.e., A dropout layer between the LSTM cell and dense layer is introduced to check overfitting. As the videos do not have same the number of utterances, padding is introduced to serve as neutral utterances. To avoid the proliferation of noise within the network, masking is done on these padded utterances to eliminate their effect in the network. Parameter tuning is done on the train set by splitting it into train and validation components with 80 20% split. RMSprop has been used as the optimizer which is known to resolve Adagrad's radically diminishing learning rates (Duchi et al., 2011) . After feeding the train set to the network, the test set is passed through it to generate their context-dependent features. LSTM cells. As this is the simple variant of the contextual LSTM, we termed it as simple contextual LSTM (sc-LSTM) h-LSTM We also test on an architecture where the dense layer after the LSTM cell is omitted. Thus, the output of the LSTM cell h i,t provides our context-dependent features and the softmax layer provides the classification. We call this architecture hidden LSTM (h-LSTM).",
                "cite_spans": [
                    {
                        "start": 698,
                        "end": 718,
                        "text": "(Duchi et al., 2011)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.2.3"
            },
            {
                "text": "bc-LSTM Bi-directional LSTMs are two unidirectional LSTMs stacked together having opposite directions. Thus, an utterance can get information from other utterances occurring before and after itself in the video. We replaced the regular LSTM with a bi-directional LSTM and named the resulting architecture as bi-directional contextual LSTM (bc-LSTM). The training process of this architecture is similar to sc-LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Different Network Architectures",
                "sec_num": null
            },
            {
                "text": "uni-SVM In this setting, we first obtain the unimodal features as explained in Section 3.1, concatenate them and then send to a SVM for the final classification. It should be noted that using a gated recurrent unit (GRU) instead of LSTM did not improve the performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Different Network Architectures",
                "sec_num": null
            },
            {
                "text": "We accomplish multimodal fusion in two different ways as explained below -",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fusion of Modalities",
                "sec_num": "3.3"
            },
            {
                "text": "In non-hierarchical framework, we concatenate context-independent unimodal features (from Section 3.1) and feed that into the contextual LSTM networks, i.e., sc-LSTM, bc-LSTM, and h-LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-hierarchical Framework",
                "sec_num": "3.3.1"
            },
            {
                "text": "Contextual unimodal features, taken as input, can further improve performance of the multimodal fusion framework explained in Section 3.3.1. To accomplish this, we propose a hierarchical deep network which comprises of two levels -Level-1: context-independent unimodal features (from 3.1) are fed to the proposed LSTM network (Section 3.2.2) to get context-sensitive unimodal feature representations for each utterance. Individual LSTM networks are used for each modality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hierarchical Framework",
                "sec_num": "3.3.2"
            },
            {
                "text": "Level-2: consists of a contextual LSTM network similar to Level-1 but independent in training and computation. Output from each LSTM network in Level-1 are concatenated and fed into this LSTM network, thus providing an inherent fusion scheme -the prime objective of this level (Fig 2 ). The performance of the second level banks on the quality of the features from the previous level, with better features aiding the fusion process. Algorithm 1 describes the overall computation for utterance classification. For the hierarchical framework, we train Level 1 and Level 2 successively but separately. 4 Experimental Results",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 282,
                        "end": 283,
                        "text": "2",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Hierarchical Framework",
                "sec_num": "3.3.2"
            },
            {
                "text": "Weight Bias Wi, W f , Wc, Wo \u2208 R d\u00d7k bi, b f , bc, bo \u2208 R d Pi, P f , Pc, PoVo \u2208 R d\u00d7d bz \u2208 R m Wz \u2208 R m\u00d7d b sf t \u2208 R c W sf t \u2208 R c\u00d7m",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hierarchical Framework",
                "sec_num": "3.3.2"
            },
            {
                "text": "Most of the research in multimodal sentiment analysis is performed on datasets with speaker overlap in train and test splits.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "Because each individual has a unique way of expressing emotions and sentiments, finding generic, person-independent features for sentimental analysis is very tricky. In real-world applications, the Algorithm 1 Proposed Architecture 1: procedure TRAINARCHITECTURE( U, V) 2:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "Train context-independent models with U 3:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "for i:[1,M] do \u25b7 extract baseline features 4:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "for j:[1,Li] do 5:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "xi,j \u2190 T extF eatures(ui,j) 6:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "x \u2032 i,j \u2190 V ideoF eatures(ui,j) 7:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "x \" i,j \u2190 AudioF eatures(ui,j)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset details",
                "sec_num": "4.1"
            },
            {
                "text": "Unimodal: 9:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Train LSTM at Level-1 with X, X \u2032 andX \" . 10:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "for i:[1,M ] do \u25b7 unimodal features 11:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Zi \u2190 getLST M F eatures(Xi) 12:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Z \u2032 i \u2190 getLST M F eatures(X \u2032 i ) 13: Z \" i \u2190 getLST M F eatures(X \" i )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "14: Multimodal: 15:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "for i:[1,M] do 16:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "for j:[1,Li] do 17:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "if Non-hierarchical fusion then 18:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "x",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "* i,j \u2190 (xi,j x \u2032 i,j x \" i,j ) \u25b7 concatenation 19:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "else 20:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "if Hierarchical fusion then 21:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "x * i,j \u2190 (zi,j z \u2032 i,j z \" i,j ) \u25b7 concatenation 22:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Train LSTM at Level-2 with X * . 23:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "for i:[1,M ] do \u25b7 multimodal features 24: Z * i \u2190 getLST M F eatures(X * i ) 25: testArchitecture( V) 26:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "return Z * 27: procedure TESTARCHITECTURE( V) 28: Similar to training phase. V is passed through the learnt models to get the features and classification outputs. Table 2 shows the trainable parameters. 29: procedure GETLSTMFEATURES(Xi) \u25b7 for i th video 30:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 169,
                        "end": 170,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Zi \u2190 \u03c6 31:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "for t:[1,Li] do \u25b7 Table 2 provides notation 32:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 24,
                        "end": 25,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "it \u2190 \u03c3(Wixi,t + Pi.ht-1 + bi) 33:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "Ct \u2190 tanh(Wcxi,t + Pcht-1 + bc) 34: ft \u2190 \u03c3(W f xt + P f ht-1 + b f ) 35: Ct \u2190 it * Ct + ft * Ct-1 36: ot \u2190 \u03c3(Woxt + Poht-1 + VoCt + bo) 37: ht \u2190 ot * tanh(Ct) \u25b7 output of lstm cell 38: zt \u2190 ReLU (Wzht + bz) \u25b7 dense layer 39: prediction \u2190 sof tmax(W sf t zt + b sf t ) 40: Zi \u2190 Zi \u222a zt 41:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "return Zi model should be robust to person variance but it is very difficult to come up with a generalized model from the behavior of a limited number of individuals To this end, we perform person-independent experiments to emulate unseen conditions. Our train/test splits of the datasets are completely disjoint with respect to speakers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "While testing, our models have to classify emotions and sentiments from utterances by speakers they have never seen before.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "8:",
                "sec_num": null
            },
            {
                "text": "The IEMOCAP contains the acts of 10 speakers in a two way conversation segmented into utterances. The database contains the following categorical labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other, but we take only the first four so as to compare with the state of the art (Rozgic et al., 2012b) and other authors. Videos by the first 8 speakers are considered in the train set. The train/test split details are provided in table 3 .",
                "cite_spans": [
                    {
                        "start": 317,
                        "end": 339,
                        "text": "(Rozgic et al., 2012b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 474,
                        "end": 475,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "IEMOCAP:",
                "sec_num": null
            },
            {
                "text": "The MOSI dataset is a dataset rich in sentimental expressions where 93 persons review topics in English. It contains positive and negative classes as its sentiment labels. The train/validation set comprises of the first 62 individuals in the dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOSI:",
                "sec_num": null
            },
            {
                "text": "MOUD: This dataset contains product review videos provided by around 55 persons. The reviews are in Spanish (we use Google Translate API1 to get the english transcripts). The utterances are labeled to be either positive, negative or neutral. However, we drop the neutral label to maintain consistency with previous work. The first 59 videos are considered in the train/val set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOSI:",
                "sec_num": null
            },
            {
                "text": "Table 3 provides information regarding train/test split of all the datasets. In these splits it is ensured that 1) No two utterances from the train and test splits belong to the same video. 2) The train/test splits have no speaker overlap. This provides the speaker-independent setting.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "MOSI:",
                "sec_num": null
            },
            {
                "text": "Table 3 It should be noted that the datasets' individual configuration and splits are same throughout all the experiments (i.e., context-independent unimodal feature extraction, LSTM-based context- dependent unimodal and multimodal feature extraction and classification).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "MOSI:",
                "sec_num": null
            },
            {
                "text": "In this section, we present unimodal and multimodal sentiment analysis performance of different LSTM network variants as explained in Section 3.2.3 and comparison with the state of the art.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Performance of Different Models and Comparisons",
                "sec_num": "4.2"
            },
            {
                "text": "Framework -As expected, trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework. Table 4 demonstrates this by comparing both hierarchical and non-hierarchical framework using the bc-LSTM network. Due to this fact, we provide all further analysis and results using the hierarchical framework. Nonhierarchical model outperforms the performance of the baseline uni-SVM. This further leads us to conclude that it is the context-sensitive learning paradigm which plays the key role in improving performance over the baseline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Hierarchical vs Non-hierarchical Fusion",
                "sec_num": null
            },
            {
                "text": "Comparison among Network Variants -It is to be noted that both sc-LSTM and bc-LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets. Since, bc-LSTM has access to both the preceding and following information of the utterance sequence, it performs consistently better on all the datasets over sc-LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hierarchical vs Non-hierarchical Fusion",
                "sec_num": null
            },
            {
                "text": "The usefulness of the dense layer in improving the performance is prominent from the experimental results as shown in Table 4 . The performance improvement is in the range of 0.3% to 1.5% on MOSI and MOUD datasets. On the IEMOCAP dataset, the performance improvement of bc-LSTM and sc-LSTM over h-LSTM is in the range of 1% to 5%.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 124,
                        "end": 125,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Hierarchical vs Non-hierarchical Fusion",
                "sec_num": null
            },
            {
                "text": "Comparison with the Baseline and state of the art -Every LSTM network variant has outperformed the baseline uni-SVM on all the datasets by the margin of 2% to 5%(see Table 4 ). These results prove our initial hypothesis that modeling the contextual dependencies among utterances, which uni-SVM cannot do, improves the classification. The higher performance improvement on the IEMOCAP dataset indicates the necessity of modeling long-range dependencies among the utterances as continuous emotion recognition is a multiclass sequential problem where a person doesnt frequently change emotions (W\u00f6llmer et al., 2008) .",
                "cite_spans": [
                    {
                        "start": 591,
                        "end": 613,
                        "text": "(W\u00f6llmer et al., 2008)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 172,
                        "end": 173,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Hierarchical vs Non-hierarchical Fusion",
                "sec_num": null
            },
            {
                "text": "We have implemented and compared with the current state-of-the-art approach proposed by Poria et al. (Poria et al., 2015) . In their method, they extracted features from each modality and fed to a multiple kernel learning (MKL) classifier. However, they did not conduct the experiment in speaker-independent manner and also did not consider the contextual relation among the utterances. Experimental results in Table 5 shows that the proposed method has outperformed Poria et al. (Poria et al., 2015) by a significant margin. For the emotion recognition task, we have compared our method with the current state of the art (Rozgic et al., 2012b) , who extracted features in a similar fashion to (Poria et al., 2015) did. However, for fusion they used SVM trees.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 121,
                        "text": "(Poria et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 467,
                        "end": 500,
                        "text": "Poria et al. (Poria et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 622,
                        "end": 644,
                        "text": "(Rozgic et al., 2012b)",
                        "ref_id": null
                    },
                    {
                        "start": 694,
                        "end": 714,
                        "text": "(Poria et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 417,
                        "end": 418,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Hierarchical vs Non-hierarchical Fusion",
                "sec_num": null
            },
            {
                "text": "As expected, in all kinds of experiments, bimodal and trimodal models have outperformed unimodal models. Overall, audio modality has performed better than visual on all the datasets. On MOSI and IEMOCAP datasets, textual classifier achieves the best performance over other unimodal classifiers. On IEMOCAP dataset, the unimodal and multimodal classifiers obtained poor performance to classify neutral utterances. Textual modality, combined with non-textual modes boosts the performance in IEMOCAP by a large margin. However, the margin is less in the other datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of the Modalities",
                "sec_num": "4.3"
            },
            {
                "text": "On the MOUD dataset, textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English. Using Spanish word vectors2 in text-CNN results in an improvement of 10% . Nonetheless, we report results using these translated utterances as opposed to utterances trained on Spanish word vectors, in order to make fair comparison with (Poria et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 383,
                        "end": 403,
                        "text": "(Poria et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of the Modalities",
                "sec_num": "4.3"
            },
            {
                "text": "To test the generalizability of the models, we have trained our framework on complete MOSI dataset and tested on MOUD dataset (Table 6 ). The performance was poor for audio and textual modality as the MOUD dataset is in Spanish while the model is trained on MOSI dataset which is in En- glish language. However, notably visual modality performs better than other two modalities in this experiment which signifies that in cross-lingual scenarios facial expressions carry more generalized, robust information than audio and textual modalities. We could not carry out the similar experiment for emotion recognition as no other utterance-level dataset apart from the IEMOCAP was available at the time of our experiments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 133,
                        "end": 134,
                        "text": "6",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Generalization of the Models",
                "sec_num": "4.4"
            },
            {
                "text": "In some cases the predictions of the proposed method are wrong given the difficulty in recognizing the face and noisy audio signal in the utterances. Also, cases where the sentiment is very weak and non contextual, the proposed approach shows some bias towards its surrounding utter-ances which further leads to wrong predictions. This can be solved by developing a context aware attention mechanism. In order to have a better understanding on roles of modalities for overall classification, we also have done some qualitative analysis. For example, this utterance -\"who doesn't have any presence or greatness at all.\", was classified as positive by the audio classifier (\"doesn't\" was spoken normally by the speaker, but \"presence and greatness at all\" was spoken with enthusiasm). However, textual modality caught the negation induced by \"doesn't\" and classified correctly. In another utterance \"amazing special effects\" as there was no jest of enthusiasm in speaker's voice and face audio-visual classifier failed to identify the positivity of this utterance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "On the other textual classifier correctly detected the polarity as positive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "On the other hand, textual classifier classified this sentence -\"that like to see comic book characters treated responsibly\" as positive, possibly because of the presence of positive phrases such as \"like to see\", \"responsibly\". However, the high pitch of anger in the person's voice and the frowning face helps identify this to be a negative utterance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "Contextual relationship among the utterances is mostly ignored in the literature. In this paper, we developed a LSTM-based network to extract contextual features from the utterances of a video for multimodal sentiment analysis. The proposed method has outperformed the state of the art and showed significant performance improvement over the baseline. As a part of the future work, we plan to propose LSTM attention model to determine importance of the utterances and contribution of modalities in the sentiment classification. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Chung-Hsien",
                "sec_num": null
            },
            {
                "text": "http://translate.google.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://crscardellino.me/SBWCE",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Affective computing and sentiment analysis",
                "authors": [
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "IEEE Intelligent Systems",
                "volume": "31",
                "issue": "2",
                "pages": "102--107",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erik Cambria. 2016. Affective computing and senti- ment analysis. IEEE Intelligent Systems 31(2):102- 107.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Multimodal human emotion/expression recognition",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [
                            "S"
                        ],
                        "last": "Lawrence S Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Tsutomu",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Ryohei",
                        "middle": [],
                        "last": "Miyasato",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakatsu",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition",
                "volume": "",
                "issue": "",
                "pages": "366--371",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lawrence S Chen, Thomas S Huang, Tsutomu Miyasato, and Ryohei Nakatsu. 1998. Multimodal human emotion/expression recognition. In Proceed- ings of the Third IEEE International Conference on Automatic Face and Gesture Recognition. IEEE, pages 366-371.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Semantic audio-visual data fusion for automatic emotion recognition",
                "authors": [
                    {
                        "first": "Dragos",
                        "middle": [],
                        "last": "Datcu",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Rothkrantz",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Euromedia",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dragos Datcu and L Rothkrantz. 2008. Seman- tic audio-visual data fusion for automatic emotion recognition. Euromedia'2008 .",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Facial emotion recognition using multi-modal information",
                "authors": [
                    {
                        "first": "Liyanage C De",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    },
                    {
                        "first": "Tsutomu",
                        "middle": [],
                        "last": "Miyasato",
                        "suffix": ""
                    },
                    {
                        "first": "Ryohei",
                        "middle": [],
                        "last": "Nakatsu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of ICICS. IEEE",
                "volume": "1",
                "issue": "",
                "pages": "397--401",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liyanage C De Silva, Tsutomu Miyasato, and Ryohei Nakatsu. 1997. Facial emotion recognition using multi-modal information. In Proceedings of ICICS. IEEE, volume 1, pages 397-401.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2121--2159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12(Jul):2121-2159.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Universal facial expressions of emotion. Culture and Personality",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    }
                ],
                "year": 1974,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Ekman. 1974. Universal facial expressions of emotion. Culture and Personality: Contemporary Readings/Chicago .",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues",
                "authors": [
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "W\u00f6llmer",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Douglas-Cowie",
                        "suffix": ""
                    },
                    {
                        "first": "Roddy",
                        "middle": [],
                        "last": "Cowie",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal on Multimodal User Interfaces",
                "volume": "3",
                "issue": "1-2",
                "pages": "7--19",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Florian Eyben, Martin W\u00f6llmer, Alex Graves, Bj\u00f6rn Schuller, Ellen Douglas-Cowie, and Roddy Cowie. 2010a. On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues. Journal on Multimodal User In- terfaces 3(1-2):7-19.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
                "authors": [
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "W\u00f6llmer",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 18th ACM international conference on Multimedia",
                "volume": "",
                "issue": "",
                "pages": "1459--1462",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Florian Eyben, Martin W\u00f6llmer, and Bj\u00f6rn Schuller. 2010b. Opensmile: the munich versatile and fast open-source audio feature extractor. In Proceedings of the 18th ACM international conference on Multi- media. ACM, pages 1459-1462.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Long Short-Term Memory in Recurrent Neural Networks",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Gers",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felix Gers. 2001. Long Short-Term Memory in Re- current Neural Networks. Ph.D. thesis, Universit\u00e4t Hannover.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "3d convolutional neural networks for human action recognition",
                "authors": [
                    {
                        "first": "Shuiwang",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE transactions on pattern analysis and machine intelligence",
                "volume": "35",
                "issue": "1",
                "pages": "221--231",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2013. 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence 35(1):221-231.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
                "authors": [
                    {
                        "first": "Loic",
                        "middle": [],
                        "last": "Kessous",
                        "suffix": ""
                    },
                    {
                        "first": "Ginevra",
                        "middle": [],
                        "last": "Castellano",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Caridakis",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal on Multimodal User Interfaces",
                "volume": "3",
                "issue": "1-2",
                "pages": "33--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Loic Kessous, Ginevra Castellano, and George Cari- dakis. 2010. Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis. Journal on Mul- timodal User Interfaces 3(1-2):33-48.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Audio-visual emotion recognition using gaussian mixture models for face and voice",
                "authors": [
                    {
                        "first": "Angeliki",
                        "middle": [],
                        "last": "Metallinou",
                        "suffix": ""
                    },
                    {
                        "first": "Sungbok",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Shrikanth",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Tenth IEEE International Symposium on ISM 2008",
                "volume": "",
                "issue": "",
                "pages": "250--257",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Angeliki Metallinou, Sungbok Lee, and Shrikanth Narayanan. 2008. Audio-visual emotion recogni- tion using gaussian mixture models for face and voice. In Tenth IEEE International Symposium on ISM 2008. IEEE, pages 250-257.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 .",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Thumbs up?: sentiment classification using machine learning techniques",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Shivakumar",
                        "middle": [],
                        "last": "Vaithyanathan",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification us- ing machine learning techniques. In Proceedings of ACL. Association for Computational Linguistics, pages 79-86.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Utterance-level multimodal sentiment analysis",
                "authors": [
                    {
                        "first": "Ver\u00f3nica",
                        "middle": [],
                        "last": "P\u00e9rez-Rosas",
                        "suffix": ""
                    },
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ACL (1)",
                "volume": "",
                "issue": "",
                "pages": "973--982",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, and Louis- Philippe Morency. 2013. Utterance-level multi- modal sentiment analysis. In ACL (1). pages 973- 982.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
                "authors": [
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    },
                    {
                        "first": "Rajiv",
                        "middle": [],
                        "last": "Bajpai",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Hussain",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of affective computing: From unimodal analysis to multimodal fusion. In- formation Fusion .",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
                "authors": [
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2539--2544",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soujanya Poria, Erik Cambria, and Alexander Gel- bukh. 2015. Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis. In Proceedings of EMNLP. pages 2539-2544.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Ensemble of SVM trees for multimodal emotion recognition",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Rozgic",
                        "suffix": ""
                    },
                    {
                        "first": "Sankaranarayanan",
                        "middle": [],
                        "last": "Ananthakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Shirin",
                        "middle": [],
                        "last": "Saleem",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Prasad",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of APSIPA ASC",
                "volume": "",
                "issue": "",
                "pages": "1--4",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Rozgic, Sankaranarayanan Ananthakrishnan, Shirin Saleem, Rohit Kumar, and Rohit Prasad. 2012a. En- semble of SVM trees for multimodal emotion recog- nition. In Proceedings of APSIPA ASC. IEEE, pages 1-4.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Ensemble of svm trees for multimodal emotion recognition",
                "authors": [
                    {
                        "first": "Sankaranarayanan",
                        "middle": [],
                        "last": "Viktor Rozgic",
                        "suffix": ""
                    },
                    {
                        "first": "Shirin",
                        "middle": [],
                        "last": "Ananthakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Saleem",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Prasad",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Signal & Information Processing Association Annual Summit and Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Viktor Rozgic, Sankaranarayanan Ananthakrishnan, Shirin Saleem, Rohit Kumar, and Rohit Prasad. 2012b. Ensemble of svm trees for multimodal emo- tion recognition. In Signal & Information Pro- cessing Association Annual Summit and Conference (APSIPA ASC), 2012",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Recognizing affect from linguistic information in 3d continuous space",
                "authors": [
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "IEEE Transactions on Affective Computing",
                "volume": "2",
                "issue": "4",
                "pages": "192--205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bj\u00f6rn Schuller. 2011. Recognizing affect from linguis- tic information in 3d continuous space. IEEE Trans- actions on Affective Computing 2(4):192-205.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [
                            "Y"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of EMNL)",
                "volume": "1631",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of EMNL). Citeseer, volume 1631, page 1642.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "W\u00f6llmer",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [
                            "W"
                        ],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "Cate",
                        "middle": [],
                        "last": "Cox",
                        "suffix": ""
                    },
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Douglas-Cowie",
                        "suffix": ""
                    },
                    {
                        "first": "Roddy",
                        "middle": [],
                        "last": "Cowie",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "597--600",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin W\u00f6llmer, Florian Eyben, Stephan Reiter, Bj\u00f6rn W Schuller, Cate Cox, Ellen Douglas-Cowie, Roddy Cowie, et al. 2008. Abandoning emo- tion classes-towards continuous emotion recognition with modelling of long-range dependencies. In In- terspeech. volume 2008, pages 597-600.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Wollmer",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Weninger",
                        "suffix": ""
                    },
                    {
                        "first": "Timo",
                        "middle": [],
                        "last": "Knaup",
                        "suffix": ""
                    },
                    {
                        "first": "Bjorn",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "Congkai",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Sagae",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Intelligent Systems",
                "volume": "28",
                "issue": "3",
                "pages": "46--53",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Wollmer, Felix Weninger, Timo Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, and Louis- Philippe Morency. 2013. Youtube movie reviews: Sentiment analysis in an audio-visual context. IEEE Intelligent Systems 28(3):46-53.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "text": "***. Confidential Review Copy. DO NOT DISTRIBUTE.",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "FIGREF1": {
                "text": "Figure 1: Contextual LSTM network: input features are passed through an unidirectional LSTM layer, followed by a dense layer and then a softmax layer. Categorical cross entropy loss is taken for training. The dense layer activations serve as the output features.",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "FIGREF2": {
                "text": "c log 2 ( \u0177 n,c ), where N = total number of utterances in a video, y n,c = original output of class c, and \u0177 n,c = predicted output.",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "FIGREF3": {
                "text": "We consider the following variants of the contextual LSTM architecture in our experimentssc-LSTM This variant of the contextual LSTM architecture consists of unidirectional",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "FIGREF4": {
                "text": "Figure 2: Hierarchical architecture for extracting contextdependent multimodal utterance features. LSTM module has been described in Figure 1.",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "TABREF0": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td>Modality</td><td>Model</td></tr><tr><td>Text</td><td>text-CNN: Deep Convolutional Neural Network with word embeddings</td></tr><tr><td>Video</td><td>3d-CNN: 3-dimensional CNNs employed on utterances of the videos</td></tr><tr><td>Audio</td><td>openSMILE: Extracts low level audio descriptors from the audio modality</td></tr></table>",
                "text": "Methods for extracting context independent baseline features from different modalities.",
                "num": null
            },
            "TABREF1": {
                "html": null,
                "type_str": "table",
                "content": "<table/>",
                "text": "Summary of notations used in Algorithm 1. Note: d -dimension of hidden unit. k -dimension of input vectors to LSTM layer . c -number of classes.",
                "num": null
            },
            "TABREF2": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td>Dataset</td><td colspan=\"4\">Train uttrnce video uttrnce video Test</td></tr><tr><td>IEMOCAP</td><td>4290</td><td>120</td><td>1208</td><td>31</td></tr><tr><td>MOSI</td><td>1447</td><td>62</td><td>752</td><td>31</td></tr><tr><td>MOUD</td><td>322</td><td>59</td><td>115</td><td>20</td></tr><tr><td>MOSI \u2192 MOUD</td><td>2199</td><td>93</td><td>437</td><td>79</td></tr><tr><td colspan=\"5\">Table 3: uttrnce: Utterance; Person-Independent Train/Test</td></tr><tr><td colspan=\"5\">split details of each dataset (\u2248 70/30 % split). Note: X\u2192Y</td></tr><tr><td colspan=\"5\">represents train: X and test: Y; Validation sets are extracted</td></tr><tr><td colspan=\"5\">from the shuffled train sets using 80/20 % train/val ratio.</td></tr></table>",
                "text": "also provides cross dataset split details where the complete datasets of MOSI and MOUD are used for training and testing respectively. The proposed model being used on reviews from different languages allows us to analyze its robustness and generalizability.",
                "num": null
            },
            "TABREF3": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td/><td/><td/><td>MOSI</td><td/><td>MOUD</td><td/><td>IEMOCAP</td></tr><tr><td/><td>Modality T</td><td colspan=\"2\">hierarchical (%) h-LSTM sc-LSTM bc-LSTM 75.5 77.4 77.6 78.1 uni-SVM</td><td>non-hier (%)</td><td>hierarchical (%) h-LSTM sc-LSTM bc-LSTM 49.5 50.1 51.3 52.1 uni-SVM</td><td>non-hier (%)</td><td>hierarchical (%) h-LSTM sc-LSTM bc-LSTM 65.5 68.9 71.4 73.6 uni-SVM</td><td>non-hier (%)</td></tr><tr><td/><td>V</td><td colspan=\"2\">53.1 55.2 55.6 55.8</td><td/><td>46.3 48.0 48.2 48.5</td><td/><td>47.0 52.0 52.6 53.2</td></tr><tr><td/><td>A</td><td colspan=\"2\">58.5 59.6 59.9 60.3</td><td/><td>51.5 56.3 57.5 59.9</td><td/><td>52.9 54.4 55.2 57.1</td></tr><tr><td/><td>T + V</td><td colspan=\"6\">76.7 78.9 79.9 80.2 78.5 50.2 50.6 51.3 52.2 50.9 68.5 70.3 72.3 75.4 73.2</td></tr><tr><td/><td>T + A</td><td colspan=\"6\">75.8 78.3 78.8 79.3 78.2 53.1 56.9 57.4 60.4 55.5 70.1 74.1 75.2 75.6 74.5</td></tr><tr><td/><td>V + A</td><td colspan=\"6\">58.6 61.5 61.8 62.1 60.3 62.8 62.9 64.4 65.3 64.2 67.6 67.8 68.2 68.9 67.3</td></tr><tr><td/><td colspan=\"7\">T + V + A 77.9 78.1 78.6 80.3 78.1 66.1 66.4 67.3 68.1 67.0 72.5 73.3 74.2 76.1 73.5</td></tr><tr><td>Modality</td><td colspan=\"5\">Sentiment (%) MOSI MOUD angry happy sad neutral Emotion on IEMOCAP (%)</td><td/></tr><tr><td>T</td><td colspan=\"2\">78.12 52.17</td><td colspan=\"3\">76.07 78.97 76.23 67.44</td><td/></tr><tr><td>V</td><td colspan=\"2\">55.80 48.58</td><td colspan=\"3\">53.15 58.15 55.49 51.26</td><td/></tr><tr><td>A</td><td colspan=\"2\">60.31 59.99</td><td colspan=\"3\">58.37 60.45 61.35 52.31</td><td/></tr><tr><td>T + V</td><td colspan=\"2\">80.22 52.23</td><td colspan=\"3\">77.24 78.99 78.35 68.15</td><td/></tr><tr><td>T + A</td><td colspan=\"2\">79.33 60.39</td><td colspan=\"3\">77.15 79.10 78.10 69.14</td><td/></tr><tr><td>V + A</td><td colspan=\"2\">62.17 65.36</td><td colspan=\"3\">68.21 71.97 70.35 62.37</td><td/></tr><tr><td>A + V + T</td><td colspan=\"2\">80.30 68.11</td><td colspan=\"3\">77.98 79.31 78.30 69.92</td><td/></tr><tr><td>State-of</td><td>73.55 1</td><td/><td/><td/><td/><td/></tr></table>",
                "text": "Comparison of models mentioned in Section 3.2.3. The table reports the accuracy of classification. Note: non-hier \u2190 Non-hierarchical bc-lstm. For remaining fusion hierarchical fusion framework is used (Section 3.3.2) 63.25 1 73.10 2 72.40 2 61.90 2 58.10 2 -the-art 1 by(Poria et al., 2015), 2 by(Rozgic et al., 2012b)",
                "num": null
            },
            "TABREF4": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td>Modality</td><td colspan=\"4\">MOSI \u2192 MOUD uni-SVM h-LSTM sc-LSTM bc-LSTM</td></tr><tr><td>T</td><td>46.5%</td><td>46.5%</td><td>46.6%</td><td>46.9%</td></tr><tr><td>V</td><td>43.3%</td><td>45.5%</td><td>48.3%</td><td>49.6%</td></tr><tr><td>A</td><td>42.9%</td><td>46.0%</td><td>46.4%</td><td>47.2%</td></tr><tr><td>T + V</td><td>49.8%</td><td>49.8%</td><td>49.8%</td><td>49.8%</td></tr><tr><td>T + A</td><td>50.4%</td><td>50.9%</td><td>51.1%</td><td>51.3%</td></tr><tr><td>V + A</td><td>46.0%</td><td>47.1%</td><td>49.3%</td><td>49.6%</td></tr><tr><td>T + V + A</td><td>51.1%</td><td>52.2%</td><td>52.5%</td><td>52.7%</td></tr></table>",
                "text": "Accuracy % on textual (T), visual (V), audio (A) modality and comparison with the state of the art. For fusion, hierarchical fusion framework was used (Section 3.3.2)",
                "num": null
            },
            "TABREF5": {
                "html": null,
                "type_str": "table",
                "content": "<table/>",
                "text": "Cross Dataset comparison. The table reports the accuracy of classification.",
                "num": null
            },
            "TABREF6": {
                "html": null,
                "type_str": "table",
                "content": "<table/>",
                "text": "Wu and Wei-Bin Liang. 2011. Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels. IEEE Transactions on Affective Computing 2(1):10-21. Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. 2016. Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages. IEEE Intelligent Systems 31(6):82-88. Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo Xu. 2016. Attentionbased bidirectional long short-term memory networks for relation classification. In The 54th Annual Meeting of the Association for Computational Linguistics. pages 207-213.",
                "num": null
            }
        }
    }
}