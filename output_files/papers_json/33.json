{
    "paper_id": "33",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-06T18:21:06.767248Z"
    },
    "title": "Linguistically Regularized LSTM for Sentiment Classification",
    "authors": [],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.",
    "pdf_parse": {
        "paper_id": "33",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney, 2002; Taboada et al., 2011) , and early machine learning based methods (Pang et al., 2002; Pang and Lee, 2005) , and recently neural network models such as convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015) , recursive autoencoders (Socher et al., 2011 (Socher et al., , 2013)) , Long Short-Term Memory (LSTM) (Mikolov, 2012; Chung et al., 2014; Tai et al., 2015; Zhu et al., 2015) , and many more.",
                "cite_spans": [
                    {
                        "start": 267,
                        "end": 281,
                        "text": "(Turney, 2002;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 282,
                        "end": 303,
                        "text": "Taboada et al., 2011)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 347,
                        "end": 366,
                        "text": "(Pang et al., 2002;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 367,
                        "end": 386,
                        "text": "Pang and Lee, 2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 467,
                        "end": 478,
                        "text": "(Kim, 2014;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 479,
                        "end": 505,
                        "text": "Kalchbrenner et al., 2014;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 506,
                        "end": 523,
                        "text": "Lei et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 549,
                        "end": 569,
                        "text": "(Socher et al., 2011",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 570,
                        "end": 594,
                        "text": "(Socher et al., , 2013))",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 627,
                        "end": 642,
                        "text": "(Mikolov, 2012;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 643,
                        "end": 662,
                        "text": "Chung et al., 2014;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 663,
                        "end": 680,
                        "text": "Tai et al., 2015;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 681,
                        "end": 698,
                        "text": "Zhu et al., 2015)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In spite of the great success of these neural models, there are some defects in previous studies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai et al., 2015; Zhu et al., 2015) , depend on parsing tree structures and expensive phrase-level annotation, whose performance drops substantially when only trained with sentence-level annotation. Second, linguistic knowledge such as sentiment lexicon, negation words or negators (e.g., not, never), and intensity words or intensifiers (e.g., very, absolutely), has not been fully employed in neural models.",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 93,
                        "text": "(Tai et al., 2015;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 94,
                        "end": 111,
                        "text": "Zhu et al., 2015)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The goal of this research is to developing simple sequence models but also attempts to fully employing linguistic resources to benefit sentiment classification. Firstly, we attempts to develop simple models that do not depend on parsing trees and do not require phrase-level annotation which is too expensive in real-world applications. Secondly, in order to obtain competitive performance, simple models can benefit from linguistic resources. Three types of resources will be addressed in this paper: sentiment lexicon, negation words, and intensity words. Sentiment lexicon offers the prior polarity of a word which can be useful in determining the sentiment polarity of longer texts such as phrases and sentences. Negators are typical sentiment shifters (Zhu et al., 2014) , which constantly change the polarity of sentiment expression. Intensifiers change the valence degree of the modified text, which is important for fine-grained sentiment classification.",
                "cite_spans": [
                    {
                        "start": 757,
                        "end": 775,
                        "text": "(Zhu et al., 2014)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In order to model the linguistic role of sentiment, negation, and intensity words, our central idea is to regularize the difference between the predicted sentiment distribution of the current position 1 , and that of the previous or next positions, in a sequence model. For instance, if the current position is a negator not, the negator should change the sentiment distribution of the next posi-tion accordingly. To summarize, our contributions lie in two folds:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We discover that modeling the linguistic role of sentiment, negation, and intensity words can enhance sentence-level sentiment classification. We address the issue by imposing linguistic-inspired regularizers on sequence LSTM models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Unlike previous models that depend on parsing structures and expensive phrase-level annotation, our models are simple and efficient, but the performance is on a par with the stateof-the-art.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of the paper is organized as follows: In the following section, we survey related work. In Section 3, we briefly introduce the background of LSTM and bidirectional LSTM, and then describe in detail the lingistic regularizers for sentiment/negation/intensity words in Section 4. Experiments are presented in Section 5, and Conclusion follows in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "There are many neural networks proposed for sentiment classification. The most noticeable models may be the recursive autoencoder neural network which builds the representation of a sentence from subphrases recursively (Socher et al., 2011 (Socher et al., , 2013;; Dong et al., 2014; Qian et al., 2015) . Such recursive models usually depend on a tree structure of input text, and in order to obtain competitive results, usually require annotation of all subphrases. Sequence models, for instance, convolutional neural network (CNN), do not require tree-structured data, which are widely adopted for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015) . Long short-term memory models are also common for learning sentence-level representation due to its capability of modeling the prefix or suffix context (Hochreiter and Schmidhuber, 1997) . LSTM can be commonly applied to sequential data but also tree-structured data (Zhu et al., 2015; Tai et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 219,
                        "end": 239,
                        "text": "(Socher et al., 2011",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 240,
                        "end": 264,
                        "text": "(Socher et al., , 2013;;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 265,
                        "end": 283,
                        "text": "Dong et al., 2014;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 284,
                        "end": 302,
                        "text": "Qian et al., 2015)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 625,
                        "end": 636,
                        "text": "(Kim, 2014;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 637,
                        "end": 663,
                        "text": "Kalchbrenner et al., 2014;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 664,
                        "end": 681,
                        "text": "Lei et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 836,
                        "end": 870,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 951,
                        "end": 969,
                        "text": "(Zhu et al., 2015;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 970,
                        "end": 987,
                        "text": "Tai et al., 2015)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Networks for Sentiment Classification",
                "sec_num": "2.1"
            },
            {
                "text": "Linguistic knowledge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 116,
                        "text": "(not, never, neither, etc.)",
                        "ref_id": null
                    },
                    {
                        "start": 150,
                        "end": 173,
                        "text": "(very, extremely, etc.)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "Sentiment lexicon (Hu and Liu, 2004; Wilson et al., 2005) usually defines prior polarity of a lexical entry, and is valuable for lexicon-based models (Turney, 2002; Taboada et al., 2011) , and machine learning approaches (Pang and Lee, 2008) . There are recent works for automatic construction of sentiment lexicons from social data (Vo and Zhang, 2016) and for multiple languages (Chen and Skiena, 2014) . A noticeable work that ultilizes sentiment lexicons can be seen in (Teng et al., 2016) which treats the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a neural network.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 36,
                        "text": "(Hu and Liu, 2004;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 37,
                        "end": 57,
                        "text": "Wilson et al., 2005)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 150,
                        "end": 164,
                        "text": "(Turney, 2002;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 165,
                        "end": 186,
                        "text": "Taboada et al., 2011)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 221,
                        "end": 241,
                        "text": "(Pang and Lee, 2008)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 381,
                        "end": 404,
                        "text": "(Chen and Skiena, 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 474,
                        "end": 493,
                        "text": "(Teng et al., 2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "Negation words play a critical role in modifying sentiment of textual expressions. Some early negation models adopt the reversing assumption that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006) . The shifting hyothesis assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009) . Since each negator can affect the modified text in different ways, the constant amount can be extended to be negatorspecific (Zhu et al., 2014) , and further, the effect of negators could also depend on the syntax and semantics of the modified text (Zhu et al., 2014) . Other approaches to negation modeling can be seen in (Jia et al., 2009; Wiegand et al., 2010; Benamara et al., 2012; Lapponi et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 244,
                        "text": "(Polanyi and Zaenen, 2006;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 245,
                        "end": 270,
                        "text": "Kennedy and Inkpen, 2006)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 367,
                        "end": 389,
                        "text": "(Taboada et al., 2011;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 390,
                        "end": 411,
                        "text": "Liu and Seneff, 2009)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 539,
                        "end": 557,
                        "text": "(Zhu et al., 2014)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 663,
                        "end": 681,
                        "text": "(Zhu et al., 2014)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 737,
                        "end": 755,
                        "text": "(Jia et al., 2009;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 756,
                        "end": 777,
                        "text": "Wiegand et al., 2010;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 778,
                        "end": 800,
                        "text": "Benamara et al., 2012;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 801,
                        "end": 822,
                        "text": "Lapponi et al., 2012)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the valence degree (i.e., sentiment intensity) of the modified text. In (Wei et al., 2011) the authors propose a linear regression model to predict the valence value for content words. In (Malandrakis et al., 2013) , a kernel-based model is proposed to combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learningto-rank model with a pair-wise strategy is proposed to predict sentiment intensity scores (Wang et al., 2016) . Linguistic intensity is not limited to sentiment or intensity words, and there are works that assign low/medium/high intensity scales to adjectives such as okay, good, great (Sharma et al., 2015) or to gradable terms (e.g. large, huge, gigantic) (Shivade et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 257,
                        "end": 275,
                        "text": "(Wei et al., 2011)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 373,
                        "end": 399,
                        "text": "(Malandrakis et al., 2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 637,
                        "end": 656,
                        "text": "(Wang et al., 2016)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 833,
                        "end": 854,
                        "text": "(Sharma et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 905,
                        "end": 927,
                        "text": "(Shivade et al., 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "In (Dong et al., 2015) , a sentiment parser is proposed, and the authors studied how sentiment changes when a phrase is modified by negators or intensifiers.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 22,
                        "text": "(Dong et al., 2015)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "3 Long Short-term Memory Network 3.1 Long Short-Term Memory (LSTM)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "Long Short-Term Memory has been widely adopted for text processing. Briefly speaking, in LSTM, the hidden states h t and memory cell c t is a function of their previous c t-1 and h t-1 and input vector x t , or formally as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "ct, ht = g (LST M ) (ct-1, ht-1, xt) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "The hidden state h t \u2208 R d denotes the representation of position t while also encoding the preceding contexts of the position. For more details about LSTM, we refer readers to (Hochreiter and Schmidhuber, 1997) .",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 211,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Knowledge for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "In LSTM, the hidden state of each position (h t ) only encodes the prefix context in a forward direction while the backward context is not considered. Bidirectional LSTM (Graves et al., 2013) exploited two parallel passes (forward and backward) and concatenated hidden states of the two LSTMs as the representation of each position. The forward and backward LSTMs are respectively formulated as follows:",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Graves et al., 2013)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "-\u2192 c t, -\u2192 h t = g (LST M ) ( -\u2192 c t-1, -\u2192 h t-1, xt) (2) \u2190 -c t, \u2190 - h t = g (LST M ) ( \u2190 -c t+1, \u2190 - h t+1, xt) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "where g (LST M ) is the same as that in Eq (1). Particularly, parameters in the two LSTMs are shared. The representation of the entire sentence is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "[ -\u2192 h n , \u2190 - h 1 ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "where n is the length of the sentence. At each position t, the new representation is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "h t = [ -\u2192 h t , \u2190 - h t ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "which is the concatenation of hidden states of the forward LSTM and backward LSTM. In this way, the forward and backward contexts can be considered simultaneously.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "The central idea of the paper is to model the linguistic role of sentiment, negation, and intensity words in sentence-level sentiment classification by regularizing the outputs at adjacent positions of a sentence. For example, in sentence \"this movie is interesting\", the predicted sentiment distributions at \"this*2 \", \"this movie*\", and \"this movie is*\" should be close to each other, while the predicted sentiment distribution at \"this movie is very interesting*\" should be quite different from the preceeding positions (\"this movie is very*\") since a sentiment word (\"interesting\") is seen.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "We propose a generic regularizer and three special regularizers based on the following linguistic observations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Non-Sentiment Regularizer: if the two adjacent positions are all non-opinion words, the sentiment distributions of the two positions should be close to each other. Though this is not always true (e.g., soap movie), this assumption holds at most cases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Sentiment Regularizer: if the word is a sentiment word found in a lexicon, the sentiment distribution of the current position should be significantly different from that of the next or previous positions. We approach this phenomenon with a sentiment class specific shifting distribution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Negation Regularizer: Negation words such as \"not\" and \"never\" are critical sentiment shifter or converter: in general they shift sentiment polarity from the positive end to the negative end, but sometimes depend on the negation word and the words they modify.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "The negation regularizer models this linguistic phenomena with a negator-specific transformation matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Intensity Regularizer: Intensity words such as \"very\" and \"extremely\" change the valence degree of a sentiment expression: for instance, from positive to very positive. Modeling this effect is quite important for finegrained sentiment classification, and the intensity regularizer is designed to formulate this effect by a word-specific transformation matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "More formally, the predicted sentiment distribution (p t , based on h t , see Eq. 5) at position t should be linguistically regularized with respect to that of the preceding (t -1) or following (t + 1) positions. In order to enforce the model to produce coherent predictions, we plug a new loss term into the original cross entropy loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "L(\u03b8) = - \u2211 i y i log p i + \u03b1 \u2211 i \u2211 t Lt,i + \u03b2||\u03b8|| 2 (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "where y i is the gold distribution for sentence i, p i is the predicted distribution, L t,i is one of the above regularizers or combination of these regularizers on sentence i, \u03b1 is the weight for the regularization term, and t is the word position in a sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Negation scope resolution is another complex problem which has been extensively studied (Zou et al., 2013; Packard et al., 2014; Fancellu et al., 2016) , which is beyond the scope of this work. Instead, we resort to sequence LSTMs for encoding surrounding contexts at a given position.",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 240,
                        "text": "(Zou et al., 2013;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 241,
                        "end": 262,
                        "text": "Packard et al., 2014;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 263,
                        "end": 285,
                        "text": "Fancellu et al., 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistically Regularized LSTM",
                "sec_num": "4"
            },
            {
                "text": "This regularizer constrains that the sentiment distributions of adjacent positions should not vary much if the additional input word x t is not a sentiment word, formally as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-Sentiment Regularizer (NSR)",
                "sec_num": "4.1"
            },
            {
                "text": "L (N SR) t = max(0, DKL(pt||pt-1) -M ) (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-Sentiment Regularizer (NSR)",
                "sec_num": "4.1"
            },
            {
                "text": "where M is a hyperparameter for margin, p t is the predicted distribution at state of position t, (i.e., h t ), and D KL (p||q) is a symmetric KL divergence defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-Sentiment Regularizer (NSR)",
                "sec_num": "4.1"
            },
            {
                "text": "DKL(p||q) = 1 2 C \u2211 l=1 p(l) log q(l) + q(l) log p(l) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-Sentiment Regularizer (NSR)",
                "sec_num": "4.1"
            },
            {
                "text": "where p, q are distributions over sentiment labels l and C is the number of labels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-Sentiment Regularizer (NSR)",
                "sec_num": "4.1"
            },
            {
                "text": "The sentiment regularizer constrains that the sentiment distributions of adjacent positions should drift accordingly if the input word is a sentiment word. Let's revisit the example \"this movie is interesting\" again. At position t = 4 we see a positive word \"interesting\" so the predicted distribution would be more positive than that at position t = 3. This is the issue of sentiment drift.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "In order to address the sentiment drift issue, we propose a polarity shifting distribution s c \u2208 R C for each sentiment class defined in a lexicon. For instance, a sentiment lexicon may have class labels like strong positive, weakly positive, weakly negative, and strong negative, and for each class, there is a shifting distribution which will be learned by the model. The sentiment regularizer states that if the current word is a sentiment word, the sentiment distribution drift should be observed in comparison to the previous position, in more details:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "p (SR) t-1 = pt-1 + s c(xt) (7) L (SR) t",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "= max(0, DKL(pt||p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(SR) t-1 ) -M )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "where p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "(SR)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "t-1 is the drifted sentiment distribution after considering the shifting sentiment distribution corresponding to the state at position t, c(x t ) is the prior sentiment class of word x t , and s c \u2208 \u03b8 is a parameter to be optimized but could also be set fixed with prior knowledge. Note that in this way all words of the same sentiment class share the same drifting distribution, but in a refined setting, we can learn a shifting distribution for each sentiment word if large-scale datasets are available.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Regularizer (SR)",
                "sec_num": "4.2"
            },
            {
                "text": "The negation regularizer approaches how negation words shift the sentiment distribution of the modified text. When the input x t is a negation word, the sentiment distribution should be shifted/reversed accordingly. However, the negation role is more complex than that by sentiment words, for example, the word \"not\" in \"not good\" and \"not bad\" have different roles in polarity change. The former changes the polarity to negative, while the latter changes to neutral instead of positive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Negation Regularizer (NR)",
                "sec_num": "4.3"
            },
            {
                "text": "To respect such complex negation effects, we propose a transformation matrix T m \u2208 R C\u00d7C for each negation word m, and the matrix will be learned by the model. The regularizer assumes that if the current position is a negation word, the sentiment distribution of the current position should be close to that of the next or previous position with the transformation. where p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Negation Regularizer (NR)",
                "sec_num": "4.3"
            },
            {
                "text": "p (N R) t-1 = sof tmax(Tx j \u00d7 pt-1) (9) p (N R) t+1 = sof tmax(Tx j \u00d7 pt+1) (10) L (N R) t = min { max(0, DKL(pt||p (N R) t-1 ) -M ) max(0, DKL(pt||p (N R) t+1 ) -M ) (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Negation Regularizer (NR)",
                "sec_num": "4.3"
            },
            {
                "text": "(N R) t-1 and p (N R)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Negation Regularizer (NR)",
                "sec_num": "4.3"
            },
            {
                "text": "t+1 is the sentiment distuibution after transformation, T x j \u2208 \u03b8 is the transformation matrix for a negation word x j , a parameter to be learned during training. In total, we train m transformation matrixs for m negation words. Such negator-specific transformation is in accordance with the finding that each negator has its individual negation effect (Zhu et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 354,
                        "end": 372,
                        "text": "(Zhu et al., 2014)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Negation Regularizer (NR)",
                "sec_num": "4.3"
            },
            {
                "text": "Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensifier can change the valence degree of the content word. The intensity regularizer models how intensity words influence the sentiment valence of a phrase or a sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intensity Regularizer (IR)",
                "sec_num": "4.4"
            },
            {
                "text": "The formulation of the intensity effect is quite the same as that in the negation regularizer, but with different parameters of course. For each intensity word, there is a transform matrix to favor the different roles of various intensifiers on sentiment drift. For brevity, we will not repeat the formulas here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intensity Regularizer (IR)",
                "sec_num": "4.4"
            },
            {
                "text": "To preserve the simplicity of our proposals, we do not consider the modification span of negation and intensity words, which is a quite challenging problem in the NLP community (Zou et al., 2013; Packard et al., 2014; Fancellu et al., 2016) . However, we can alleviate the problem by leveraging bidirectional LSTM. For a single LSTM, we employ a backward LSTM from the end to the beginning of a sentence. This is because, at most times, the modified words of negation and intensity words are usually at the right side of the modified text. But sometimes, the modified words are at the left side of negation and intensity words. To better address this issue, we employ bidirectional LSTM and let the model determine which side should be chosen.",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 195,
                        "text": "(Zou et al., 2013;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 196,
                        "end": 217,
                        "text": "Packard et al., 2014;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 218,
                        "end": 240,
                        "text": "Fancellu et al., 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "More formally, in Bi-LSTM, we compute a transformed sentiment distribution on -\u2192 p t-1 of the forward LSTM and also that on \u2190p t+1 of the backward LSTM, and compute the minimum distance of the distribution of the current position to the two distributions. This could be formulated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "-\u2192 p (R) t-1 = sof tmax(Tx j \u00d7 -\u2192 p t-1) (12) \u2190 -p (R) t+1 = sof tmax(Tx j \u00d7 \u2190 -p t+1) (13) L (R) t = min { max(0, DKL( -\u2192 p t|| -\u2192 p (R) t-1 ) -M ) max(0, DKL( \u2190 -p t|| \u2190 -p (R) t+1 ) -M )",
                        "eq_num": "(14)"
                    }
                ],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "- \u2192 p (R) t-1 and \u2190 -p (R)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "t+1 are the sentiment distributions transformed from the previous distribution -\u2192 p t-1 and next distribution \u2190p t+1 respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "Note that R \u2208 {N R, IR} indicating the formulation works for both negation and intensity regularizers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "Due to the same consideration, we redefine L (N SR) t and L (SR) t with bidirectional LSTM similarly. The formulation is the same and omitted for brevity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Linguistic Regularizers to Bidirectional LSTM",
                "sec_num": "4.5"
            },
            {
                "text": "Our models address these linguistic factors with mathematical operations, parameterized with shifting distribution vectors or transformation matrices. In the sentiment regularizer, the sentiment shifting effect is parameterized with a classspecific distribution (but could also be wordspecific if with more data). In the negation and intensity regularizers, the effect is parameterized with word-specific transformation matrices. This is to respect the fact that the mechanism of how negation and intensity words shift sentiment expression is quite complex and highly dependent on individual words. Negation/Intensity effect also depends on the syntax and semantics of the modified text, however, for simplicity we resort to sequence LSTM for encoding surrounding contexts in this paper. We partially address the modification scope issue by applying the minimization operator in Eq. 11 and Eq. 14, and the bidirectional LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "4.6"
            },
            {
                "text": "",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment",
                "sec_num": "5"
            },
            {
                "text": "Two datasets are used for evaluating the proposed models: Movie Review (MR) (Pang and Lee, 2005) where each sentence is annotated with two classes as negative, positive and Stanford Sentiment Treebank (SST) (Socher et al., 2013) with five classes { very negative, negative, neutral, positive, very positive}. Note that SST has provided phrase-level annotation on all inner nodes, but we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 96,
                        "text": "(Pang and Lee, 2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 207,
                        "end": 228,
                        "text": "(Socher et al., 2013)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset and Sentiment Lexicon",
                "sec_num": "5.1"
            },
            {
                "text": "The sentiment lexicon contains two parts. The first part comes from MPQA (Wilson et al., 2005) , which contains 5, 153 sentiment words, each with polarity rating. The second part consists of the leaf nodes of the SST dataset (i.e., all sentiment words) and there are 6, 886 polar words except neural ones. We combine the two parts and ignore those words that have conflicting sentiment labels, and produce a lexicon of 9, 750 words with 4 sentiment labels. For negation and intensity words, we collect them manually since the number is small, some of which can be seen in Table 2 .",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 94,
                        "text": "(Wilson et al., 2005)",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 578,
                        "end": 579,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Dataset and Sentiment Lexicon",
                "sec_num": "5.1"
            },
            {
                "text": "Due to the length limit, we present the implementation details and a full list of resources in the supplementary file. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset and Sentiment Lexicon",
                "sec_num": "5.1"
            },
            {
                "text": "",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": null
            },
            {
                "text": "We include several baselines, as listed below: RNN/RNTN: Recursive Neural Network over parsing trees, proposed by (Socher et al., 2011) and Recursive Tensor Neural Network (Socher et al., 2013) employs tensors to model correlations between different dimensions of child nodes' vectors.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 135,
                        "text": "(Socher et al., 2011)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 172,
                        "end": 193,
                        "text": "(Socher et al., 2013)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "LSTM/Bi-LSTM: Long Short-Term Memory (Cho et al., 2014) and the bidirectional variant as introduced previously.",
                "cite_spans": [
                    {
                        "start": 37,
                        "end": 55,
                        "text": "(Cho et al., 2014)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "Tree-LSTM: Tree-Structured Long Short-Term Memory (Tai et al., 2015) introduces memory cells and gates into tree-structured neural network.",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 68,
                        "text": "(Tai et al., 2015)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "CNN: Convolutional Neural Network (Kalchbrenner et al., 2014) generates sentence representation by convolution and pooling operations.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 61,
                        "text": "(Kalchbrenner et al., 2014)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "CNN-Tensor: In (Lei et al., 2015) , the convolution operation is replaced by tensor product and a dynamic programming is applied to enumerate all skippable trigrams in a sentence. Very strong results are reported.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 33,
                        "text": "(Lei et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "DAN: Deep Average Network (DAN) (Iyyer et al., 2015) averages all word vectors in a sentence and connects an MLP layer to the output layer.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 52,
                        "text": "(Iyyer et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "Neural Context-Sensitive Lexicon: NCSL (Teng et al., 2016) treats the sentiment score of a sentence as a weighted sum of prior scores of words in the sentence where the weights are learned by a neural network.",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 58,
                        "text": "(Teng et al., 2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "Firstly, we evaluate our model on the MR dataset and the results are shown in Table 3 . We have the following observations:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 84,
                        "end": 85,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "First, both LR-LSTM and LR-Bi-LSTM outperforms their counterparts (81.5% vs. 77.4% and 82.1% vs. 79.3%, resp.), demonstrating the effectiveness of the linguistic regularizers. Second, LR-LSTM and LR-Bi-LSTM perform slightly better than Tree-LSTM but Tree-LSTM leverages a constituency tree structure while our model is a simple sequence model. As future work, we will apply such regularizers to tree-structured models. Last, on the MR dataset, our model is comparable to or slightly better than CNN.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "For fine-grained sentiment classification, we evaluate our model on the SST dataset which has five sentiment classes { very negative, negative, neutral, positive, very positive} so that we can evaluate the sentiment shifting effect of intensity words. The results are shown in Table 3 . We have the following observations:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 283,
                        "end": 284,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "First, linguistically regularized LSTM and Bi-LSTM are better than their counterparts. It's worth noting that LR-Bi-LSTM (trained with just sentence-level annotation) is even comparable to Bi-LSTM trained with phrase-level annotation. That means, LR-Bi-LSTM can avoid the heavy phrase-level annotation but still obtain comparable results. Second, our models are comparable to Tree-LSTM but our models are not dependent on a parsing tree and more simple, and hence more efficient. Further, for Tree-LSTM, the model is heavily dependent on phrase-level annotation, otherwise the performance drops substantially (from 51% to 48.1%). Last, on the SST dataset, our model is better than CNN, DAN, and NCSL. We conjecture that the strong performance of CNN-Tensor may be due to the tensor product operation, the enumeration of all skippable trigrams, and the concatenated representations of all pooling layers for final classification. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Comparison",
                "sec_num": "5.2"
            },
            {
                "text": "",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": null
            },
            {
                "text": "In order to reveal the effect of each individual regularizer, we conduct ablation experiments. Each time, we remove a regularizer and observe how the performance varies. First of all, we conduct this experiment on the entire datasets, and then we experiment on sub-datasets that only contain negation words or intensity words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of Different Regularizers",
                "sec_num": "5.3"
            },
            {
                "text": "The experiment results are shown in Table 4 where we can see that the non-sentiment regularizer (NSR) and sentiment regularizer (SR) play a key role 3 , and the negation regularizer and intensity regularizer are effective but less important than NSR and SR. This may be due to the fact that only 14% of sentences contains negation words in the test datasets, and 23% contains intensity words, and thus we further evaluate the models on two subsets, as shown in Table 5 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 42,
                        "end": 43,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 467,
                        "end": 468,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The Effect of Different Regularizers",
                "sec_num": "5.3"
            },
            {
                "text": "The experiments on the subsets show that: 1) With linguistic regularizers, LR-Bi-LSTM outperforms Bi-LSTM remarkably on these subsets; 2) When the negation regularizer is removed from the model, the performance drops significantly on ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of Different Regularizers",
                "sec_num": "5.3"
            },
            {
                "text": "To further reveal the linguistic role of negation words, we compare the predicted sentiment distributions of a phrase pair with and without a negation word. The experimental results performed on MR are shown in Fig. 1 . Each dot denotes a phrase pair (for example, <interesting, not interesting>), where the x-axis denotes the positive score4 of a phrase without negators (e.g., interesting), and the y-axis indicates the positive score for the phrase with negators (e.g., not interesting). The curves in the figures show this function:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 216,
                        "end": 217,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "[1 -y, y] = sof tmax(T nw * [1 -x, x]) where [1 -x, x] is a sentiment distribution on [negative, positive], x",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "is the positive score of the phrase without negators (x-axis) and y that of the phrase with negators (y-axis), and T nw is the transformation matrix for the negation word nw (see Eq. 9). By looking into the Figure 1 : The sentiment shifts with negators. Each dot < x, y > indicates that x is the sentiment score of a phrase without negator and y is that of the phrase with a negator.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 214,
                        "end": 215,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "detailed results of our model, we have the following statements: First, there is no dot at the up-right and bottomleft blocks, indicating that negators generally shift/convert very positive or very negative phrases to other polarities. Typical phrases include not very good, not too bad.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "Second, the dots at the up-left and bottom-right respectively indicates the negation effects: changing negative to positive and positive to negative. Typical phrases include never seems hopelessly (up-left), no good scenes (bottom-right), not interesting (bottom-right), etc. There are also some positive/negative phrases shifting to neutral sentiment such as not so good, and not too bad.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "Last, the dots located at the center indicate that neutral phrases maintain neutral sentiment with negators. Typical phrases include not at home, not here, where negators typically modify nonsentiment words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Effect of the Negation Regularizer",
                "sec_num": "5.4"
            },
            {
                "text": "To further reveal the linguistic role of intensity words, we perform experiments on the SST dataset, as illustrated in Figure 2 . We show the matrix that indicates how the sentiment shifts after being modified by intensifiers. Each number in a cell (m ij ) indicates how many phrases are predicted with a sentiment label i but the prediction of the phrases with intensifiers changes to label j. For instance, the number 20 (m 21 ) in the second matrix , means that there are 20 phrases predicted with a class of negative (-) but the prediction changes to very negative (--) after being modified by intensifier \"very\". Results in the first matrix show that, for intensifier \"most\", there are 21/21/13/12 phrases whose sentiment is shifted after being modified by intensifiers, from negative There are also many phrases retaining the sentiment after being modified with intensifiers. Not surprisingly, for very positive/negative phrases, phrases modified by intensifiers still maintain the strong sentiment. For the left phrases, they fall into three categories: first, words modified by intensifiers are non-sentiment words, such as most of us, most part; second, intensifiers are not strong enough to shift sentiment, such as most complex (from neg. to neg.), most traditional (from pos. to pos.); third, our models fail to shift sentiment with intensifiers such as most vital, most resonant film.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 126,
                        "end": 127,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "The Effect of the Intensity Regularizer",
                "sec_num": "5.5"
            },
            {
                "text": "We present linguistically regularized LSTMs for sentence-level sentiment classification. The proposed models address the sentient shifting effect of sentiment, negation, and intensity words. Furthermore, our models are sequence LSTMs which do not depend on a parsing tree-structure and do not require expensive phrase-level annotation. Results show that our models are able to address the linguistic role of sentiment, negation, and intensity words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "To preserve the simplicity of the proposed models, we do not consider the modification scope of negation and intensity words, though we partially address this issue by applying a minimization operartor (see Eq. 11, Eq. 14) and bi-directional LSTM. As future work, we plan to apply the linguistic regularizers to tree-LSTM to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Note that in sequence models, the hidden state of the current position also encodes forward or backward contexts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The asterisk denotes the current position.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The score is obtained from the predicted distribution, where 1 means positive and 0 means negative.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "How do negation and modality impact on opinions?",
                "authors": [
                    {
                        "first": "Farah",
                        "middle": [],
                        "last": "Benamara",
                        "suffix": ""
                    },
                    {
                        "first": "Baptiste",
                        "middle": [],
                        "last": "Chardon",
                        "suffix": ""
                    },
                    {
                        "first": "Yannick",
                        "middle": [],
                        "last": "Mathieu",
                        "suffix": ""
                    },
                    {
                        "first": "Vladimir",
                        "middle": [],
                        "last": "Popescu",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Asher",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics. Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "10--18",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Farah Benamara, Baptiste Chardon, Yannick Math- ieu, Vladimir Popescu, and Nicholas Asher. 2012. How do negation and modality impact on opin- ions? In Proceedings of the Workshop on Extra- Propositional Aspects of Meaning in Computational Linguistics. Association for Computational Linguis- tics, pages 10-18.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Building sentiment lexicons for all major languages",
                "authors": [
                    {
                        "first": "Yanqing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Skiena",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "383--389",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yanqing Chen and Steven Skiena. 2014. Building sen- timent lexicons for all major languages. In ACL (2). pages 383-389.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
                "authors": [
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Van Merri\u00ebnboer",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1406.1078"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 .",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                "authors": [
                    {
                        "first": "Junyoung",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.3555"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing. arXiv preprint arXiv:1412.3555 .",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A statistical parsing framework for sentiment classification",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and Ke Xu. 2015. A statistical parsing framework for sentiment classification. Computational Linguistics .",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "AAAI. AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014. Adaptive multi-compositionality for recursive neu- ral models with applications to sentiment analysis. In AAAI. AAAI.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Neural networks for negation scope detection",
                "authors": [
                    {
                        "first": "Federico",
                        "middle": [],
                        "last": "Fancellu",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Lopez",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Webber",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "495--504",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1047"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Federico Fancellu, Adam Lopez, and Bonnie Web- ber. 2016. Neural networks for negation scope detection. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers). Association for Computational Linguistics, pages 495-504. https://doi.org/10.18653/v1/P16-1047.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Hybrid speech recognition with deep bidirectional lstm",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    },
                    {
                        "first": "Abdel-Rahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Automatic Speech Recognition and Understanding (ASRU)",
                "volume": "",
                "issue": "",
                "pages": "273--278",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo- hamed. 2013. Hybrid speech recognition with deep bidirectional lstm. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, pages 273-278.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural Computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9(8):1735-1780.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Mining and summarizing customer reviews",
                "authors": [
                    {
                        "first": "Minqing",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": "",
                "issue": "",
                "pages": "168--177",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl- edge discovery and data mining. ACM, pages 168- 177.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Deep unordered composition rivals syntactic methods for text classification",
                "authors": [
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Varun",
                        "middle": [],
                        "last": "Manjunatha",
                        "suffix": ""
                    },
                    {
                        "first": "Jordan",
                        "middle": [],
                        "last": "Boyd-Graber",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2015. Deep unordered compo- sition rivals syntactic methods for text classification. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "The effect of negation on sentiment analysis and retrieval effectiveness",
                "authors": [
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Weiyi",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 18th ACM conference on Information and knowledge management",
                "volume": "",
                "issue": "",
                "pages": "1827--1830",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The effect of negation on sentiment analysis and retrieval effectiveness. In Proceedings of the 18th ACM con- ference on Information and knowledge management. ACM, pages 1827-1830.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A convolutional neural network for modelling sentences",
                "authors": [
                    {
                        "first": "Nal",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "655--665",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. In ACL. pages 655-665.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Sentiment classification of movie reviews using contextual valence shifters",
                "authors": [
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Kennedy",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Inkpen",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Computational intelligence",
                "volume": "22",
                "issue": "2",
                "pages": "110--125",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair Kennedy and Diana Inkpen. 2006. Senti- ment classification of movie reviews using contex- tual valence shifters. Computational intelligence 22(2):110-125.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1746--1751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP. pages 1746- 1751.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Representing and resolving negation for sentiment analysis",
                "authors": [
                    {
                        "first": "Emanuele",
                        "middle": [],
                        "last": "Lapponi",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Read",
                        "suffix": ""
                    },
                    {
                        "first": "Lilja",
                        "middle": [],
                        "last": "\u00d8vrelid",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "2012 IEEE 12th International Conference on Data Mining Workshops",
                "volume": "",
                "issue": "",
                "pages": "687--692",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emanuele Lapponi, Jonathon Read, and Lilja \u00d8vre- lid. 2012. Representing and resolving negation for sentiment analysis. In 2012 IEEE 12th Inter- national Conference on Data Mining Workshops. IEEE, pages 687-692.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Molding cnns for text: non-linear, non-consecutive convolutions. ACL",
                "authors": [
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Tommi",
                        "middle": [],
                        "last": "Jaakkola",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2015. Molding cnns for text: non-linear, non-consecutive convolutions. ACL .",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Review sentiment scoring via a parse-and-paraphrase paradigm",
                "authors": [
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Seneff",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "161--169",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingjing Liu and Stephanie Seneff. 2009. Review sen- timent scoring via a parse-and-paraphrase paradigm. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association for Computational Linguis- tics, pages 161-169.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Distributional semantic models for affective text analysis",
                "authors": [
                    {
                        "first": "Nikolaos",
                        "middle": [],
                        "last": "Malandrakis",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandros",
                        "middle": [],
                        "last": "Potamianos",
                        "suffix": ""
                    },
                    {
                        "first": "Elias",
                        "middle": [],
                        "last": "Iosif",
                        "suffix": ""
                    },
                    {
                        "first": "Shrikanth",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
                "volume": "21",
                "issue": "11",
                "pages": "2379--2392",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nikolaos Malandrakis, Alexandros Potamianos, Elias Iosif, and Shrikanth Narayanan. 2013. Distribu- tional semantic models for affective text analysis. IEEE Transactions on Audio, Speech, and Language Processing 21(11):2379-2392.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Statistical language models based on neural networks. Presentation at Google, Mountain View",
                "authors": [
                    {
                        "first": "Tom\u00e1\u0161",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom\u00e1\u0161 Mikolov. 2012. Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April .",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Simple negation scope resolution through deep parsing: A semantic solution to a semantic problem",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "Emily"
                        ],
                        "last": "Woodley Packard",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Bender",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Read",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Oepen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dridan",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "69--78",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/P14-1007"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Woodley Packard, M. Emily Bender, Jonathon Read, Stephan Oepen, and Rebecca Dridan. 2014. Sim- ple negation scope resolution through deep parsing: A semantic solution to a semantic problem. In Pro- ceedings of the 52nd Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, pages 69-78. https://doi.org/10.3115/v1/P14-1007.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "115--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit- ing class relationships for sentiment categorization with respect to rating scales. In ACL. pages 115- 124.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Opinion mining and sentiment analysis",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Foundations and trends in information retrieval",
                "volume": "2",
                "issue": "1-2",
                "pages": "1--135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in infor- mation retrieval 2(1-2):1-135.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Thumbs up?: sentiment classification using machine learning techniques",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Shivakumar",
                        "middle": [],
                        "last": "Vaithyanathan",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In ACL. pages 79-86.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Contextual valence shifters",
                "authors": [
                    {
                        "first": "Livia",
                        "middle": [],
                        "last": "Polanyi",
                        "suffix": ""
                    },
                    {
                        "first": "Annie",
                        "middle": [],
                        "last": "Zaenen",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Computing attitude and affect in text: Theory and applications",
                "volume": "",
                "issue": "",
                "pages": "1--10",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Livia Polanyi and Annie Zaenen. 2006. Contextual va- lence shifters. In Computing attitude and affect in text: Theory and applications, Springer, pages 1-10.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Learning tag embeddings and tag-specific composition functions in recursive neural network",
                "authors": [
                    {
                        "first": "Qiao",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "ACL",
                "volume": "1",
                "issue": "",
                "pages": "1365--1374",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan Zhu, and Xiaoyan Zhu. 2015. Learning tag embed- dings and tag-specific composition functions in re- cursive neural network. In ACL. volume 1, pages 1365-1374.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Adjective intensity and sentiment analysis",
                "authors": [],
                "year": 2015,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Raksha Sharma, Mohit Gupta, Astha Agarwal, and Pushpak Bhattacharyya. 2015. Adjective intensity and sentiment analysis. EMNLP2015, Lisbon, Por- tugal .",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Corpus-based discovery of semantic intensity scales",
                "authors": [
                    {
                        "first": "Chaitanya",
                        "middle": [],
                        "last": "Shivade",
                        "suffix": ""
                    },
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "De Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Folser-Lussier",
                        "suffix": ""
                    },
                    {
                        "first": "Albert",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of NAACL-HTL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chaitanya Shivade, Marie-Catherine de Marneffe, Eric Folser-Lussier, and Albert Lai. 2015. Corpus-based discovery of semantic intensity scales. In Proceed- ings of NAACL-HTL .",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "H"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "151--161",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predict- ing sentiment distributions. In EMNLP. pages 151- 161.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [
                            "Y"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1631--1642",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep mod- els for semantic compositionality over a sentiment treebank. In EMNLP. pages 1631-1642.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Lexicon-based methods for sentiment analysis",
                "authors": [
                    {
                        "first": "Maite",
                        "middle": [],
                        "last": "Taboada",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Brooke",
                        "suffix": ""
                    },
                    {
                        "first": "Milan",
                        "middle": [],
                        "last": "Tofiloski",
                        "suffix": ""
                    },
                    {
                        "first": "Kimberly",
                        "middle": [],
                        "last": "Voll",
                        "suffix": ""
                    },
                    {
                        "first": "Manfred",
                        "middle": [],
                        "last": "Stede",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Computational linguistics",
                "volume": "37",
                "issue": "2",
                "pages": "267--307",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maite Taboada, Julian Brooke, Milan Tofiloski, Kim- berly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational lin- guistics 37(2):267-307.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Improved semantic representations from tree-structured long short-term memory networks",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sheng",
                        "suffix": ""
                    },
                    {
                        "first": "Tai",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075 .",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Context-sensitive lexicon features for neural sentiment analysis",
                "authors": [
                    {
                        "first": "Zhiyang",
                        "middle": [],
                        "last": "Teng",
                        "suffix": ""
                    },
                    {
                        "first": "Duy-Tin",
                        "middle": [],
                        "last": "Vo",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1629--1638",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016. Context-sensitive lexicon features for neural senti- ment analysis. In Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing. pages 1629-1638.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Turney",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "417--424",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classi- fication of reviews. In ACL. pages 417-424.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Dont count, predict! an automatic approach to learning sentiment lexicons for short text",
                "authors": [
                    {
                        "first": "Tin",
                        "middle": [],
                        "last": "Duy",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Vo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "219--224",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duy Tin Vo and Yue Zhang. 2016. Dont count, predict! an automatic approach to learning sentiment lexi- cons for short text. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics. volume 2, pages 219-224.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Ecnu at semeval-2016 task 7: An enhanced supervised learning method for lexicon sentiment intensity ranking",
                "authors": [
                    {
                        "first": "Feixiang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhihua",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Man",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of SemEval pages",
                "volume": "",
                "issue": "",
                "pages": "491--496",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Feixiang Wang, Zhihua Zhang, and Man Lan. 2016. Ecnu at semeval-2016 task 7: An enhanced super- vised learning method for lexicon sentiment inten- sity ranking. Proceedings of SemEval pages 491- 496.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "A regression approach to affective rating of chinese words from anew",
                "authors": [
                    {
                        "first": "Wen-Li",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Chung-Hsien",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jen-Chun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Affective Computing and Intelligent Interaction",
                "volume": "",
                "issue": "",
                "pages": "121--131",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wen-Li Wei, Chung-Hsien Wu, and Jen-Chun Lin. 2011. A regression approach to affective rating of chinese words from anew. In Affective Comput- ing and Intelligent Interaction, Springer, pages 121- 131.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "A survey on the role of negation in sentiment analysis",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Wiegand",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Balahur",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Dietrich",
                        "middle": [],
                        "last": "Klakow",
                        "suffix": ""
                    },
                    {
                        "first": "Andr\u00e9s",
                        "middle": [],
                        "last": "Montoyo",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the workshop on negation and speculation in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "60--68",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Wiegand, Alexandra Balahur, Benjamin Roth, Dietrich Klakow, and Andr\u00e9s Montoyo. 2010. A survey on the role of negation in sentiment analy- sis. In Proceedings of the workshop on negation and speculation in natural language processing. Associ- ation for Computational Linguistics, pages 60-68.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Recognizing contextual polarity in phraselevel sentiment analysis",
                "authors": [
                    {
                        "first": "Theresa",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "Janyce",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "347--354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase- level sentiment analysis. In EMNLP. pages 347- 354.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "An empirical study on the effect of negation words on sentiment",
                "authors": [
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyu",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Saif",
                        "middle": [],
                        "last": "Mohammad",
                        "suffix": ""
                    },
                    {
                        "first": "Svetlana",
                        "middle": [],
                        "last": "Kiritchenko",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "304--313",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014. An empirical study on the effect of negation words on sentiment. In ACL. pages 304-313.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Long short-term memory over recursive structures",
                "authors": [
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Parinaz",
                        "middle": [],
                        "last": "Sobhani",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyu",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "1604--1612",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over recursive structures. In ICML. pages 1604-1612.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Tree kernel-based negation and speculation scope detection with structured syntactic parse features",
                "authors": [
                    {
                        "first": "Bowei",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Qiaoming",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "968--976",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2013. Tree kernel-based negation and speculation scope detection with structured syntactic parse features. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Associa- tion for Computational Linguistics, pages 968-976.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "text": "Figure 2: The sentiment shifting with intensifiers. The number in cell(m ij ) indicates how many phrases are predicted with sentiment label i but the prediction of phrases with intensifiers changes to label j.",
                "type_str": "figure",
                "num": null,
                "uris": null
            },
            "TABREF0": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td/><td/><td>MR</td><td>SST</td></tr><tr><td colspan=\"2\"># sentences in total</td><td colspan=\"2\">10,662 11,885</td></tr><tr><td colspan=\"4\">#sen containing sentiment word 10,446 11,211</td></tr><tr><td colspan=\"2\">#sen containing negation word</td><td>1,644</td><td>1,832</td></tr><tr><td colspan=\"2\">#sen containing intensity word</td><td>2,687</td><td>2,472</td></tr><tr><td>Negation word</td><td colspan=\"3\">no, nothing, never, neither, not, seldom, scarcely, etc.</td></tr><tr><td>Intensity word</td><td colspan=\"3\">terribly, greatly, absolutely, too, very, completely, etc.</td></tr></table>",
                "text": "The data statistics.",
                "num": null
            },
            "TABREF1": {
                "html": null,
                "type_str": "table",
                "content": "<table/>",
                "text": "Examples of negation and intensity words.",
                "num": null
            },
            "TABREF2": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td/><td>MR</td><td>SST phrase-level</td><td>SST sentence-level</td></tr><tr><td>RNN</td><td>77.7*</td><td>44.8#</td><td>43.2*</td></tr><tr><td>RNTN</td><td>75.9#</td><td>45.7*</td><td>43.4#</td></tr><tr><td>LSTM</td><td>77.4#</td><td>46.4*</td><td>45.6#</td></tr><tr><td>Bi-LSTM</td><td>79.3#</td><td>49.1*</td><td>46.5#</td></tr><tr><td>Tree-LSTM</td><td>80.7#</td><td>51.0*</td><td>48.1#</td></tr><tr><td>CNN</td><td>81.5*</td><td>48.0*</td><td>46.9#</td></tr><tr><td>CNN-Tensor</td><td>-</td><td>51.2*</td><td>50.6*</td></tr><tr><td>DAN</td><td>-</td><td>-</td><td>47.7*</td></tr><tr><td>NCSL</td><td>-</td><td>51.1*</td><td>47.1#</td></tr><tr><td colspan=\"2\">LR-Bi-LSTM 82.1</td><td>-</td><td>48.6</td></tr><tr><td>LR-LSTM</td><td>81.5</td><td>-</td><td>48.2</td></tr></table>",
                "text": "The accuracy on MR and SST. Phraselevel means the models use phrase-level annotation for training. And sentence-level means the models only use sentence-level annotation. Results marked with * are re-printed from the references, while those with # are obtained either by our own implementation or with the same codes shared by the original authors.",
                "num": null
            },
            "TABREF3": {
                "html": null,
                "type_str": "table",
                "content": "<table><tr><td>Method</td><td/><td colspan=\"2\">MR SST</td></tr><tr><td>LR-Bi-LSTM</td><td/><td colspan=\"2\">82.1 48.6</td></tr><tr><td colspan=\"4\">LR-Bi-LSTM (-NSR) 80.8 46.9</td></tr><tr><td colspan=\"2\">LR-Bi-LSTM (-SR)</td><td colspan=\"2\">80.6 46.9</td></tr><tr><td colspan=\"2\">LR-Bi-LSTM (-NR)</td><td colspan=\"2\">81.2 47.6</td></tr><tr><td colspan=\"2\">LR-Bi-LSTM (-IR)</td><td colspan=\"2\">81.7 47.9</td></tr><tr><td>LR-LSTM</td><td/><td colspan=\"2\">81.5 48.2</td></tr><tr><td colspan=\"2\">LR-LSTM (-NSR)</td><td colspan=\"2\">80.2 46.4</td></tr><tr><td colspan=\"2\">LR-LSTM (-SR)</td><td colspan=\"2\">80.2 46.6</td></tr><tr><td colspan=\"2\">LR-LSTM (-NR)</td><td colspan=\"2\">80.8 47.4</td></tr><tr><td>LR-LSTM (-IR)</td><td/><td colspan=\"2\">81.2 47.4</td></tr><tr><td colspan=\"5\">Table 4: The accuracy for LR-Bi-LSTM and LR-</td></tr><tr><td colspan=\"5\">LSTM with regularizer ablation. NSR, SR, NR and</td></tr><tr><td colspan=\"5\">IR denotes Non-sentiment Regularizer, Sentiment</td></tr><tr><td colspan=\"5\">Regularizer, Negation Regularizer, and Intensity</td></tr><tr><td colspan=\"2\">Regularizer respectively.</td><td/><td/></tr><tr><td colspan=\"5\">both MR and SST subsets; 3) Similar observations</td></tr><tr><td colspan=\"5\">can be found regarding the intensity regularizer.</td></tr><tr><td>Method</td><td colspan=\"4\">Neg. Sub. MR SST MR SST Int. Sub.</td></tr><tr><td>BiLSTM</td><td colspan=\"4\">72.0 39.8 83.2 48.8</td></tr><tr><td colspan=\"3\">LR-Bi-LSTM (-NR) 74.2 41.6</td><td>-</td><td>-</td></tr><tr><td>LR-Bi-LSTM (-IR)</td><td>-</td><td>-</td><td colspan=\"2\">85.2 50.0</td></tr><tr><td>LR-Bi-LSTM</td><td colspan=\"4\">78.5 44.4 87.1 53.2</td></tr><tr><td colspan=\"5\">Table 5: The accuracy on the negation sub-dataset</td></tr><tr><td colspan=\"5\">(Neg. Sub.) that only contains negators, and in-</td></tr><tr><td colspan=\"5\">tensity sub-dataset (Int. Sub.) that only contains</td></tr><tr><td>intensifiers.</td><td/><td/><td/></tr></table>",
                "text": "Kindly note that almost all sentences contain sentiment words, see Tab. 1.",
                "num": null
            }
        }
    }
}