<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<address>
									<postBox>000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047</postBox>
									<postCode>048 049</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postBox>100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postBox>300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<postBox>700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<postBox>800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 ACL 2017 Submission 180. Confidential Review Copy. DO NOT DISTRIBUTE. 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949</postBox>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20CBD1FD8760E0BD7A9573CE38D13401</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0-SNAPSHOT" ident="GROBID" when="2024-09-06T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate datasets consisting of data from four different forums. Each of these forums constitutes its own "fine-grained domain" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semisupervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques.</p><p>We release a dataset of 93,924 posts from across 4 forums, with annotations for 1,938 posts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NLP can be extremely useful for enabling scientific inquiry, helping us to quickly and efficiently understand large corpora, gather evidence, and test hypotheses <ref type="bibr" target="#b0">(Bamman et al., 2013;</ref><ref type="bibr">O'Connor et al., 2013)</ref>. One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits <ref type="bibr">(Krebs, 2013a,b)</ref>. However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models <ref type="bibr" target="#b15">(McClosky et al., 2010)</ref> and deep models <ref type="bibr" target="#b25">(Zhang et al., 2017)</ref>. Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging.</p><p>In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums. We define a token-level annotation task where, for each post, we annotate references to the product or products being bought or sold in that post. Having the ability to automatically tag posts in this way lets us characterize the composition of a forum in terms of what products it deals with, identify trends over time, associate users with particular activity profiles, and connect to price information to better understand the marketplace. Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible. Our dataset has already proven enabling for case studies on these particular forums <ref type="bibr">(anon. work in press)</ref>.</p><p>Our task has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned. Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL <ref type="bibr" target="#b21">(Tjong Kim Sang and De Meulder, 2003)</ref>. While prior work has dealt with these messy characteristics for syntax <ref type="bibr" target="#b6">(Kaljahi et al., 2015)</ref> and for discourse <ref type="bibr" target="#b13">(Lui and Baldwin, 2010;</ref><ref type="bibr" target="#b7">Kim et al., 2010;</ref><ref type="bibr" target="#b23">Wang et al., 2011)</ref>, our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective.</p><p>Having annotated a dataset, we examine basic supervised learning approaches to the product extraction problem. Simple binary classification of tokens as product names can be effective, but performance drops off precipitously when a system trained on one forum is applied to a different fo- rum: in this sense, even two different cybercrime forums seem to represent different "fine-grained domains." Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation <ref type="bibr">(Garrette and Baldridge, 2013;</ref><ref type="bibr">Garrette et al., 2013)</ref>, tokenlevel annotation <ref type="bibr">(Daume III, 2007)</ref>, and semisupervised approaches <ref type="bibr" target="#b22">(Turian et al., 2010)</ref>. 2 We find little improvement from these methods and discuss why they fail to have a larger impact. Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset and Annotation</head><p>We consider several forums that vary in the nature of products being traded:</p><p>• Darkode: Cybercriminal wares, including exploit kits, spam services, ransomware programs, and stealthy botnets.</p><p>• Hack Forums: A mixture of cyber-security and computer gaming blackhat and noncybercrime products.</p><p>• Blackhat: Blackhat Seach Engine Optimization techniques.</p><p>• Nulled: Data stealing tools and services.</p><p>Table <ref type="table" target="#tab_0">1</ref> gives some statistics of these forums.</p><p>Figure <ref type="figure">1</ref> shows two examples of posts from Darkode. In addition to aspects of the annotation, which we will discuss shortly, we see that 2 Of course, these techniques can also be combined in a single system <ref type="bibr" target="#b10">(Kshirsagar et al., 2015</ref> Figure <ref type="figure">1</ref>: Example post and annotations from Darkode, with one sentence per line. Annotated product tokens are underlined. The first example is quite short and straightforward. The second exhibits our annotations of both the core product (mod DCIBot) and the method for obtaining that product (sombody).</p><p>the text exhibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual formatting, particularly in thread titles. Additionally, note how some words are present that might be products in other contexts, but are not here (e.g., obfuscator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Process</head><p>We developed our annotation guidelines through six preliminary rounds of annotation, covering 560 posts. Each round was followed by discussion and resolution of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The data annotated during this process is not included in We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit <ref type="bibr" target="#b14">(Manning et al., 2014)</ref>. Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task slightly easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words.</p><p>Our full annotation guide will be available in supplementary material. Our basic annotation principle is to annotate tokens when they are either the product that will be delivered or are an integral part of the method leading to the delivery of that product. For example, in Figure <ref type="figure">1</ref>, the fragment mod DCIBot is the core product, but coder is annotated as well, since this person is inextricably linked to the service being provided: the post seeks someone to perform the task. However, the Backconnect bot example is a more straightforward transaction (the human agent is less integral), so only the product is annotated. Verbs may also be annotated, as in the case of hack an account: here hack is the method and the deliverable is the account, so both are annotated.</p><p>When the product is a multiword expression (e.g., Backconnect bot), it is almost exclusively a noun phrase, in which case we annotate the head word of the noun phrase (bot). Annotating single tokens instead of spans meant that we avoided having to agree on an exact parse of each post, since even the boundaries of base noun phrases can be quite difficult to agree on in ungrammatical text.</p><p>If multiple different products are being bought/sold, we annotate them all. We do not annotate:</p><p>• Features of products, e.g., Update Cmd in Figure <ref type="figure">1</ref>.</p><p>• Generic product references, e.g., this, them.</p><p>• Product mentions inside "vouches" (reviews from other users).</p><p>• Product mentions outside of the first and last 10 lines of each post. <ref type="foot" target="#foot_2">4</ref>Table <ref type="table" target="#tab_0">1</ref> shows interannotator agreement according to our annotation scheme. We use the Fleiss' Kappa measurement <ref type="bibr" target="#b3">(Fleiss, 1971)</ref>, treating our task as a token-level annotation where every token is annotated as either a product or not. We chose this measure as we are interested in agreement between more than two annotators (ruling out Cohen's kappa), have a binary assignment (ruling out correlation coefficients) and have datasets large enough that the biases Krippendorff's Alpha addresses are not a concern. The values indicate reasonable agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discussion</head><p>Because we annotate entities in a context-sensitive way (i.e., only annotating those in product context), our task resembles a post-level information extraction task. The product information in a post can be thought of as a list-valued slot to be filled in the style of TAC KBP <ref type="bibr" target="#b19">(Surdeanu, 2013;</ref><ref type="bibr" target="#b20">Surdeanu and Ji, 2014)</ref>, with the token-level annotations constituting provenance information. However, we chose to anchor the task fully at the token level to simplify the annotation task: at the post level, we would have to decide whether two distinct product mentions were actually distinct products or not, which requires heavier domain knowledge. Our approach also resembles the fully token-level annotations of entity and event information in the ACE dataset <ref type="bibr" target="#b16">(NIST, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics</head><p>In light of the various views on this task and its different requirements for different potential applications, we describe and motivate a few distinct evaluation metrics below. The choice of metric will impact system design, as we discuss in the following sections.</p><p>Token-level accuracy We can follow the approach used in token-level tasks like NER and compute precision, recall, and F 1 over the set of tokens labeled as products. This most closely mimics our annotation process.</p><p>Type-level product extraction (per post) For many applications, the primary goal of the extraction task is more in line with KBP-style slot filling, where we care about the set of products extracted from a particular post. Without a domain-specific lexicon containing full synsets of products we care about (e.g., something that could recognize the synonymity of hack and access), it is difficult to evaluate this in a fully satisfying way. However, we can approximate this evaluation by comparing the set of (lowercased, stemmed) product types in a post with the set of product types predicted by the system. Again, we can consider precision, recall, and F 1 over these two sets. This metric favors systems that consistently make correct postlevel predictions even if they do not retrieve every token-level occurrence of the product.</p><p>Post-level accuracy Our type-level extraction will naturally be a conservative estimate of performance simply because there may seem to be multiple "products" that are actually just different ways of referring to one core product. Roughly 60% of posts in the two forums contain multiple annotated tokens that are distinct beyond stemming and lowercasing. However, we analyzed 100 of these multiple product posts across Darkode and Hack Forums, and found that only 6 of them were actually selling multiple products, indicating that posts selling multiple types of products are actually quite rare (roughly 3% of cases overall). In the rest of the cases, the variations were due to slightly different ways of describing the same product.</p><p>In light of this, we also might consider asking the system to extract some product reference from the post, rather than all of them. Specifically, we compute accuracy on a post-level by checking whether a single product type extracted by the system is contained in the annotated set of product types. Because most posts feature one product, this metric is sufficient to evaluate whether we understood what the core product of the post was.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase-level Evaluation</head><p>Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. In Figure <ref type="figure">1</ref>, for example, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product.</p><p>We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. We used the parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> to parse all sentences of each post. For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake. In Figure <ref type="figure">1</ref>, the entire noun phrase Backconnect bot would be labeled as a product. For products realized as verbs (e.g., hack), we leave the annotation as the single token.</p><p>Throughout the rest of this work, we will evaluate sometimes at the token-level and sometimes at the NP-level<ref type="foot" target="#foot_3">5</ref> (including for the product type evaluation and post-level accuracy); we will specify which evaluation is used where.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We consider several simple baselines for product extraction as well as two learning-based methods.</p><p>Baselines A simple frequency baseline is to take the most frequent noun or verb in a post and classify all occurrences of that word type as products. A more sophisticated lexical baseline is based on a product dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. Finally, we can tag the first noun phrase of the post as a product, which will often capture the product if it is mentioned in the title of the post. <ref type="foot" target="#foot_4">6</ref>Binary classifier One learning-based approach to this task is to employ a binary SVM classifier for each token in isolation. <ref type="foot" target="#foot_5">7</ref> Our features look at both the token under consideration as well as neighboring tokens, as described in the next paragraph. A vector of "base features" is extracted for each of these target tokens: these include 1) sentence position in the document and word position in the current sentence as bucketed indices; 2) word identity (for common words), POS tag, and dependency relation to parent for each word in a window of size 3 surrounding the current word; 3) character 3-grams of the current word. The same base feature set is used for every token.</p><p>Our token-classifying SVM extracts base features on the token under consideration as well as its syntactic parent. Before inclusion in the final classifier, these features are conjoined with an indicator of their source (i.e., the current token or the parent token). Our NP-classifying SVM extracts base features on first, last, head, and syntactic parent tokens of the noun phrase, again with each feature conjoined with its token source.</p><p>We weight false positives and false negatives differently to adjust the precision/recall curve (tuned on development data for each forum), and we also empirically found better performance by upweighting the contribution to the objective of singleton products (product types that occur only once in the training set).</p><p>Post-level classifier As discussed in Section 3, one metric we are interested in is whether we can find any occurrence of a product in a post. This task may be easier than the general tagging problem: for example, if we can effectively identify the product in the title of a post, then we do not need to identify additional references to that product in the body of the post. Therefore, we also consider a post-level model, which directly tries to select one token (or NP) out of a post as the most likely product. Structuring the prediction problem in this way naturally lets the model be more conservative in its extractions and simplifies the task, since highly ambiguous product mentions can be ignored if a clear product mention is present. Put another way, it supplies a form of prior knowledge that can be useful for the task, namely that each post has exactly one product.</p><p>Our post-level system is formulated as an instance of a latent SVM (Yu and <ref type="bibr" target="#b24">Joachims, 2009)</ref>. The output space is the set of all tokens (or noun phrases, in the NP case) in the post. The latent variable is the choice of token/NP to select, since there may be multiple annotated tokens. The features used on each token/NP are the same as in the token classifier. We trained all of the learned models by subgradient descent on the primal form of the objective <ref type="bibr" target="#b18">(Ratliff et al., 2007;</ref><ref type="bibr" target="#b11">Kummerfeld et al., 2015)</ref>. We use AdaGrad <ref type="bibr" target="#b2">(Duchi et al., 2011)</ref> to speed convergence in the presence of a large weight vector with heterogeneous feature types. All product extractors in this section are trained for 5 iterations with 1 -regularization tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Results</head><p>Table <ref type="table">2</ref> shows development set results on Darkode for each of the four systems for each metric described in Section 3. Our learning-based systems substantially outperform the baselines on the metrics they are optimized for. The post-level system underperforms the binary classifier on the token evaluation, but is superior at not only post-level accuracy but also product type F 1 . This lends credence to our hypothesis that picking one product suffices to characterize a large fraction of posts.</p><p>When evaluated on noun phrases, we see that the systems generally perform better, an unsurprising result given that this evaluation is more  forgiving (token distinctions within noun phrases are erased). The post-level NP system achieves an F-score of roughly 80 on product type identification and post-level accuracy is around 90%. While there is room for improvement, this variant of the system is accurate enough to enable analysis of the entire Darkode forum with automatic annotation.</p><p>Throughout the rest of this work, we focus on NP-level evaluation and post-level NP accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Domain Adaptation</head><p>Table <ref type="table">2</ref> only showed results for training and evaluating within the same forum (Darkode). However, we wish to apply our system to extract product occurrences from a wide variety of forums, so we are interested in how well the system will generalize to a new forum. Tables <ref type="table" target="#tab_5">3</ref> and<ref type="table" target="#tab_6">4</ref> show full results of several systems in within-forum and crossforum evaluation settings. Performance is severely degraded in the cross-forum setting compared to the within-forum setting, e.g., on NP-level F 1 , a Hack Forums-trained model is 14.4 F 1 worse at the Darkode task than a Darkode-trained model (63.2 vs. 77.6). Differences in how the systems adapt between different forums will be explored more thoroughly in Section 5.4.</p><p>In the next few sections, we explore several possible methods for improving results in the crossforum settings and attempting to build a more domain-general system. These techniques generally reflect two possible hypotheses for the difficult cross-domain effects:</p><p>Hypothesis 1: Product inventories are the primary difference across domains; context-based features will transfer, but the main challenge is not being able to recognize unknown products. Hypothesis 2: Product inventories and stylistic conventions both differ across domains; we need to capture both to adapt models successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Brown Clusters</head><p>To test Hypothesis 1, we investigate whether additional lexical information helps identify productlike words in new domains. A classic semisupervised technique for exploiting unlabeled target data is to fire features over word clusters or word vectors <ref type="bibr" target="#b22">(Turian et al., 2010)</ref>. These features should generalize well across domains that the clusters are formed on: if product nouns occur in similar contexts across domains and therefore wind up in the same cluster, then a model trained on domain-limited data should be able to learn that that cluster identity is indicative of products. We form Brown clusters on our unlabeled data from both Darkode and Hack Forums (see Table <ref type="table" target="#tab_0">1</ref> for sizes). We use <ref type="bibr" target="#b12">Liang (2005)</ref>'s implementation to learn 50 clusters. 8 Upon inspection, these clusters do indeed capture some of the semantics relevant to the problem: for example, the cluster 110 has as its most frequent members service, account, price, time, crypter, and server, many of which are product-associated nouns. We incorporate these as features into our model by characterizing each token with prefixes of the Brown cluster ID; we used prefixes of length 2, 4, and 6.</p><p>Tables <ref type="table" target="#tab_5">3</ref> and<ref type="table" target="#tab_6">4</ref> show the results of incorporating Brown cluster features into our trained models. Overall, these features lead almost uniformly to slight improvements on NP-level product F 1 . Results are more mixed on post-level accuracy, and overall, only one result improves significantly. This indicates that Brown clusters might be a useful feature, but do not solve the domain adaptation problem in this context. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Type-level Annotation</head><p>Another approach following Hypothesis 1 is to use small amounts of supervised data, One cheap approach for annotating data in a new domain is to exploit type-level annotation <ref type="bibr">(Garrette and Baldridge, 2013;</ref><ref type="bibr">Garrette et al., 2013)</ref>. Our tokenlevel annotation standard is relatively complex and time-consuming, but a researcher could quite easily provide a few exemplar products for a new forum based on just a few minutes of reading posts and analyzing the forum.</p><p>Given the data that we've already annotated, we can simulate this process by iterating through our labeled data and collecting annotated product names that are sufficiently common. Specifically, we take all (lowercased, stemmed) product tokens and keep those occurring at least 4 times in the training dataset (recall that these datasets are ≈ 700 posts). This gives us a list of 121 products in Darkode and 105 products in Hack Forums.</p><p>To incorporate this information into our system, we add a new feature on each token indicating whether or not it occurs in the gazetteer. At training time, we scrape the gazetteer from the training set. At test time, we use the gazetteer from 8 The number of clusters was selected based on experimentation with our various development sets. 9 Vector representations of words are also possible here; however, our initial experiments with these did not show better gains than with using Brown clusters. That is consistent with the results of <ref type="bibr" target="#b22">Turian et al. (2010)</ref> who showed similar performance between Brown clusters and word vectors for chunking and NER. Figure <ref type="figure">2</ref>: Token-supervised domain adaptation results for two settings. As our system is trained on an increasing amount of target-domain data (xaxis), its performance generally improves. However, adaptation from Hack Forums to Darkode is much more effective than the other way around, and using domain features as in Daume <ref type="bibr">III (2007)</ref> gives little benefit over naïve use of the new data.</p><p>the target test domain as a form of partial typelevel supervision. Tables <ref type="table" target="#tab_5">3</ref> and<ref type="table" target="#tab_6">4</ref> shows the results of incorporating the gazetteer into the system. In aggregate, gazetteers appear to provide a slight gain over the baseline system, like Brown clusters, though many of these individual improvements are not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Token-level Annotation</head><p>We now turn our attention to methods that might address Hypothesis 2. If we assume the domain transfer problem is more complex, we really want to leverage labeled data in the target domain rather than attempting to transfer features based only on type-level information. Specifically, we are interested in cases where a relatively small number of labeled posts (less than 100) might provide substantial benefit to the adaptation; a researcher could plausibly do this annotation in a few hours. We consider two ways of exploiting labeled target-domain data. The first is to simply take these posts as additional training data. The second is to also employ the "frustratingly easy" domain adaptation method of Daume <ref type="bibr">III (2007)</ref>. In this framework, each feature fired in our model is actually fired twice: one copy is domaingeneral and one is conjoined with the domain label (here, the name of the forum). <ref type="foot" target="#foot_6">10</ref>  Table <ref type="table">5</ref>: Product token OOV rates on development sets (test set for Blackhat and Nulled) of various forums with respect to training on Darkode and Hack Forums. We also show the recall of an NP-level system on seen (R seen ) and OOV (R oov ) tokens. Darkode seems to be more "general" than Hack Forums: the Darkode system generally has lower OOV rates and provides more consistent performance on OOV tokens than the Hack Forums system. so, the model should gain some ability to separate domain-general from domain-specific feature values. For both training methods, we upweight the contribution of the target-domain posts in the objective by a factor of 5. Figure <ref type="figure">2</ref> shows learning curves for both of these methods in two adaptation settings as we vary the amount of labeled target-domain data. The system trained on Hack Forums is able to make good use of labeled data from Darkode: having access to 20 labeled posts leads to gains of roughly 7 F 1 . Interestingly, the system trained on Darkode is not able to make good use of labeled data from Hack Forums, and the domain-specific features actually cause a substantial drop in performance until we have included a substantial amount of data from Hack Forums. This likely indicates we are overfitting the small Hack Forums training set with the domain-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>In order to understand the variable performance and shortcomings of the domain adaptation approaches we explored, it is useful to examine our two initial hypotheses and characterize the datasets a bit further. To do so, we break down system performance on products seen in the training set versus novel products. Because our systems depend on lexical and character n-gram features, we expect that they will do better at predicting products we have seen before.</p><p>Table <ref type="table">5</ref> confirms this intuition: it shows product out-of-vocabulary rates in each of the four forums relative to training on both Darkode and Hack Forums, along with recall of a NP-level system on both previously seen and OOV products. As expected, performance is substantially higher on invocabulary products. Interestingly, OOV rates of a Darkode-trained system are generally lower, indicating that that forum has better all-around product to up to k + 1 total versions of each feature. coverage. A system trained on Darkode is therefore in some sense more domain-general than one trained on Hack Forums.</p><p>This would seem to confirm Hypothesis 1. Table <ref type="table" target="#tab_5">3</ref> shows that the Hack Forums-trained system achieves a 20% error reduction on Hack Forums compared to a Darkode-trained system. In contrast, the Darkode-trained system obtains a 43% error reduction on Darkode relative to a Hack Forums-trained system. Darkode's better product coverage helps explain why Section 5.3 showed better performance of adapting Hack Forums to Darkode than vice versa: augmenting Hack Forums data with a few posts from Darkode can give critical knowledge about new products, but this is less true if the forums are reversed. Duplicating features and adding parameters to the learner also has less of a clear benefit when adapting from Darkode, when the types of knowledge that need to be added are less concrete.</p><p>Note, however, that these results do not tell the full story. Table <ref type="table">5</ref> reports recall values, but not all systems have the same precision/recall tradeoff: although they were tuned to balance precision and recall on their respective development sets, the Hack Forums-trained system is much more precision-oriented on Nulled than the Darkodetrained system. In fact, Table <ref type="table" target="#tab_5">3</ref> shows that the Hack Forums-trained system actually performs better on Nulled. This indicates that there is some truth to Hypothesis 2: product coverage is not the only important factor determining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a new dataset of posts from cybercrime marketplaces annotated with product references, a task which blends IE and NER. Learning-based methods degrade in performance when applied to new forums, and while we explore methods for fine-grained domain adaption in this data, effective methods for this task are still an open question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Forum statistics. The left columns (posts and words per post) are calculated over all data, while the right columns are based on annotated data only. Slashes indicate the train/development/test split for Darkode and train/test split for Hack Forums. Agreement is measured using Fleiss' Kappa; the two columns cover data where three annotators labeled each post and a subset labeled by all annotators.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Words Products</cell><cell cols="4">Annotated Annotators Inter-annotator agreement</cell></row><row><cell>Forum</cell><cell cols="3">Posts per post per post</cell><cell>posts</cell><cell cols="3">per post 3-annotated all-annotated</cell></row><row><cell cols="2">Darkode 3,368</cell><cell>61.5</cell><cell cols="2">3.2 660/100/100</cell><cell>3/8/8</cell><cell>0.62</cell><cell>0.66</cell></row><row><cell cols="2">Hack Forums 51,271</cell><cell>58.9</cell><cell>2.2</cell><cell>758/140</cell><cell>3/4</cell><cell>0.58</cell><cell>0.65</cell></row><row><cell>Blackhat</cell><cell>167</cell><cell>174</cell><cell>3.2</cell><cell>80</cell><cell>3</cell><cell>0.66</cell><cell>0.67</cell></row><row><cell cols="2">Nulled 39,118</cell><cell>157</cell><cell>2.3</cell><cell>100</cell><cell>3</cell><cell>0.77</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>).</figDesc><table><row><cell>0-initiator4856</cell></row><row><cell>TITLE: [ buy ] Backconnect bot</cell></row><row><cell>BODY: Looking for a solid backconnect bot .</cell></row><row><cell>If you know of anyone who codes them please let</cell></row><row><cell>me know</cell></row><row><cell>0-initiator6830</cell></row><row><cell>TITLE: Coder</cell></row><row><cell>BODY: Need sombody too mod DCIBot for me add the</cell></row><row><cell>following :</cell></row><row><cell>Update Cmd</cell></row><row><cell>Autorun Obfuscator ( each autorun diffrent &amp; fud )</cell></row><row><cell>Startup Mod ( needs too work on W7/VISTA )</cell></row><row><cell>Pm .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test set results at the NP level in within-forum and cross-forum settings for a variety of different systems. Using either Brown clusters or gazetteers gives mixed results on cross-forum performance: none of the improvements are statistically significant with p &lt; 0.05 according to a bootstrap resampling test. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test set results at the whole-post level in within-forum and cross-forum settings for a vari-</figDesc><table><row><cell></cell><cell cols="4">Darkode Hack Forums Blackhat Nulled</cell></row><row><cell></cell><cell></cell><cell>Trained on Darkode</cell><cell></cell><cell></cell></row><row><cell>Dict</cell><cell>62.5</cell><cell>43.3</cell><cell>48.3</cell><cell>61.6</cell></row><row><cell>Post</cell><cell>95.8</cell><cell>67.6</cell><cell>75.8</cell><cell>82.5</cell></row><row><cell>+Brown</cell><cell>92.7</cell><cell>69.8</cell><cell>75.8</cell><cell>86.0</cell></row><row><cell>+Gaz</cell><cell>93.7</cell><cell>71.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Trained on Hack Forums</cell><cell></cell><cell></cell></row><row><cell>Dict</cell><cell>51.0</cell><cell>52.2</cell><cell>51.6</cell><cell>56.9</cell></row><row><cell>Post</cell><cell>84.3</cell><cell>80.8</cell><cell>80.6</cell><cell>83.7</cell></row><row><cell>+Brown</cell><cell>†89.5</cell><cell>81.6</cell><cell>80.6</cell><cell>83.7</cell></row><row><cell>+Gaz</cell><cell>85.4</cell><cell>†85.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">ety of different systems. Here, Brown clusters and</cell></row><row><cell cols="5">gazetteers may be slightly more impactful; dag-</cell></row><row><cell cols="5">gers indicate statistically significant gains over the</cell></row><row><cell cols="5">post-level system with p &lt; 0.05 according to a</cell></row><row><cell cols="3">bootstrap resampling test.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>In doing</figDesc><table><row><cell>Test</cell><cell></cell><cell>Darkode</cell><cell></cell><cell></cell><cell>Hack Forums</cell><cell></cell><cell></cell><cell>Blackhat</cell><cell></cell><cell></cell><cell>Nulled</cell><cell></cell></row><row><cell>System</cell><cell cols="12">% OOV Rseen Roov % OOV Rseen Roov % OOV Rseen Roov % OOV Rseen Roov</cell></row><row><cell>Binary (Darkode)</cell><cell>21</cell><cell>82</cell><cell>67</cell><cell>41</cell><cell>69</cell><cell>51</cell><cell>44</cell><cell>73</cell><cell>51</cell><cell>33</cell><cell>73</cell><cell>50</cell></row><row><cell>Binary (HF)</cell><cell>52</cell><cell>75</cell><cell>35</cell><cell>36</cell><cell>77</cell><cell>40</cell><cell>54</cell><cell>71</cell><cell>36</cell><cell>38</cell><cell>79</cell><cell>37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available upon publication, along with all models and code from this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Not pictured is the fact that we annotated small numbers of Darkode training, Hack Forums training, and Blackhat test posts with all annotators in order to simply check agreement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>In preliminary annotation we found that content in the middle of the post typically described features or gave instructions, without explicitly mentioning the product. Most posts are unaffected by this rule (96% of Darkode, 77% of Hack Forums, 84% of Blackhat, and 93% of Nulled), but it still substantially reduced annotator effort because the posts it does affect are quite long -61 lines on average.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Where NP-level means "noun phrases and verbs" as described in Section 3.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>  6  Since this baseline fundamentally relies on noun phrases, we only evaluate it in the noun phrase setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>This performs similarly to using a token-level CRF with a binary tagset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>If we are training on data from k domains, this gives rise</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Latent Personas of Film Characters</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
	<note type="raw_reference">David Bamman, Brendan O&apos;Connor, and Noah A. Smith. 2013. Learning Latent Personas of Film Characters. In Proceedings of the 51st An- nual Meeting of the Association for Computa- tional Linguistics (ACL). Association for Computa- tional Linguistics, Sofia, Bulgaria, pages 352-361. http://www.aclweb.org/anthology/P13-1035.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P07-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL). Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics (ACL). Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2014. 2007</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</note>
	<note type="raw_reference">Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). http://aclweb.org/anthology/D/D14/D14-1082.pdf. Hal Daume III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Lin- guistics (ACL). Association for Computational Lin- guistics, Prague, Czech Republic, pages 256-263. http://www.aclweb.org/anthology/P07-1033.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/duchi11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learn- ing and Stochastic Optimization. Journal of Machine Learning Research 12:2121-2159. http://jmlr.org/papers/v12/duchi11a.html.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. L. Fleiss. 1971. Measuring nominal scale agree- ment among many raters. Psychological Bulletin 76(5):378-382.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a Part-of-Speech Tagger from Two Hours of Annotation</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dan Garrette and Jason Baldridge. 2013. Learning a Part-of-Speech Tagger from Two Hours of Anno- tation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT). Association for Computa- tional Linguistics, Atlanta, Georgia, pages 138-147. http://www.aclweb.org/anthology/N13-1014.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1057" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguis- tics (ACL). Association for Computational Linguistics, Sofia, Bulgaria, pages 583-592. http://www.aclweb.org/anthology/P13-1057.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Foreebank: Syntactic Analysis of Customer Support Forums</title>
		<author>
			<persName><forename type="first">Rasoul</forename><surname>Kaljahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johann</forename><surname>Roturier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Ribeyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roux</forename></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1341" to="1347" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rasoul Kaljahi, Jennifer Foster, Johann Roturier, Corentin Ribeyre, Teresa Lynn, and Joseph Le Roux. 2015. Foreebank: Syntactic Analysis of Customer Support Forums. In Proceedings of the Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP). Association for Computational Linguistics, Lisbon, Portugal, pages 1341-1347. http://aclweb.org/anthology/D15-1157.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagging and Linking Web Forum Posts</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W10-2923" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
	<note type="raw_reference">Su Nam Kim, Li Wang, and Timothy Bald- win. 2010. Tagging and Linking Web Fo- rum Posts. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL). Association for Computational Linguistics, Uppsala, Sweden, pages 192-202. http://www.aclweb.org/anthology/W10-2923.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Brian</forename><surname>Krebs</surname></persName>
		</author>
		<ptr target="http://www.krebsonsecurity.com/2013/12/cards-stolen-in-target-breach-flood-underground-markets" />
		<title level="m">Cards Stolen in Target Breach Flood Underground Markets</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Krebs. 2013a. Cards Stolen in Tar- get Breach Flood Underground Markets. http://www.krebsonsecurity.com/2013/12/cards- stolen-in-target-breach-flood-underground-markets.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Brian</forename><surname>Krebs</surname></persName>
		</author>
		<ptr target="http://www.krebsonsecurity.com/2013/12/whos-selling-credit-cards-from-target" />
		<title level="m">Who&apos;s Selling Credit Cards from Target</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Krebs. 2013b. Who&apos;s Sell- ing Credit Cards from Target? http://www.krebsonsecurity.com/2013/12/whos- selling-credit-cards-from-target.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Frame-Semantic Role Labeling with Heterogeneous Annotations</title>
		<author>
			<persName><forename type="first">Meghana</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-2036" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="218" to="224" />
		</imprint>
	</monogr>
	<note type="raw_reference">Meghana Kshirsagar, Sam Thomson, Nathan Schnei- der, Jaime Carbonell, Noah A. Smith, and Chris Dyer. 2015. Frame-Semantic Role Labeling with Heterogeneous Annotations. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics (ACL). Association for Compu- tational Linguistics, Beijing, China, pages 218-224. http://www.aclweb.org/anthology/P15-2036.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Optimization for Max-Margin NLP</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D15/D15-1032.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, and Dan Klein. 2015. An Empirical Analysis of Optimization for Max-Margin NLP. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Lisbon, Portugal. http://aclweb.org/anthology/D/D15/D15-1032.pdf.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning for Natural Language Processing</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s Thesis</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Percy Liang. 2005. Semi-Supervised Learning for Nat- ural Language Processing. In Master&apos;s Thesis, Mas- sachusetts Institute of Technology.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classifying User Forum Participants: Separating the Gurus from the Hacks, and Other Tales of the Internet</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/U/U10/U10-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop (ALTA)</title>
		<meeting>the Australasian Language Technology Association Workshop (ALTA)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
	<note type="raw_reference">Marco Lui and Timothy Baldwin. 2010. Classi- fying User Forum Participants: Separating the Gurus from the Hacks, and Other Tales of the Internet. In Proceedings of the Aus- tralasian Language Technology Association Work- shop (ALTA). Melbourne, Australia, pages 49- 57. http://www.aclweb.org/anthology/U/U10/U10- 1009.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-5010" />
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>The Stanford CoreNLP Natural Language Processing Toolkit</note>
	<note type="raw_reference">Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP Natu- ral Language Processing Toolkit. In Proceed- ings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demon- strations (ACL). Association for Computational Linguistics, Baltimore, Maryland, pages 55-60. http://www.aclweb.org/anthology/P14-5010.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Automatic domain adaptation for parsing</note>
	<note type="raw_reference">David McClosky, Eugene Charniak, and Mark John- son. 2010. Automatic domain adaptation for pars- ing. In Proceedings of the Conference of the North American Chapter of the Association for Computa- tional Linguistics (NAACL). Association for Compu- tational Linguistics, Los Angeles, California, pages 28-36. http://www.aclweb.org/anthology/N10- 1004.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ACE 2005 Evaluation Plan</title>
		<author>
			<persName><surname>Nist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>In NIST</note>
	<note type="raw_reference">NIST. 2005. The ACE 2005 Evaluation Plan. In NIST.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Extract International Relations from Political Context</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1094" to="1104" />
		</imprint>
	</monogr>
	<note type="raw_reference">Brendan O&apos;Connor, Brandon M. Stewart, and Noah A. Smith. 2013. Learning to Extract International Re- lations from Political Context. In Proceedings of the 51st Annual Meeting of the Association for Com- putational Linguistics (ACL). Association for Com- putational Linguistics, Sofia, Bulgaria, pages 1094- 1104. http://www.aclweb.org/anthology/P13-1108.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">(Online) Subgradient Methods for Structured Prediction</title>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">J</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v2/ratliff07a/ratliff07a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007">Martin Zinkevich. 2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nathan J. Ratliff, Andrew Bagnell, and Mar- tin Zinkevich. 2007. (Online) Subgradi- ent Methods for Structured Prediction. In Proceedings of the International Confer- ence on Artificial Intelligence and Statistics. http://www.jmlr.org/proceedings/papers/v2/ratliff07a/ratliff07a.pdf.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the TAC2013 Knowledge Base Population Evaluation: English Slot Filling and Temporal Slot Filling</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC-KBP 2013 Workshop</title>
		<meeting>the TAC-KBP 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mihai Surdeanu. 2013. Overview of the TAC2013 Knowledge Base Population Evaluation: English Slot Filling and Temporal Slot Filling,. In Proceed- ings of the TAC-KBP 2013 Workshop.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC-KBP 2014 Workshop</title>
		<meeting>the TAC-KBP 2014 Workshop</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mihai Surdeanu and Heng Ji. 2014. Overview of the English Slot Filling Track at the TAC2014 Knowl- edge Base Population Evaluation. In Proceedings of the TAC-KBP 2014 Workshop.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recog- nition. In Proceedings of the Conference on Natural Language Learning (CoNLL).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note type="raw_reference">Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben- gio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learn- ing. In Proceedings of the 48th Annual Meet- ing of the Association for Computational Lin- guistics (ACL). Association for Computational Linguistics, Uppsala, Sweden, pages 384-394. http://www.aclweb.org/anthology/P10-1040.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting Thread Discourse Structure over Technical Web Forums</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D11-1002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
	<note type="raw_reference">Li Wang, Marco Lui, Su Nam Kim, Joakim Nivre, and Timothy Baldwin. 2011. Predicting Thread Discourse Structure over Technical Web Forums. In Proceedings of the Conference on Empir- ical Methods in Natural Language Processing (EMNLP). Association for Computational Linguis- tics, Edinburgh, Scotland, UK., pages 13-25. http://www.aclweb.org/anthology/D11-1002.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Structural SVMs with Latent Variables</title>
		<author>
			<persName><forename type="first">Chun-Nam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553523</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553523" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chun-Nam John Yu and Thorsten Joachims. 2009. Learning Structural SVMs with Latent Variables. In Proceedings of the 26th Annual Interna- tional Conference on Machine Learning (ICML). ACM, New York, NY, USA, pages 1169-1176. https://doi.org/10.1145/1553374.1553523.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aspect-augmented Adversarial Networks for Domain Adaptation</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2017. Aspect-augmented Adversarial Networks for Domain Adaptation. In Transactions of the Associ- ation for Computational Linguistics (TACL).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
