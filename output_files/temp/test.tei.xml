<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MARG: Multi-Agent Review Generation for Scientific Papers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-01-08">8 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mike</forename><forename type="middle">D '</forename><surname>Arcy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Hope</surname></persName>
							<email>tomh@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Birnbaum</surname></persName>
							<email>l-birnbaum@northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>dougd@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MARG: Multi-Agent Review Generation for Scientific Papers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-08">8 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">DABB7B93AB3166461B32CF4B650644B6</idno>
					<idno type="arXiv">arXiv:2401.04259v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0-SNAPSHOT" ident="GROBID" when="2024-08-22T01:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the ability of LLMs to generate feedback for scientific papers and develop MARG, 1 a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the capabilities of large language models (LLMs) have advanced dramatically, resulting in modern models such as GPT-4 that can perform comparably to humans in some tasks <ref type="bibr">(Ope-nAI, 2023)</ref>. These advancements provide hope that LLMs may be able to assist human researchers with their writing <ref type="bibr" target="#b12">(Fok and Weld, 2023;</ref><ref type="bibr" target="#b28">Shen et al., 2023;</ref><ref type="bibr" target="#b22">Mahlow, 2023;</ref><ref type="bibr">Gmeiner and Yildirim, 2023)</ref>; however, most modern LLMs can only consume limited amounts of text and are primarily trained on non-technical text such as news articles and websites. The ability of models to comprehend and produce long, highly technical text-such as that of scientific papers-remains under-explored.</p><p>In this paper, we study the task of automatically generating actionable peer-review feedback for a scientific paper. This task comprises several reasoning challenges: a reviewer must understand the 1 https://github.com/allenai/marg-reviewer intent and significance of a work, the technical details of the methodology, and the nuances of how an experiment or proof can be claimed to support a particular conclusion. They must then identify the ways in a which a paper does or does not fall short and articulate suggestions for improvement.</p><p>Modern large language models (LLMs) face a technical challenge in addition to the reasoning challenges involved in generating reviews: namely, they are limited in the total number of tokens they can effectively reason over at once. As scientific papers can be quite long (thousands or tens of thousands of tokens, in our case), there are many cases in which it is not even possible to provide the whole paper in the model's input. Even for models that technically support large inputs, they often cannot use the full capacity effectively in practice <ref type="bibr" target="#b27">(Qin et al., 2023;</ref><ref type="bibr">Liu et al., 2023)</ref>.</p><p>We propose multi-agent review generation (MARG), a method for generating peer-review feedback by prompting an LLM (GPT-4). We find that by using multiple instances of GPT (hereinafter referred to as "agents"), giving each a portion of the paper, and allowing the agents to communicate with each other, it is possible to generate feedback across the whole paper. We additionally find that by including aspect-specific "expert" GPT agents to separately assist with generating comments on experiments, clarity, and impact, the method can perform significantly better than when having a lone agent attempt to generate all types of feedback at once; we refer to this specialized variant of our method as MARG-S.</p><p>In a user study, MARG-S generated 3.7 "good" comments per paper (rated by users), whereas a simple baseline of having a single agent generate all comments generated only 1.7 good comments, and a recently proposed method <ref type="bibr" target="#b19">(Liang et al., 2023)</ref> produced only 0.3. In addition, we found that while users perceived the majority of the comments generated by the baselines as being generic, the vast ma-jority (71%) of MARG-S's comments were rated as specific. Finally, we analyze the weaknesses of MARG-S, including high cost and internal communication errors (e.g., failing to include key information in some messages), and suggest directions for future work.</p><p>In summary, our contributions are as follows:</p><p>• We propose a novel method (MARG) that can generate high-quality peer-review feedback even for papers longer than the context size of the base model.</p><p>• We evaluate the quality of our generated feedback against two baselines, using both automatic metrics and a user study. We find that our method outperforms the strongest baseline by 6.1 recall points in the automated evaluation and generates 2.2x as many helpful comments per review in the user study.</p><p>• We conduct a thorough analysis of the generated feedback, finding that our proposed method preserves accuracy while generating much more specific comments.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Review generation</head><p>There has been a variety of work that aims to score or improve papers in specific aspects, such as checking statistical tests (Nuijten and Polanin, 2020), plagiarism detection <ref type="bibr" target="#b15">(Kalnins et al., 2015)</ref>, citation recommendation <ref type="bibr" target="#b0">(Ali et al., 2020)</ref>, and review score prediction <ref type="bibr" target="#b2">(Basuki and Tsuchiya, 2022;</ref><ref type="bibr" target="#b6">Bharti et al., 2023)</ref>, among others <ref type="bibr" target="#b17">(Kousha and Thelwall, 2023)</ref>. While these are useful tools, they are limited in scope compared to the breadth of feedback authors receive from a real review; our work aims to produce free-form textual review comments across a variety of aspects. Past work on automatic review generation primarily does so using (relatively) small models that cannot consume the full text of a paper <ref type="bibr" target="#b38">(Yuan and Liu, 2022)</ref> or use template-filling instead of generating nuanced free-form comments <ref type="bibr">(Wang et al., 2020a)</ref>. More recent work has explored using <ref type="bibr">GPT-4 (OpenAI, 2023)</ref> to verify author checklists <ref type="bibr" target="#b21">(Liu and Shah, 2023)</ref>, but this limits the variety in generated comment types.</p><p>Impressona <ref type="bibr" target="#b5">(Benharrak et al., 2023)</ref> is an editor that allows writers to create AI personas (via  to write comments on their work; this is valuable for personalization of feedback, but doesn't focus on finding good techniques and prompts for scientific review generation, and doesn't explore LM-LM interactions; as we show, a simple prompt (akin to what a user might try initially) does poorly on our task compared to our method.</p><p>Contemporaneously with our work, <ref type="bibr" target="#b19">Liang et al. (2023)</ref> conducted a large user study of review generation using GPT-4, finding that GPT-4 could generate helpful review comments. However, that work simply truncated long papers and did not attempt to address the input size limitations of GPT-4. In addition, they used a single prompt rather than attempting to construct specialized prompts and "experts" for different comment types, as we do. We compare our proposed method to that of <ref type="bibr" target="#b19">Liang et al. (2023)</ref> and find that while their approach is more efficient, ours produces more helpful comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-agent modeling</head><p>In games and robotics tasks, where there are often distinct roles being performed or multiple physical agents operating in the same environment, various problem-solving algorithms and reinforcement learning techniques have been studied to enable cooperation between agents <ref type="bibr" target="#b39">(Zhang et al., 2021;</ref><ref type="bibr" target="#b25">Oroojlooy and Hajinezhad, 2022)</ref>. Not all of these use communication for cooperation, and those that do typically exchange symbols or vectors rather than natural-language messages.</p><p>Recent work has explored multi-persona interaction with prompted LLMs to simulate artificial societies <ref type="bibr" target="#b18">(Li et al., 2023;</ref><ref type="bibr" target="#b26">Park et al., 2023)</ref> and to improve reasoning abilities <ref type="bibr" target="#b9">(Du et al., 2023;</ref><ref type="bibr">Wang et al., 2023b)</ref>, but this work does not explore the use of multi-agent modeling to scale input size limits and does not investigate their potential for highly technical tasks like scientific review generation.</p><p>Contemporaneously with our work, <ref type="bibr">Hong et al. (2023)</ref> and <ref type="bibr" target="#b35">Wu et al. (2023)</ref> have proposed general frameworks for multi-agent modeling with large language models such as GPT. <ref type="bibr">Wang et al. (2023b)</ref> has also proposed multi-persona collaboration as a way to improve LLM creativity, although they do not investigate the ability of multi-agent modeling to scale input size limits. However, none of these works explore review generation applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LLM context management</head><p>One advantage of multi-agent modeling is to circumvent the input-size limitations of LLMs, which are often prohibitive for long documents. A variety of other techniques have been investigated in prior work.</p><p>Several works have proposed methods for modifying LLM architectures in order to increase the effective input size by using alternative attention formulations <ref type="bibr">(Wang et al., 2020b;</ref><ref type="bibr" target="#b16">Kitaev et al., 2020;</ref><ref type="bibr" target="#b4">Beltagy et al., 2020;</ref><ref type="bibr" target="#b14">Ivgi et al., 2023)</ref> or incorporating memory retrieval <ref type="bibr" target="#b36">(Wu et al., 2022)</ref>. However, architecture changes often cannot be applied without retraining models from scratch, and powerful LLMs such as GPT are sometimes available only through a fixed API that does not allow low-level model modifications. This motivates us to explore techniques that can be applied without changing the underlying model.</p><p>Recently, there has been work exploring context management in LLMs by having models summarize a large input one chunk at a time and then operate on the concatenation of the summaries <ref type="bibr" target="#b34">(Wu et al., 2021)</ref>, recursively summarize their input/output history to compress it <ref type="bibr">(Wang et al., 2023a)</ref>, or incorporate retrieval <ref type="bibr" target="#b37">(Xu et al., 2023;</ref><ref type="bibr" target="#b1">Bai et al., 2023)</ref>. These strategies are effective when only part of the input is needed or when it is clear in advance what details will be important; however, in our review generation task, a paper's shortcomings may involve nuanced details that would be lost with extraction or summarization techniques, so we divide the input among multiple agents that collectively retain the full text throughout the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task definition</head><p>We formulate our task as follows: given a scientific paper, generate a list of actionable feedback comments that could help authors to improve the paper. Actionable feedback is defined the same way as in D' <ref type="bibr">Arcy et al. (2023)</ref>; that is, we focus on suggestions and criticism (including implied suggestions-e.g., a question might imply a need for clarification in the paper) rather than positive remarks (e.g., "The paper is sound and of certain interest"). In addition, we focus on substantive comments rather than simple grammatical or stylistic errors.</p><p>In both our multi-agent approach and our simple baseline, a paper is split into chunks of text so that each chunk can fit into the model's input. The splits are made on paragraph boundaries to avoid breaking sentences, and when presenting the text to the model we annotate each paragraph with its position in the paper (paragraph 1, 2, 3, etc) and the name of the section it appears in.</p><p>We note that the input format we use does not include figures or tables (as GPT-4 is a pure language model,<ref type="foot" target="#foot_0">2</ref> it cannot consume this information), and many equations are garbled or incomplete due to parsing limitations. Nonetheless, we expect that many comments can be identified from the text alone, as the main conclusions from tables and figures are often stated in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-agent review generation</head><p>In this section, we describe our proposed multiagent method for generating peer-review feedback, which we call MARG-S (Multi-Agent Review Generation with Specialized Agents). At a high level, our multi-agent architecture is formulated as follows: We define an agent as one instance of a chat-based LLM (ChatGPT, in our case); each agent has its own chat history and prompt(s). We initialize a set of agents, including three distinct types: (1) a leader agent, which is in charge of coordinating the task and the communication among agents, (2) one or more worker agents, which each receive a chunk of the task data (the paper), and (3) zero or more expert agents, which are prompted to specialize in some sub-task that serves to assist the leader agent in performing the task effectively. The leader agent is given a protocol with which it can broadcast a message to all other agents and receive responses. Finally, the leader agent is given the task instructions, and must send messages to other agents in order to obtain information and delegate sub-tasks in order to produce the final output.</p><p>Prompts for all of our methods can be found in Appendix A. An overview of our multi-agent architecture is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Agents</head><p>Chat-based LLMs, such as ChatGPT, take an input that consists of a list of messages. Each message consists of the message content and the "role" that the message is from, which in ChatGPT's case can be either the "system", the "user", or the "assistant" (i.e., generated by the LLM). Typically, an input to the model starts with a "system" message that describes general instructions that it must follow (e.g., "always give concise and helpful answers"), then the "user" writes a message ("summarize the Figure <ref type="figure" target="#fig_5">2</ref>: Overview of MARG-S, which consists of several specialized multi-agent groups. The comments from each group are concatenated to produce the overall review, and each comment is refined (and potentially pruned) by an additional multi-agent group to produce the final review. following passage: ..."), and the generated response is treated as an "assistant" message. The message history serves as a form of context management; with it, agents can use information from previous interactions in the conversation when formulating future responses.</p><p>We use the "system" message at the start of a message history to give unique instructions to each agent type. For example, the "leader" agent is told that it is the leader, that it must coordinate other agents to complete the user's requests, and that it can communicate by using a special "SEND MESSAGE" command to broadcast messages to other agents. It is also given some guidelines to improve its reasoning; for example, it is instructed to create a high-level plan from its task instructions before it begins communicating and performing sub-tasks. The "worker" agents are told that they must obey instructions from the leader agent, and "experts" are given special instructions depending on the subtask they need to perform.</p><p>Despite their name, expert agents do not actually have more information or expertise than any other agent. Rather, they are given a special prompt that is designed to encourage them to specialize in a particular sub-task. For example, an expert agent that is asked to focus on experiments and evaluation is given a prompt that encourages it to think about the kinds of experiments that it would expect to see in order to support a particular claim, and then compare those hypothesized experiments to the real experiments in the paper. We found in preliminary testing that giving such instructions to the leader agent tends to work poorly and often ignores details of the instructions, as though the model is overloaded by the number of instructions it is trying to follow, while refactoring the subtask to the separate expert model produces a much higher-quality result.</p><p>All agents are given some information about the agent group; they are told how many agents are in the group and the IDs of the agents (while the IDs are not directly used in the communication protocol, they are useful for internal chain-of-thought; for example, the leader might note that it needs to follow up with a particular agent). In our setting there is always exactly one leader agent, N worker agents for a paper with N chunks, and zero or more expert agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Communication</head><p>The leader agent is able to send messages to other agents by outputting a special string ("SEND MESSAGE:") followed by the message content. The message is then broadcast to all other agents in the group. When an agent receives a message, it is appended to the history as a "system" message with the header "Message from &lt;agent id&gt;:" preceding the message. The LLM is then run to generate a response to the received message, and this response is always treated as a reply to the leader agent. Replies from all agents are added to the message history of the leader agent before generating the next output from the leader.</p><p>When the leader agent generates an output that does not send a message (and thus does not seek any additional information), the task is complete and we prompt the agent to return the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error correction</head><p>We attempt to correct a few common errors that occur in practice when agents try to use the communication protocol. In some cases, agents get stuck in a loop, often when the task is complete. For example, the leader agent might send a message saying "Thank you all for your feedback and cooperation.", the workers respond with "You're welcome, Agent 0.", the leader says "Thank you all for your responses.", and this loop of unending pleasantries continues. Such loops typically devolve into exactly the same messages being sent repeatedly, so we check if a message is ever duplicated and if so, we interject with a user message indicating that the message has been duplicated and that it should not be sent again.</p><p>We also observe from preliminary experiments that the leader agent does not always remember to follow the protocol for sending a message and simply writes the message body without the necessary header, especially as the conversation grows longer. This is mitigated by including a short reminder every time the leader agent receives messages, reminding it that it must use the appropriate protocol if it wants to respond.</p><p>Finally in some cases the leader agent explicitly addresses a message to one agent (e.g., the expert), but that agent does not recognize the message as being addressed to them. To identify such cases and speed up inference, we add a prompt instruction with a specific string an agent should output if they wish to not respond to a message. We detect the presence of any agent ID in a sent message, and if the agent in question outputs the no-response string, we inject a follow-up message reminding them that their name is in the message and encouraging them to respond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Context management</head><p>Our experiments use the 8k-token version of GPT-4. Versions that can consume more tokens have been developed, but they were not available to us as of this work, and some studies have suggested that even models that can ostensibly consume a large number of tokens may not be able to attend to all of them effectively in practice <ref type="bibr" target="#b27">(Qin et al., 2023;</ref><ref type="bibr">Liu et al., 2023)</ref>. MARG can scale beyond the token limit of the base LLM by distributing the input across worker agents, and does not require any individual agent to process a large number of tokens. However, with a large number of agents or many turns of inter-agent discussion (both of which increase with paper length), the combined tokens of messages sent between agents could cause the message history to eventually exceed the input token limit for especially long papers.</p><p>To mitigate the impact of long discussions between agents, we prune old messages from the history on each round of communication. The pruning strategy is different depending on the agent type.</p><p>History length is most limited for the worker agents, which each have a paper chunk occupying most of their token limit, so the histories for workers were trimmed to the initial prompts plus the three most recent messages. For the leader agent, we observe that (1) a long history is sometimes necessary for in-depth discussions, (2) the majority of tokens in the history arise from all the messages it receives from (potentially many) other agents, and (3) as the leader relays information between other agents, it generally summarizes any important information from messages it receives. We therefore prune the past messages received from other agents, but keep the full history of outgoing messages. Finally, for expert agents we never observed issues with the token limit, so no pruning was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Review generation</head><p>To tune prompts for review generation, we performed several hundred rounds of manual iteration on a small set of papers from ARIES <ref type="bibr" target="#b8">(D'Arcy et al., 2023)</ref>. As the review generation task is somewhat subjective and there are a large number of potential shortcomings with different levels of severity, it is not always straightforward to determine whether a model has made a clear error or if it simply has a difference of opinion with respect to what the most important comments are. We found it helpful to manually alter some of the papers to create severe and obvious errors that we could expect the model to identify; for example, removing an entire section or adding an unfounded claim (e.g., "the proposed method achieves artificial general intelligence"). Surprisingly, these "obvious" errors were often not trivial for the system to recognize, making the altered papers useful for finding and mitigating blind-spots. The final prompts are shown in Appendix A, and an outline of our system structure is described in the following paragraphs and shown in Figure <ref type="figure" target="#fig_5">2</ref>.</p><p>We use three independent multi-agent groups to generate different kinds of review comments. The task prompt given to the leader agent is different for each comment type, and each group has one expert. The comment types are based loosely on points in the ICLR reviewer guidelines. 3 In particular, it asks "[...] is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?" We group and slightly reframe these points to arrive at the following comment types:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and evaluation:</head><p>The leader is instructed to focus on verifying that the experiments and theoretical proofs are correct and adequately support the paper's claims. The expert in this group is told to "design high-quality experiments" given the main claims made in the paper, inspired in part by the fact that making predictions is an effective active reading strategy to improve comprehension in humans <ref type="bibr" target="#b11">(Fielding and Others, 1990;</ref><ref type="bibr" target="#b10">Duke and Pearson, 2009)</ref>. In preliminary experiments without the expert, the model could identify some bad experiments and give generic comments, but struggled to realize when an experiment was missing. Explicitly designing experiments provides a baseline with which to compare the experiments in the paper, allowing the model to recognize missing or incomplete experiments.</p><p>3 https://iclr.cc/Conferences/2023/ ReviewerGuide</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarity and reproducibility:</head><p>The leader is instructed to focus on ensuring that the paper clearly explains key concepts and proposed methods, and that it provides all necessary details to implement any proposed methods and reproduce experiments. The expert in this case is instructed to be "highly curious" and to ask questions of the leader agent in order to learn more about the paper. This process aids in identifying any questions that can't be answered based on the paper, which become comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novelty and impact:</head><p>The leader is instructed to focus on the novelty and impact of the paper. However, we note that for our study the task of accurately retrieving related work is out of scope, so this comment type is limited to identifying errors in the paper's own explanations. Specifically, the model is instructed to verify that the paper clearly states and justifies its motivations, goals, and key findings, and that it thoroughly discusses how it fits into the existing literature. The expert in this case is instructed to be skeptical of the paper and ask questions to determine if it actually makes a significant contribution to its field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Refinement</head><p>After generating a review, we find that it is very helpful to include a "refinement" stage, in which the model is given the review comments and asked to improve (or remove) them. Various errors can arise during the initial comment generation, and we observe that models tend to be poor at self-reflection and correction during that stage. Including refinement as a separate stage can resolve many of the errors introduced during the initial generation.</p><p>To refine comments, we initialize a new multiagent group with no expert agent. For each comment, we provide the comment to the leader agent with a prompt instructing it to ensure that the comment is clear, that it is specific, and that it is valid (i.e., does not suggest something that is already done in the paper). The model outputs a list; usually this list contains one element (the newly-refined comment), but may contain more (if the original comment mixed two different suggestions) or be null (if the comment was invalid). The comments are processed independently (i.e., by separate multiagent groups).</p><p>In this section, we will describe the baseline methods that we compare against our multi-agent approach. We consider three baselines: a simple baseline that treats chunks independently and uses a one-line prompt, a baseline that treats paper chunks independently but uses a more sophisticated prompt, and a recently proposed method for generating peer-review feedback <ref type="bibr" target="#b19">(Liang et al., 2023)</ref>. Prompts for these methods can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-Agent Review Generation with</head><p>Basic prompt (SARG-B)</p><p>This baseline is designed to emulate a simple approach that a ChatGPT user might use to get feedback on their paper if they did not want to do any prompt tuning. We use a single agent to generate all comments for the paper. The paper is split into the same chunks as for the multi-agent baseline, but the chunks are processed independently using a very simple prompt:</p><p>Write feedback comments in the style of a scientific paper review for the following portion of a scientific paper . You can skip minor grammar comments .</p><p>After applying the model to each chunk, the resulting comment lists are combined by a similarly simple prompt:</p><p>Here are some lists of review comments that were made about different portions of the paper : &lt; comment lists &gt; Merge these lists into a final list of review comments . Any comments that are duplicates ( saying essentially the same thing as other comments ) should be merged or deleted .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Single-Agent Review Generation with</head><p>Tuned Prompt (SARG-TP)</p><p>This baseline is designed to emulate a more sophisticated approach that a ChatGPT user might use to get feedback on their paper if they were willing to do some prompt tuning. We use a single agent to generate all comments for the paper, but we use a more sophisticated prompt (subsection A.3) that is designed to encourage the model to generate more specific and actionable comments. As with the other simple baseline, we generate comments independently for each paper chunk and then merge the resulting lists with GPT. Similarly to our multi-agent method, we include a refinement step in this baseline. For each paper chunk, we give the model the chunk and the final list of comments, and ask it to output a new, refined list of comments. This provides an opportunity to remove incorrect comments that arise from the independent processing of each chunk. For example, if one chunk contains the introduction but not the experiments, the model might initially write a comment that claims the experiments are missing, but in the refinement stage will be able to prune it when it sees the chunk that does contain experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-Agent Review Generation with</head><p>Tuned Prompt (MARG-TP)</p><p>This baseline is designed to provide a direct comparison with the prompt-tuned single-agent baseline and explore the benefits of multi-agent modeling. Whereas our full MARG-S approach leverages several advantages of multi-agent that would be difficult to directly compare in a single-agent setting (e.g., the use of expert agents), this multiagent baseline uses a prompt designed to be as similar as possible to the prompt-tuned single-agent baseline. Of course, we still must include some instructions that explain the communication protocol and instruct the agents to work together, but the task prompt includes all the same language as in the single-agent setting. Similarly, we use a refinement prompt that is as similar as possible to the single-agent setting, although the refinement stage still differs in that we do not manually apply it on each chunk (as this would defeat the point of using multiple agents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi-Agent Review Generation with Specialized Agents (MARG-S)</head><p>Our full MARG-S approach is described in section 4, and uses three independent multi-agent groups to generate different kinds of review comments. MARG-S outputs the concatenation of the three mini-reviews generated by those groups. In addition to the full approach, we evaluate each of the three mini-reviews separately. We refer to these as MARG-S (experiments), MARG-S (clarity), and MARG-S (impact). In addition, we include a "no refinement" baseline that skips the refinement stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Liang et al. (2023) baseline (LiZCa)</head><p>We also compare against a recently proposed method for generating peer-review feedback <ref type="bibr" target="#b19">(Liang et al., 2023)</ref>, which we refer to as "LiZCa" (from the names of the lead authors of that paper; the method was not given a name in that work). Unlike our methods, this method simply truncates the paper rather than applying to multiple chunks. In addition, it includes the captions of figures and tables in the input. The prompt used in <ref type="bibr" target="#b19">Liang et al. (2023)</ref> instructs the model to generate an "outline" style review, and includes non-actionable positive comments. Fortunately, when comparing their method's comments with real reviews, they developed a prompt to extract and merge the parts of an outline that focus on "criticisms" and to ignore minor grammar comments. This roughly matches the type of comments we target, so we use that prompt to produce the final list of comments that we use in this baseline.</p><p>We note that <ref type="bibr" target="#b19">Liang et al. (2023)</ref> used a different PDF parsing library (pikepdf) than ours (Grobid), but for consistency with our other baselines we run it with Grobid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Automated evaluation</head><p>To automatically evaluate the quality of generated reviews, we measure their overlap with real reviews from papers in the ARIES corpus <ref type="bibr" target="#b8">(D'Arcy et al., 2023)</ref>. That is, we attempt to match the generated comments to comments extracted from real (human-written) reviews. Because ARIES only has comment annotations for a small set of reviews, we use GPT<ref type="foot" target="#foot_1">4</ref> to extract comments from all reviews for a subset of 30 papers and treat this as our test set. To match our intended type of feedback, GPT is instructed to focus only on actionable feedback comments and to ignore minor comments on style and grammar.</p><p>We note that this form of evaluation is imperfect in that real reviewers do not always identify every reasonable critique of a paper, and in some cases they may make critiques that are unreasonable. Thus, the generated review could contain good comments that happen to be different from ones the real reviewers made, or it could miss comments that are actually invalid. Thus, the measured overlap should be treated as a lower bound for the fraction of good-quality comments. In addition, the nuanced nature of the matching task makes it impossible to fully capture the similarities and differences between real and generated comments using binary alignments, and this could lead to biases. We nonetheless use automated evaluation as an inexpensive but rough approximation of the rel-ative quality of different methods, and separately conduct a user study in section 7 to obtain a more realistic evaluation.</p><p>The matching procedure and results are outlined in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Measuring overlap</head><p>Given a set of generated review comments C gen and the set of ground truth real-reviewer comments C real for a paper, we automatically align individual comments between the reviews that have the same meaning. That is, we ultimately obtain a binary label for every comment pair (C i gen , C j real ) indicating whether the two comments are making the same request. To do this, we begin with a "manymany" matching stage that efficiently compares the full set of comments in both reviews and identifies possibly-matching pairs, followed by a more accurate (but more expensive) pairwise stage that examines the candidate pairs to produce a final list.</p><p>In the many-many matching stage, we feed all comments from both reviews into GPT-4 and prompt it to output a list of all matching comments. As GPT has somewhat inconsistent performance, we do five such passes, randomly permuting both the order of comments within each review and the order in which reviews are presented. The final output of this stage is the list of comment pairs that were produced by at least two of the five runs-a ratio we heuristically found to work well in preliminary experiments.</p><p>In the pairwise stage, we give one comment pair at a time to GPT and prompt it to produce two scores: one of four levels of relatedness ("none", "weak", "medium", or "high"), and a "relative specificity" ("less", "same", "more") indicating how specific the generated comment is relative to the real review comment. To be considered a match, a comment pair must have "medium" or "high" relatedness, and the generated comment must have "same" or "more" specificity compared to the human comment. An example of an aligned pair of comments can be found in Table <ref type="table">1</ref>.</p><p>The final output is a list of alignment edges between the lists of generated and real-reviewer comments. We note that this may result in a manymany mapping; one generated comment might match multiple reviewer comments, and one reviewer comment might match multiple generated comments. This can happen when there are similar comments within one list or if, for example, a re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-reviewer comment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated comment</head><p>The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.</p><p>More details about the experiments conducted would be beneficial. This should include information about the datasets used, the training process, and the evaluation process. To ensure the reproducibility of the results, consider providing the code used to implement the model, the specific parameters used, and any other necessary information. This will allow other researchers to replicate your work and further validate your findings.</p><p>[high relatedness, more specific] The paper does not include enough baselines for Fair Federated Learning to compare against. Even if some methods do not satisfy privacy considerations, they should still be included for the reader to understand how the proposed method compares against such methods, especially given that the results are not promising. Some baselines to consider include Cui et al or Tran et al.</p><p>The authors should consider including a comparison of their proposed method with existing methods in the experimental results section. This would help to highlight the advantages and improvements of their proposed method.</p><p>[high relatedness, less specific] The datasets used in the study are not representative due to their simplicity and experimental nature.</p><p>The evaluation of the proposed method may not be comprehensive enough. The authors could include more datasets in their evaluation to demonstrate the robustness of their method. The paper could benefit from a more detailed discussion on the limitations of the proposed method.</p><p>[medium relatedness, more specific] Table <ref type="table">1</ref>: Aligned pairs of comments with corresponding relatedness and relative specificity scores from the alignment model; the bold is added to emphasize key differences. Notice that in the third row with "medium" relatedness, the reviewer comment is suggesting that the datasets need to be more representative (but a larger number of datasets is not necessarily needed) whereas the generated comment only asks for more datasets (not identifying the issue with the current datasets). In the two "high" relatedness cases, one comment fully subsumes the other (high relatedness) but includes much more specific details and rationales (less/more relative specificity).</p><p>viewer makes a broad suggestion like "Evaluate on more datasets" and the generated review contains several comments, each with a different specific dataset recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Metrics</head><p>Using the alignments between C gen and C real , we evaluate several metrics, described below. However, we note that the many-many nature of the mapping between the comments indicates that these are not proper sets, and traditional set-based metrics such as the union and intersection are not well-defined. For our purposes, we define directional intersection operators ← ∩ and → ∩ representing the set of aligned elements in the left or right operand, respectively. For example, C gen ← ∩ C real is the set of elements of C gen that align to any element in C real .</p><p>• Recall:</p><formula xml:id="formula_0">|Cgen → ∩ C real | |C real |</formula><p>, the fraction of realreviewer comments that are aligned to any generated comment.</p><p>• Precision:</p><formula xml:id="formula_1">|Cgen ← ∩ C real | |Cgen|</formula><p>, the fraction of generated comments that are aligned to any realreviewer comment.</p><p>• (Pseudo-)Jaccard:</p><p>The Jaccard index is a commonly-used measure of set overlap.</p><p>Let intersection</p><formula xml:id="formula_2">= |Cgen ← ∩ C real |+|Cgen → ∩ C real | 2 ; then the Jaccard index is intersection |Cgen|+|C real |-intersection .</formula><p>To compute these metrics over a set of papers, we macro-average on the level of reviews. That is, given a set of papers in our test set, we generate a review for each, measure the aforementioned metrics between each generated review and each corresponding real review, and then average all of the results to obtain a single value for each metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We include a selection of example generated comments in Table <ref type="table" target="#tab_1">3</ref>. Results of the automated evaluation are shown in Table <ref type="table" target="#tab_0">2</ref>. We additionally include a human-review baseline, which is the average of the metrics computed between each real review and each other real review for the same paper (i.e., 1 n n i=1 metric(human i , {human k |k ̸ = i}). Note that while this is theoretically unbiased for recall, it may result in lower precision and Jaccard scores for human reviewers.</p><p>We find that our proposed MARG-S method outperforms all baselines in terms of recall, but generates more comments than other baselines and thus has lower precision and Jaccard scores. With that said, we believe that recall is the most important metric in this evaluation. While higher precision and Jaccard should be preferred at similar levels of recall, it is relatively easy for a human to recognize and ignore bad comments; thus, it is more important for the system to maximize the number of good comments than to minimize the number of bad ones.</p><p>The simple baseline (SARG-B) performs poorly on all metrics; despite being tied with MARG-S for the highest number of generated comments, it has the lowest recall of all methods. This is not unexpected, but highlights the importance of careful prompting with GPT-4.</p><p>Interestingly, we find that between SARG-TP and MARG-TP (which use essentially the same task prompt), SARG-TP generates more comments and has better recall. This suggests that simply applying a multi-agent approach does not always result in a performance improvement; instead, the use of multiple agents enables the design of richer internal problem-solving structures via expert agents. Indeed, we see that the specialized MARG-S (impact) is able to approximately match the performance of MARG-TP despite focusing on only one type of comment.</p><p>We notice that the human baseline actually has a lower recall than some of the LLM baselines, although it has the highest precision. This is consistent with the results of <ref type="bibr" target="#b19">Liang et al. (2023)</ref>, which found that Human-Human agreement was slightly lower than LiZCa-Human agreement. <ref type="foot" target="#foot_2">5</ref> Humans generate fewer comments than other approaches, which offers a partial explanation for the low recall, but it is nonetheless interesting to observe that human reviewers can have very different perspectives of the same work, and highlights the challenge of the review generation task (and the potential weaknesses of alignment-based evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S ablations:</head><p>Among the sub-reviewers of MARG-S, the impactfocused model tends to produce the best results. The experiment-focused model does well considering the small number of comments it produces, but as it produces half as many comments as the impact model it also has half the recall. Finally, the clarity-focused model struggles compared to the other two. The poor performance of the clarity model may be due in part to the subjective nature of clarity judgements and the fact that language models do not necessarily perceive text in the same way that humans do (e.g., humans prefer that terms be defined before they are used, but a model that consumes a full document at once might not see a problem if terms are defined later). In addition, we note that due to the fact that the input does not capture visual information such as figures, tables, and the arrangement of symbols in equations, there are many resulting clarity issues that are not present in the full paper, and getting the model to identify the "real" issues from among the large number of parsing-and input-format-related issues is challenging. We observe that without the refinement stage, MARG-S's performance is reduced on all metrics, but it still obtains reasonable results; recall remains the second-highest of all methods. Interestingly, the number of generated comments is slightly lower than with the refinement stage, indicating that the refinement stage splits one comment into multiple comments more often than it prunes comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the matching thresholds</head><p>We qualitatively observe that several of the methods we evaluate produce many generic or vague comments. Many of these are not counted towards recall due to our constraint that a generated comment must be equally or more specific compared to the real comment it matches with. In addition, some aligned pairs of comments are questionable, especially for pairs that have only "medium" relatedness; for example, a comment asking for a "more thorough comparison" to baselines is considered a match for one that indicates that the proposed method underperforms the baselines in some cases.</p><p>To evaluate the impact of threshold choices, we select our method and the LiZCa baseline and evaluate all combinations of thresholds for relative specificity and "medium" or "high" relatedness. The results are shown in Figure <ref type="figure">3</ref>.</p><p>The difference between thresholds is striking in the figure. LiZCa generates a large number of comments that broadly match to a real comment, but are much more vague (and thus less useful) and often do not have exactly the same meaning. When either the relatedness or the specificity thresholds are increased even by one step, the recall drops sharply. In contrast, MARG-S loses relatively little recall at higher specificity thresholds, as it almost always outputs specific comments. However, MARG-S still has a drop when requiring "high" relatedness, indicating that it has many matches in which the generated comment does not express exactly the same sentiment as the real one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost</head><p>In Table <ref type="table" target="#tab_3">4</ref> we report the average number of tokens generated by each method during the alignment-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Example comment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARG-B</head><p>The paper could benefit from a more detailed discussion of the results, including the implications of the findings and how they contribute to the existing body of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiZCa</head><p>The experimental evaluation could be more comprehensive. The authors should consider including more diverse tasks and environments in their experiments to demonstrate the robustness of their method. The paper could benefit from a more detailed analysis of the experimental results, including a discussion on why the proposed method outperforms the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (experiments)</head><p>The authors have compared their method with several baselines, including DeepMDP, HiP-BMDP-nobisim, Distral, PCGrad, GradNorm, and PEARL. However, it would be beneficial to include comparisons with other state-of-the-art methods in multi-task and Meta-RL setups to further validate the effectiveness of the proposed method. This would help ensure that the results are not specific to the current set of comparisons and can generalize across different settings. Additionally, providing a detailed discussion on why the proposed method outperforms each baseline could offer more insights into the strengths and weaknesses of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (clarity)</head><p>The paper mentions an encoder that maps observations from state space to a learned, latent representation, but it does not provide specific details about the type of encoder used or the process of how it learns the latent representation. These details are crucial for understanding how the model works and how it achieves its performance. Therefore, I recommend that the authors include this information in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (impact)</head><p>The authors provide a theoretical proof for the 'Transfer bound' formula, which is a significant contribution. This formula is crucial for measuring the transferability of a policy learned on one task to another, taking into account the error from the learned representation. However, to ensure its robustness and applicability in real-world scenarios, it would be beneficial if the authors could empirically test this formula in reinforcement learning environments. For instance, the agent could be trained on one game and then tested on a different game with similar mechanics. This would provide empirical evidence supporting the theoretical proof and demonstrate the practical utility of the formula. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S Recall</head><p>Figure <ref type="figure">3</ref>: Recall of MARG-S and LiZCa for different alignment cutoff levels of relatedness and relative specificity. The ("medium", "same") cell corresponds to our default setting. LiZCa obtains very high recall in the most lenient setting, but rapidly drops for stricter settings that prevent vague comments from being counted as matches. MARG-S obtains relatively consistent results for all levels of specificity (as most of its comments are considered "more" specific) but still experiences a decline when requiring highly-related matches.</p><p>based evaluation. LiZCa generates the fewest tokens and has the best cost to recall ratio overall, making it an attractive choice in budget-constrained settings. While MARG-S has the best recall, it also generates roughly an order of magnitude more tokens than other methods, suggesting that it takes on diminishing returns in efficiency to obtain the recall improvement.</p><p>The extra tokens used by MARG-S result in it taking roughly an hour longer than other methods to generate reviews. This may serve as an inconvenience in practice, and it would be beneficial to explore ways to reduce it. For example, it may be possible to dynamically switch to cheaper LLMs to handle simpler messages or develop methods to route communications more effectively (reducing the number of redundant messages). We also note that our implementation performs only one inference at a time for simplicity, but in theory, it is highly parallelizable (due to having three sepa-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">User study</head><p>We conduct a user study to obtain a more reliable (but more expensive) evaluation compared to the automated metrics. To reduce burden on participants, we only evaluate a subset of methods in the user study: MARG-S (our best method on the automated metrics), LiZCa (baseline from prior work), and SARG-B (the simplest baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Study design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruit 9 volunteers<ref type="foot" target="#foot_3">6</ref> from a large research organization to participate in the study. All participants are researchers in the fields of natural language processing and human-computer interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Survey</head><p>The study was conducted using a web interface in which participants could upload a paper PDF. We then ran each review generation method to produce a set of reviews, where each review was a list of comments. When all reviews were generated, participants would receive an email notification with a link to page with reviews and a set of survey .</p><p>. . questions, depicted in Figure <ref type="figure" target="#fig_1">4</ref>. The survey page did not describe the review generation methods or give any indication of which method generated a given review, and the generated reviews were displayed in a random order to reduce bias (the order of comments within reviews was not randomized, however).</p><p>For each comment, participants were asked to rate its specificity, accuracy, and to provide an overall rating. The following guidelines for these ratings were provided at the start of the survey:</p><p>• Specificity: Does the comment make a suggestion specific to the paper, or is it generic (could apply to many papers)? Please note that a comment may be verbose without being specific, or vice versa.</p><p>• Accuracy: Does the comment display an accurate understanding of the paper and make a valid critique? For example, suppose a comment says the paper is missing statistical significance tests and should include them. If the paper doesn't have significance tests and could potentially benefit from including them, please rate the comment as "accurate" (even if the importance of those tests is questionable).</p><p>If the paper has tests on one or two results but not all, and the comment doesn't mention this, the comment would have a "minor" inaccuracy. If the paper already has extensive significance tests or provides substantial justification for not including them, the comment would have a "major" inaccuracy.</p><p>• Overall rating: How helpful is the comment overall? Is the comment one that you would want to see in a review (Good), one that you might not mind seeing but don't care much about (Neutral), or one that is useless or invalid (Bad)?</p><p>In addition, participants were asked questions at the end of each review. Specifically, they were asked to rate whether the review was too long or too short on a 5-point scale and to provide an overall rating for the review on a 5-point scale.</p><p>Finally, we asked participants about their research and reviewing experience, and about their authorship of the submitted paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Total good comments</head><p>As in the alignment-based evaluation, we argue that bad comments have relatively small cost compared to the value of good comments. As there is no straightforward way to adjust the total number of generated comments (unlike in a classification task, where the decision threshold could be adjusted continuously), the total number of good comments is the most appropriate metric with which to compare methods.</p><p>Table <ref type="table">5</ref> shows the average number of each comment rating per review for each method. We find that MARG-S generates more good comments than SARG-B (p=0.09, related-sample t-test) and LiZCa (p=0.003). LiZCa generates substantially fewer comments than the other methods, and therefore has the fewest bad comments per review but also the fewest good comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review length</head><p>MARG-S generates the most good comments, but does this come at the cost of generating overly-long reviews? It seems that in general, MARG-S reviews do tend to be longer than authors would like, while LiZCa reviews are too short. Specifically, MARG-S was rated as "way too long" by 6 of the 9 participants (and "just right" by the other three), while LiZCa was rated as "too short" by 4, "way too short" by 3, and "just right" by 2 of the participants. SARG-B occupied a middle ground, rated as "too short" by 2, "too long" by 3, and "just right" by 4 of the participants. Although SARG-B generates a similar number of comments as MARG-S, the comments it generates are much shorter, which is likely why its length is perceived as being more reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Average comment ratings</head><p>The distribution of user ratings of comment quality, accuracy, and specificity are shown in Figure <ref type="figure">5</ref>, Figure <ref type="figure">6</ref>, and Figure <ref type="figure">7</ref>, respectively.</p><p>We find that MARG-S has the highest proportion of "good" comments, and is significantly better than SARG-B (p=0.02 for per-comment Barnard's exact test, p=0.12 for per-user related sample ttest), although the difference between MARG-S and LiZCa is not significant (p=0.09 per-comment, p=0.16 per-user). When asked about the overall helpfulness of the reviews, participants rated MARG-S as an average of 1.0 points higher on the 5-point scale (and 5 of the 9 participants rated MARG-S as 2-3 points higher).</p><p>The accuracy ratings in Figure <ref type="figure">6</ref> show a similar trend as the comment quality ratings. MARG-S has the highest proportion of fully accurate comments, but the differences are not significant.</p><p>The most striking difference between the methods is in specificity. MARG-S has "very specific" comments at more than triple the rate of the other two methods, a significant increase (p=0.002, peruser related-sample t-test). Overall, 71% of its comments are rated as "specific" or "very specific", compared to only 40% for LiZCa (p=0.08).</p><p>Finally, we observe that MARG-S has a high proportion of "good" comments rated by users despite having a relatively low precision in the automated evaluation (Table <ref type="table" target="#tab_0">2</ref>). The difference suggests that it may generate many comments which are helpful but also different than the kinds of suggestions a real reviewer would tend to make. This could be a promising sign indicating that MARG-S can serve as a useful source of novel inspiration for authorseven when the paper has already been reviewed by humans-and that it may be a source of inspiration for reviewers as well.<ref type="foot" target="#foot_4">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Relationships between factors</head><p>Does the high specificity of MARG-S impact the quality and accuracy ratings? In particular, more specific comments may be easier to make strong judgements about, whereas comments that are generic or vague may be hard to clearly classify; Random effect std. dev σ = 0.92</p><p>Table <ref type="table">6</ref>: Cumulative link fixed effects for specificity, accuracy, and method on the overall rating of a comment. Specificity is positively associated ratings, as is accuracy (inaccuracies have a negative effect). The review generation method has a relatively small independent effect compared to the other factors, suggesting that specificity and accuracy capture a large portion of the aspects that contribute to perceived comment quality. Random effect std. dev σ = 1.12 σ = 0.76 σ = 0.93</p><p>Table <ref type="table">7</ref>: Mixed-effects logistic regression coefficients and p-values for the effect of specificity, accuracy, and method on the probability of a comment receiving a given overall rating. Specificity is positively associated with neutral and good ratings, while major inaccuracies are strongly predictive of bad ratings. Note: there are no cases where a major inaccurate comment was rated as "good", leading to the extreme coefficient in that cell.</p><p>Inaccuracy=major this could cause MARG-S to be over-represented at both extremes of accuracy and quality compared to the other methods. To investigate this, we fit logistic regression mixed-effects models to find the effect of specificity on the classification probabilities of the overall rating and on the accuracy while controlling for the generation method. In addition, we analyze the tendency of both specificity and accuracy to result in higher ratings using a cumulative link mixed-effects model. We binarize specificity in these analyses by grouping "specific" and "very specific" judgements together as well as "generic" and "very generic" ones. The logistic regression and cumulative link models are implemented in R, using the lme4.glmer <ref type="bibr" target="#b3">(Bates et al., 2009) and</ref><ref type="bibr">ordinal.clmm (Christensen, 2015)</ref> functions, respectively. We treat the submission ID as a group variable (random effect).</p><p>Results of the logistic regression analysis are shown in Table <ref type="table">7</ref> (predicting overall rating) and Table <ref type="table">8</ref> (predicting accuracy given specificity). Surprisingly, we find that specificity has a positive association with neutral ratings, contradicting our original speculation that the high specificity of MARG-S might push ratings to extremes. Higher specificity does not appear to produce a more extreme accuracy distribution either, and instead seems to weakly correspond with higher accuracy. It is unclear why specificity would influence accuracy in this way, but we speculate on three possibilities:</p><p>• Calibration: There is evidence that humans tend to give more precise answers when they are more confident <ref type="bibr" target="#b33">(Welsh et al., 2011)</ref>. The model may mimic this tendency and write more specific comments when it has greater confidence.</p><p>• GPT-4 mode switching: GPT-4 may have an intrinsic tendency to write comments that are either good in both specificity and accuracy or bad in both. It has been rumored that GPT-4 uses a mixture-of-experts architecture,<ref type="foot" target="#foot_5">8</ref> in which case the correlated behavior may be related to expert routing.</p><p>• Human bias: Humans may have a tendency to perceive comments as more specific when they are more accurate, even if the specificity is not actually relevant to the accuracy. For example, "There is only one baseline for comparison. You should add more.", is very generic, and this is easy to see when it is inaccurate. However, if there really is only one baseline and adding more would be useful, it may be perceived as more specific because it appears to demonstrate an understanding of the paper.</p><p>The analysis in Table <ref type="table">6</ref> shows that accuracy is highly predictive of overall rating, particularly for major inaccuracies. In fact, we find that 97% of all comments with a major inaccuracy are rated as bad, as opposed to 30% for minor inaccuracies and 23% for accurate comments. Specificity plays a larger role among accurate comments; within this group, only 19% of non-specific comments were rated as good, while 57% of specific comments were. Still, specificity and accuracy are not perfect predictors of comment quality; even among comments that were rated as both fully accurate and very specific, only 59% were rated as good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Compliments and ratings</head><p>We observe qualitatively that some generated comments include compliments or flattering remarks; for example, a comment might say "While the authors have done a commendable job in [...], the paper could benefit from [...]". To test whether these compliments might bias the user ratings, we use GPT-4 to detect the presence of such remarks in all generated comments, using the following prompt:</p><p>Determine whether following review comment for a scientific paper includes a compliment or flattering remark about the paper . Output a JSON object with the key " has_compliment " set to true or false . Output only JSON with no additional commentary .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comment : { comment }</head><p>We find that 19% of MARG-S comments contain compliments, compared to 25% for SARG-B and 0% for LiZCa.</p><p>We then fit a cumulative link mixed-effect model with accuracy, specificity, method, and "has_compliment" as fixed effects and submission id as a random effect. We find that "has_compliment" has a coefficient of 0.11 (p=0.76), which is small relative to the coefficients of other factors we observed in Table <ref type="table">6</ref> and smaller than the random effect standard deviation (σ = 0.92), and we cannot reject the null hypothesis that the coefficient is 0. Thus, it does not appear that flattery causes a meaningful bias. Of course, we note that detecting compliments is somewhat subjective and can be a matter of degree, so it is still possible that there are more subtle biases in user ratings; we leave further analyses to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Failure analysis</head><p>While MARG-S does well relative to other methods, there are still a large number of comments rated as "bad", and the precision and recall in the automated evaluation are still rather low in absolute terms. In this section, we qualitatively analyze the conversation message logs of the multi-agent system and identify several common classes of errors in the communication. The analysis was carried out by an author of this work with several publications in the field of machine learning and natural language processing, and the papers being analyzed were broadly related to the topic of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Scope</head><p>There are two main stages of the multi-agent system: (1) the "main" stage, in which the model comes up with a list of comments, and (2) the refinement stage, in which the comments from the main stage are refined and potentially pruned if they are redundant. For 10 papers from the automated evaluation, we analyze the main stage for all three sub-reviewers (experiments, clarity, impact), for a total of 30 conversations. We additionally analyze the refinement stage for one randomly-selected comment from each of the 30 papers in the automated evaluation test set.</p><p>Checking each message against the paper for factual inconsistencies is expensive and error-prone, especially given the number of claims and comments that can be generated in the main stage, so for the main stage we only consider errors that are apparent from the conversations themselves. For the refinement stage, we do refer to the paper to check whether the models missed basic facts; however, it is important to note that only a limited amount of time (approximately 5-15 minutes) was spent to check comments against each paper, and due to the highly technical nature of these works it is possible that some factual errors were overlooked. Nonetheless, the fraction of invalid comments identified in this analysis is similar to the fraction of bad-rated comments found in the user study, so we believe the findings are reasonably accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Main stage</head><p>Below, we describe the error categories we identified for the main stage, along with the percentage of conversations that contain the error type. If the same error type appears multiple times in the same message log, we only count it once. It is worth noting that not all errors ultimately result in erroneous comments, as it is possible for agents to point out each others' errors and address them.</p><p>Overall, 70% of conversations contain at least one of these error types:</p><p>• Missing context (MC) (53%): The leader agent fails to include key context in a message to another agent. In general, this tends to happen when it messages an expert agent and fails to include some information about the paper that the expert needs to proceed.</p><p>• Missing context -misplaced SEND MESSAGE (MC-MSM) (47%): A subtype of MC, this error occurs when the leader agent does include the necessary context in its generated output, but places the SEND MESSAGE marker after it instead of before.</p><p>• Fails to Identify Error (FIE) (17%): When the leader makes one of the aforementioned errors, worker or expert agents should point this out and ask the leader to try again, but they sometimes fail to do this.</p><p>• Ignores Relevant Information (IRI) (10%):</p><p>An agent ignores part of a message that it should have responded to.</p><p>• Failure to Respond (FR) (7%): An agent does not recognize a message as being relevant and gives an empty or vapid response.</p><p>• Skipping Steps (SS) (7%): The leader moves to a later step too early. For example, writing the final review comments before the expert's questions are resolved, or skipping the initial step where it is supposed to get a summary of the paper.</p><p>• Message loop (LOOP) (7%): The agents enter a loop of similar messages, triggering the duplicate-message detector described in section 4.2.</p><p>• Exceeds input token limit (EITL) (7%):</p><p>The conversation exceeds the input token limit for the underlying model. These cases occur when the expert asks too many questions, which can happen when the expert repeatedly asks for slightly more details each time it gets an answer to a question.</p><p>Qualitatively, we noticed that there is a very common pattern for missing-context errors. Specifically, when the leader first addresses the expert, it tries to include a summary of the paper to give context for the expert, but it misplaces the SEND MESSAGE indicator.</p><p>This error occurs in 33% of conversations, but in 80% of those cases the expert points out the error and the leader corrects it. Interestingly, in many instances of the error, the leader tries to use a placeholder ("[insert summary here]") despite never being instructed to do so; for example (magenta text verbatim, black text is edited):</p><p>Agent 0 (leader): Summary: &lt;omitted for brevity&gt;</p><p>Step 3: Share the summary with Agent 3 and ask for their input, specifically focusing on potential shortcomings of the paper's assumptions.</p><p>SEND MESSAGE: Agent 3, here is a summary of the paper: [insert summary here].</p><p>Could you please provide your input on potential shortcomings of the paper's assumptions? Also interesting is the fact that in all cases when the leader fails to include the summary, there are no additional missing-context errors in the remainder of the message log. We conjecture that the early failure (and the following correction) may serve as a form of one-shot example that encourages the model to avoid such errors later in the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Refinement stage</head><p>Because the refinement stage works with one comment at a time, we do check the comment against the paper to determine if it is relevant. While this is somewhat subjective, we attempt to give the model the benefit of the doubt; if the comment is factually consistent and does raise a potentially valid suggestion (even if minor or difficult to address), we consider the comment as valid in the sense that it is fine for the system not to prune it.</p><p>Below, we describe the error categories we identified for the refinement stage, along with the percentage of conversations that contain the error type. As with the main stage, if the same error type appears multiple times in the same conversation, we only count it once.</p><p>• Failure to prune a comment (47%): The system fails to prune a comment that is invalid. This can happen for several reasons:</p><p>-Ignored information (17%): The comment is already addressed in the parsed paper text or contradicts information in the text, but the model did not recognize it. -Unavailable information (13%): The comment is already addressed in the paper or contradicts information in the paper, but that information is not available in the parsed text (either due to parsing errors, or because it is in a figure or table). -Irrelevant (17%): The comment asks for something that is trivial or does not make sense in the context of the paper (e.g., requesting an experiment to confirm a claim that the paper does not make or that is trivially true by definition).</p><p>• Revising instead of pruning (30%): The original comment for refinement is invalid, but the system revises it rather than pruning it. This is not always a failure, as the revision may result in a good comment, but there are also many cases where the revised comment has questionable relevance.</p><p>• Failure to respond (17%): An agent does not recognize a message as being relevant and gives an empty or vapid response.</p><p>• Message loop (10%): The agents enter a loop of similar messages, triggering the duplicatemessage detector described in section 4.2.</p><p>• Pruned (3%): The system marks the comment as invalid and prunes it. This only occurred for one comment in our sample, and the decision to prune was correct in that case.</p><p>Our analysis finds that the system fails to prune invalid comments in 47% of cases, which is close to the 53% of comments that were rated as bad in the user study.<ref type="foot" target="#foot_6">9</ref> Most of these cases appear to occur because the system lacks some information from the paper, either because the information isn't extracted by the PDF parser or because the model simply ignores it. The relatively high fraction of cases where models ignore information is quite interesting and suggests that GPT-4 can still fail in very basic ways, despite the strong results that have been seen in other studies of its reasoning ability. Our result is supported by some recent work, however, which suggests that large models do not always use their full input effectively, and have a particularly hard time attending to tokens near the middle of the input <ref type="bibr">(Liu et al., 2023)</ref>.</p><p>Another common trend we find is that even when presented with information that would render the original comment invalid (e.g., the paper already covers it), the GPT-4 leader agent strongly prefers to revise the comment to extend the request rather than pruning the comment. For example, consider the following comment: "The paper does not provide specific details about the experiment setup using the MiniImagenet benchmark. The authors should provide more detail about this setup, including the specific settings and how the results were measured."</p><p>The paper actually does include details, which the leader agent summarizes:</p><p>Agent 0 (leader): Summary of findings: -Agent 1 and Agent 2 did not find any information about the MiniImagenet benchmark in their parts of the paper.</p><p>-Agent 3 found a section that discusses the setup for the MiniImagenet benchmark, including details about data efficiency and computational efficiency, and the tuning of hyper-parameters.</p><p>-Agent 4 did not find any information about the MiniImagenet benchmark in their part of the paper.</p><p>-Agent 5 found a section titled "D.2 FEW-SHOT MINIIMAGENET" which provides details about the experiment setup using the MiniImagenet benchmark, including the structure of the dataset, the protocol for meta-training, and the comparison of the BMG meta-objective against the MG meta-objective.</p><p>-Agent 6 found information about the model However, while it recognizes the original comment as invalid, it refines the comment to ask for the information to be presented more clearly instead of pruning it:</p><p>Agent 0 (leader): ... The comment is not valid and should be removed. However, it may be helpful to suggest that the authors consolidate the information about the MiniImagenet benchmark into one section for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revised comment:</head><p>"The paper provides detailed information about the experiment setup using the MiniImagenet benchmark, including the specific settings and how the results were measured. However, this information is spread across different sections of the paper, which may make it difficult for readers to find and understand. The authors could improve the paper by consolidating this information into one section."</p><p>In this case, the revised comment is a valid comment. However, this is not always the case; the leader often doesn't check the validity of the new revised comment with the other agents, so if the refinement introduces an invalid request it typically will not catch the error. It may be possible to improve accuracy by repeating the refinement stage several times, although it would be expensive to do so for every comment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we have introduced MARG, a novel method for review generation, which uses a network of LLM agents that communicate to share information across different parts of a paper and to engage in internal discussion to write better comments. We evaluated our best variant (MARG-S) against both our own simple baselines and a contemporaneously-published GPT-4 baseline and found that MARG-S produces more good comments in both an alignment-based evaluation and a user study. The user study found that MARG-S is especially strong in terms of specificity and tends to generate very detailed comments compared to other methods. However, a majority of comments across all methods (including MARG-S) are rated as bad, and 38-48% are rated as highly inaccurate, suggesting that substantial work is still needed.</p><p>MARG-S is substantially more expensive to use compared to other methods (in terms of both time and API cost), and exploring ways to reduce this, such as dynamically switching to faster and cheaper models for simpler parts of the task, could be a promising avenue for future work. In addition, future work could extend the method to incorporate background literature, which would enable more informed critiques of related work and baseline choices. Finally, while splitting the paper into chunks allows MARG-S to consume papers beyond the base model's input size limits, it is still limited in that very large inputs can result in a large number of messages on each round of communication (one per chunk) which overflow the input; it would be interesting to explore ways to compress or prune messages to further increase the system's effective input capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Prompts</head><p>Our experiments use OpenAI's gpt-4-0613 model. For our experiments, we split the paper into chunks of 4096 tokens given to the worker agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MARG-S MARG-S: Leader agent system prompt</head><p>You are part of a group that needs to perform tasks that involve a scientific paper . However , the paper is very long , so each agent has only been given part of it . You are the leader in charge of interacting with the user and coordinating the group to accomplish tasks . You will need to collaborate with other agents by asking questions or giving instructions , as they are the ones who have the paper text .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication protocol :</head><p>To broadcast a message other agents , write " SEND MESSAGE : " and then your message ; alternatively , if you forget to include it until the end of your message , you can write " SEND FULL MESSAGE " and everything you just wrote will be sent . This will be a common failure , so if other agents remark that you didn ' t include some information , check that you used the right version of SEND MESSAGE , and consider using SEND FULL MESSAGE instead .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional instructions :</head><p>When you are given a task , your first step should be to draft a high -level plan with a list of steps , concisely describing how you will approach the task and your strategy for communicating with other agents . Then , execute the plan . When executing the plan , write the current step you are working on each time you move to the next step , to remind yourself where you are . You are allowed to create a sub -plan for a step if it is complicated to do in one pass .</p><p>You should continue to pay attention to details in the original task instructions even after you draft your plan . Optionally , it may be helpful to share a plan with other agents to help guide them in some cases .</p><p>Other agents do not know anything about the task being performed , so it is your responsibility to convey any information about the task that is necessary for them to provide helpful responses . You should make this part of your high -level plan . Depending on the task , you may need to do multiple rounds of communication to exchange all the necessary information ; you should follow up with other agents if they provide a bad response or seem to have misunderstood the task . In addition , because other agents can only communicate with you but not each other , you may need to help relay information between agents .</p><p>Because each agent has a different piece of the paper , communication is key for performing tasks that require understanding the full paper . In addition , depending on the responses you receive , you may need to ask follow -up questions , clarify your requests , or engage in additional discussion to fully reason about the task . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S: Worker system prompt</head><p>You are part of a group that needs to perform tasks that involve a scientific paper . However , the paper is very long , so each agent has only been given part of it . The leader of the group is Agent 0, who will coordinate with the user and convey questions or task instructions to you .</p><p>Sometimes you will need more information in order to understand a question or task or to interpret your portion of the paper ; in these cases , you should send a message to request this information from other agents . For example , if there are key terms that you don ' t know the definitions for or parts of the paper chunk that you are missing important context for , you might need to ask for more information in order to understand it . <ref type="bibr">In</ref>  Because the leader always broadcasts messages to all agents , you might sometimes get messages that aren ' t relevant to you ; in this case , just respond with " This doesn 't seem relevant to me , so I will stand by for further instructions .". However , if the message contains information that contradicts information in your part of the paper , you should respond and mention the issue , even if the message wasn ' t directed at you . In addition , you should be aware that sometimes the leader accidentally leaves some information out from its messages , so if a message looks like it might be directed at you but is simply incomplete , you should ask follow -up questions to confirm . Write " Ready " if you have understood the assignment .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S: Worker chunk prompt</head><p>You will then receive messages .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (experiments): Leader task prompt</head><p>Task : Write a list of feedback comments , similar to the suggestions a reviewer might make . In addition , focus on major comments rather than minor comments ; major comments are important things that affect the overall impact of the paper , whereas minor comments are small things like style / grammar or small details that don 't matter much for whether the paper should be accepted to a venue .</p><p>Be specific in your suggestions , including details about method or resource names and any particular steps the authors should follow . However , don ' t suggest things that have already been included or addressed in the paper . Remember that you can collaborate if necessary , but also remember that other agents can ' t see anything you write prior to " SEND MESSAGE " , so you may need to repeat information so that they are aware of it . For example , if you write some comments and ask for additional ones , you may want to provide your original comments so that the agent knows what they are .</p><p>Your review comments should be specific and express an appropriate level of importance . For example , suppose a paper is missing some important details needed to understand a proposed method . A comment like " The authors could add more details about the proposed method , such as XYZ ." is bad because it is too generic ; even for a paper with a good method description it is always possible to add more details , so it isn ' t clear if there is actually a significant problem with the current paper . Instead , in this scenario it is much better to leave a comment like " The description of the proposed method is unclear because it is missing some key details such as XYZ .</p><p>Without these details it is hard to know whether ___ .". Make sure your high -level plan mentions this instruction . Some comments are a matter of degree . For example , maybe the paper includes one baseline but no others ; you would need to determine whether or not that is acceptable for meeting the goals of the paper and supporting its claims , and decide whether it is important enough to leave a comment about . You can discuss with other agents as needed to help determine this .</p><p>You will need to communicate with other agents to understand the paper and learn what has already been addressed and what is still missing from the paper .</p><p>The main type of feedback you should focus on the thoroughness of the experiments and consistency of claims . You should ensure that information is consistent across the paper and that claims are appropriately supported by evidence . Your highlevel plan should be roughly as follows : 1. Identify the main goals , contributions , and claims of the paper . What questions is the paper trying to answer , and why are those questions important or interesting ? What findings does it contribute to the field ? a . Go through the paper paragraph by paragraph and write down anything that looks like it might be part of the main goals or contributions , and ask other agents to do the same .</p><p>b . Put all the information together , filtering out anything that turned out to be unimportant and merging similar points . This should result in a concise list of summarized claims . 2. Identify expectations for fulfilling the goals and claims . For this part , you should collaborate closely with the experiment design expert . Give them information about the paper 's topic and the claims and goals you summarized in the previous step , and explain the task so that they can help you .</p><p>Remember to put the information after SEND MESSAGE so that it gets sent correctly . Note that other agents will see your message and may try to respond despite not being the expert ; you should make it clear that you only want to communicate with the expert , and only respond to the true expert ' s messages . During this step , you must obey all of the expert ' s instructions and answer all of their questions . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (experiments): Expert prompt</head><p>You are part of a group of agents that must perform tasks involving a scientific paper . You are an expert scientist that designs high -quality experiments , ablations , and analyses for scientific papers . When the leader sends a message to you to ask for assistance in coming up with experiments to include in a paper or judging the quality of experiments that are in a paper , you should help .</p><p>You should ensure that you fully understand the claims and goals of the paper before giving suggestions . You can send messages back to the leader to ask questions about the paper 's claims , goals , methods , and so on . It is crucial to understand what the paper is attempting to investigate in order to design experiments to support the investigation . Obtain any information you need in order to design good experiments , and ask follow up questions if needed .</p><p>Be detailed and specific in the experimental suggestions you give . What should the setup be ? What settings or methods should be compared ? What metrics or measurement techniques should be used ? How should the results be analyzed ? Make it clear which specific details are important and why (e.g., particular choices of settings , baselines , metrics , environments , procedures , and so on ) , and which details are unimportant .</p><p>If you are asked to check the quality of an existing experimental procedure , one useful approach is to come up with how you would have conducted the experiments and compare the given approach to that in order to generate potential areas for improvement . If you find a shortcoming , explain the issue clearly : why is the existing experiment misleading or why does it fail to fulfill the goals of the investigation ?</p><p>Finally , note that you may receive messages from the group leader that are not relevant to you . This is because the group leader always broadcasts all messages to all agents . If you get an irrelevant message , simply respond by saying "I do not believe the request is relevant to me , as I do not have a paper chunk . I will stand by for further instructions .".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (impact): Leader task prompt</head><p>Task : Write a list of feedback comments , similar to the suggestions a reviewer might make . The main type of feedback you should focus on is the novelty and significance of the work . The motivations , goals , and key findings of the paper need to be clearly explained , and the paper needs to explain how it fits into the related literature in the field and how it builds and expands on this work in a meaningful way . If any of those things are unclear or missing from the paper , you should comment on them .</p><p>Once you have established what the motivations , goals , and key findings of the paper are , you should carefully scrutinize whether they are reasonable and well -justified or if they need to be improved . For example , if a paper proposes a new method that is motivated by real -world use cases , but requires unrealistic assumptions to operate , the paper needs to justify that somehow .</p><p>Important : { expert_1 } doesn ' t have a paper chunk , but they are good at coming up with questions and potential shortcomings of the paper 's assumptions . Explain the paper to { expert_1 } and answer any questions they have until they say they are finished . You will likely need to pass their questions and comments along to the other agents that have the paper , and pass the answers back to the expert . Write feedback based on any points { expert_1 } indicates are in need of improvement .</p><p>Think carefully in a logical , step -by -step way . Ask questions or give instructions to other agents to help you accomplish the task , including follow -up questions or requests as needed . Write potential feedback comments as you come up with them so that you can keep them in mind ; you can always remove or revise them later for the final list .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (impact): Expert prompt</head><p>You are part of a group of agents working with a scientific paper . You are highly curious and skeptical of papers , and your job is to help ensure that the paper has clearly explained its motivations , goals , and key findings and determine whether the paper actually makes a significant contribution to its field . The group leader will give you a summary of the paper , and you should ask questions to fully understand the paper ' s motivations , goals , and key findings . This includes asking follow -up questions as needed .</p><p>Scrutinize the paper heavily , identifying any hidden assumptions or potential issues that could undermine the paper ' s claimed goals and motivations .</p><p>For example , suppose a paper proposes a robot navigation algorithm that implicitly works only with omnidirectional instantly -accelerating robots ; a questionable hidden assumption in this case would be that real -world robots can effectively be treated as omnidirectional , which is often untrue . It would be important for the authors to provide some kind of justification for the assumption in this case ( for example , that there exist robots that can turn in place and accelerate quickly enough to be treated as omnidirectional in practice ). Keep in mind that the issues might not be so obvious in practice , so you should think carefully and explore multiple perspectives and possibilities .</p><p>Think of the kinds of questions a scientific paper reviewer might ask , or what they might suggest is confusing or poorly justified in the paper .</p><p>Always make sure that you understand the terms and concepts used in the paper . If you are unsure about the definition of a term or how it is meant to be interpreted in a particular context , you should ask about it , as it is important for the paper to explain such things .</p><p>You will communicate with the group leader , who in turn will handle communications with other agents who have the paper itself . Because the leader always broadcasts messages to all agents , you might sometimes get messages that aren 't relevant to you ; in this case , just respond with " This doesn ' t seem relevant to me , so I will stand by for further instructions .". However , if you have asked questions and it doesn 't seem like the leader is responding or trying to get information from other agents so that it can respond to you , you should interject and tell the leader that they need to answer you .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (clarity): Leader task prompt</head><p>Task : Write a list of feedback comments , similar to the suggestions a reviewer might make . The main type of feedback you should focus on is the clarity and reproducibility of the work . The methods , experimental settings , and key concepts of the paper need to be clearly explained , and the paper needs to provide enough context and background information for anyone with general experience in the field to understand it . If any of those things are unclear or missing from the paper , you should comment on them .</p><p>Once you have established what the methods , experiments , and key concepts of the paper are , you should carefully scrutinize whether they are clearly explained and detailed or if they need to be improved .</p><p>Important : { expert_1 } doesn ' t have a paper chunk , but they are good at coming up with questions that test the paper 's clarity . Explain the paper to { expert_1 } and answer any questions they have until they say they are finished . You will likely need to pass their questions and comments along to the other agents that have the paper , and pass the answers back to the expert . Write feedback based on any points { expert_1 } indicates are in need of improvement .</p><p>Think carefully in a logical , step -by -step way . Ask questions or give instructions to other agents to help you accomplish the task , including follow -up questions or requests as needed . Write potential feedback comments as you come up with them so that you can keep them in mind ; you can always remove or revise them later for the final list .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S (clarity): Expert prompt</head><p>You are part of a group of agents working with a scientific paper . You are highly curious and have incredible attention to detail , and your job is to help ensure that the paper has clearly explained its methods , experimental settings , and key concepts and determine whether the paper is well -organized and can be easily understood and reproduced . The group leader will give you a summary of the paper , and you should ask questions to fully understand the paper 's methods , experimental settings , and key concepts . This includes asking follow -up questions as needed .</p><p>Scrutinize the paper heavily , identifying any missing details or potential issues that could make it ambiguous or hard to understand . Keep in mind that the issues might not be so obvious in practice , so you should think carefully and explore multiple perspectives and possibilities . In particular , make sure the paper provides all information necessary to implement any proposed methods , including any information on any background concepts needed to understand how the methods work . Also ensure that the paper provides enough information to replicate the experimental settings , including any hyperparameters , equipment and material specifications , or other implementation details .</p><p>Think of the kinds of questions a scientific paper reviewer might ask , or what they might suggest is confusing or poorly explained in the paper .</p><p>Always make sure that you understand the terms and concepts used in the paper . If you are unsure about the definition of a term or how it is meant to be interpreted in a particular context , you should ask about it , as it is important for the paper to explain such things .</p><p>You will communicate with the group leader , who in turn will handle communications with other agents who have the paper itself . Because the leader always broadcasts messages to all agents , you might sometimes get messages that aren 't relevant to you ; in this case , just respond with " This doesn ' t seem relevant to me , so I will stand by for further instructions .". However , if you have asked questions and it doesn 't seem like the leader is responding or trying to get information from other agents so that it can respond to you , you should interject and tell the leader that they need to answer you .</p><p>When you are done talking with the group leader , tell them that you are done with your review , and give them a summary list of any missing or misleading information , ambiguous statements , poorly organized points , or other suggestions that you identified .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-S: Refinement prompt</head><p>Refine and improve the following review comment that was written about a scientific paper . The goal is for the comment to be detailed and helpful , similar to a comment that a scientific paper reviewer might write . The comment should not ask for things that are already in the paper , it should include enough detail for an author to know clearly how to improve their paper , the purpose and value of the suggestion should be clearly justified , and so on . Remove the comment if it is bad (i.e., if it fails to meet those criteria ). You may need to incorporate additional information in the paper to refine the comment . You should focus on " major " comments that are important and have a significant impact on the paper ' s quality , as opposed to minor comments about things like writing style or grammar . If the comment you are given is minor , express this fact as part of the revised comment .</p><p>Your revised review comment should be specific and express an appropriate level of importance . For example , suppose a paper is missing some important details needed to understand a proposed method . A comment like " The authors could add more details about the proposed method , such as XYZ ." is bad because it is too generic ; even for a paper with a good method description it is always possible to add more details , so it isn ' t clear if there is actually a significant problem with the current paper . Instead , in this scenario it is much better to leave a comment like " The description of the proposed method is unclear because it is missing some key details such as XYZ . Without these details it is hard to know whether ___ .". Make sure your high -level plan references this instruction .</p><p>Note that only you are being given the comment ; you will need to share it with other agents if you want them to have context . When receiving responses , it may be helpful to first summarize the findings from all agents before applying the information to the review comment . Some comments are a matter of degree . For example , maybe the paper includes one baseline but no others ; you would need to determine whether or not that is acceptable for meeting the goals of the paper and supporting its claims , and decide whether it is important enough to leave a comment about . You can discuss with other agents as needed to help determine this .</p><p>It may be helpful to work step -by -step examining one aspect of the comment at a time and considering what information is needed to verify that it is valid and important as well as what kind of clarification and rewording could help to make it clearer and more specific .</p><p>Here is the comment : { review_comments }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SARG-B SARG-B: System prompt</head><p>You are ReviewGPT , an expert scientific paper reviewer .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARG-B: Task prompt</head><p>Write feedback comments in the style of a scientific paper review for the following portion of a scientific paper . You can skip minor grammar comments .</p><p>---START PAPER CHUNK ---{ paper_chunk } ---END PAPER CHUNK ---</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SARG-TP SARG-TP: System prompt</head><p>You need to perform tasks that involve a scientific paper . When you are given a task , your first step should be to draft a high -level plan , concisely describing how you will approach the task . Then execute that plan .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARG-TP: Chunk prompt</head><p>A chunk of text from a scientific paper is shown below : ---START PAPER CHUNK ---{ paper_chunk } ---END PAPER CHUNK ---Write " Ready " if you have understood the assignment .</p><p>You will then be given tasks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SARG-TP: Task prompt</head><p>Task : Write a list of feedback comments , similar to the suggestions a reviewer might make . Focus on major comments rather than minor comments ; major comments are important things that affect the overall impact of the paper , whereas minor comments are small things like style / grammar or small details that don ' t matter much for whether the paper should be accepted to a venue .</p><p>Be specific in your suggestions , including details about method or resource names and any particular steps the authors should follow . However , don ' t suggest things that have already been included or addressed in the paper .</p><p>Your review comments should have a clear purpose ; obviously , it is always possible to simply say the authors should include more details or do more experiments , but in practice the authors have limited space to write and limited time to work , so each comment needs to have a clear purpose .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 MARG-TP MARG-TP: Leader system prompt</head><p>You are part of a group that needs to perform tasks that involve a scientific paper . However , the paper is very long , so each agent has only been given part of it . You are the leader in charge of interacting with the user and coordinating the group to accomplish tasks . You will need to collaborate with other agents by asking questions or giving instructions , as they are the ones who have the paper text .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication protocol :</head><p>To broadcast a message other agents , write " SEND MESSAGE : " and then your message ; alternatively , if you forget to include it until the end of your message , you can write " SEND FULL MESSAGE " and everything you just wrote will be sent . This will be a common failure , so if other agents remark that you didn ' t include some information , check that you used the right version of SEND MESSAGE , and consider using SEND FULL MESSAGE instead .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional instructions :</head><p>When you are given a task , your first step should be to draft a high -level plan with a list of steps , concisely describing how you will approach the task and your strategy for communicating with other agents . Then , execute the plan . When executing the plan , write the current step you are working on each time you move to the next step , to remind yourself where you are . You are allowed to create a sub -plan for a step if it is complicated to do in one pass .</p><p>You should continue to pay attention to details in the original task instructions even after you draft your plan . Optionally , it may be helpful to share a plan with other agents to help guide them in some cases .</p><p>Other agents do not know anything about the task being performed , so it is your responsibility to convey any information about the task that is necessary for them to provide helpful responses . You should make this part of your high -level plan . Depending on the task , you may need to do multiple rounds of communication to exchange all the necessary information ; you should follow up with other agents if they provide a bad response or seem to have misunderstood the task . In addition , because other agents can only communicate with you but not each other , you may need to help relay information between agents .</p><p>Because each agent has a different piece of the paper , communication is key for performing tasks that require understanding the full paper . In addition , depending on the responses you receive , you may need to ask follow -up questions , clarify your requests , or engage in additional discussion to fully reason about the task .</p><p>To reduce communication errors , after you send a message you should write a short description of what you expect the response to look like . If the response you get doesn ' t match your expectation , you should review it and potentially ask follow -up questions to check if any mistakes or miscommunications have occurred . It could be the case that an agent ( including yourself ) has misread something or made a logic error .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-TP: Worker system prompt</head><p>You are part of a group that needs to perform tasks that involve a scientific paper . However , the paper is very long , so each agent has only been given part of it . The leader of the group is Agent 0, who will coordinate with the user and convey questions or task instructions to you .</p><p>Sometimes you will need more information in order to understand a question or task or to interpret your portion of the paper ; in these cases , you should send a message to request this information from other agents . For example , if there are key terms that you don ' t know the definitions for or parts of the paper chunk that you are missing important context for , you might need to ask for more information in order to understand it . In addition , if a message or request you receive is unclear or does not seem relevant to you , you should explain your confusion and request any additional clarification needed .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication protocol :</head><p>To send a message to the group leader , write " SEND MESSAGE : " and then your message . Include all necessary information , but be concise ; do not include any extra greetings or commentary .</p><p>To reduce communication errors , after you send a message you should write a short description of what you expect the response to look like . If the response you get doesn 't match your expectation , it is not necessarily wrong , but you should review it and potentially ask follow -up questions to ensure that no mistakes or miscommunications have occurred . Write " Ready " if you have understood the assignment .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-TP: Worker chuk prompt</head><p>You will then receive messages .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARG-TP: Task prompt</head><p>Task : Write a list of feedback comments , similar to the suggestions a reviewer might make . Focus on major comments rather than minor comments ; major comments are important things that affect the overall impact of the paper , whereas minor comments are small things like style / grammar or small details that don ' t matter much for whether the paper should be accepted to a venue .</p><p>Be specific in your suggestions , including details about method or resource names and any particular steps the authors should follow . However , don ' t suggest things that have already been included or addressed in the paper .</p><p>Your review comments should have a clear purpose ; obviously , it is always possible to simply say the authors should include more details or do more experiments , but in practice the authors have limited space to write and limited time to work , so each comment needs to have a clear purpose .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Example multi-agent interaction</head><p>In this section, we show example conversation logs from MARG-S (experiments), MARG-S (clarity), MARG-S (impact), and the refinement stage. Note that for the leader agent, the full output is shown, but only the portion following the SEND MESSAGE marker is actually sent to other agents. Messages consisting of the no-response indicator have been pruned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 MARG-S (experiments)</head><p>Agent 0 (leader): High-Level Plan:</p><p>1. Identify the main goals, contributions, and claims of the Step 1a: Ask each agent to summarize the main goals, contributions, and claims from their section of the paper.</p><p>SEND MESSAGE: Please provide a summary of the main goals, contributions, and claims from your section of the paper. Be as specific as possible.</p><p>Expected response: Each agent should provide a summary of the main goals, contributions, and claims from their section of the paper.</p><p>Agent 1 (worker):  2. The method is based on maximizing flow imbalance between clusters, which is a novel approach.</p><p>3. The paper introduces a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DIGRAC can naturally incorporate node features and does</head><p>not require label supervision.</p><p>5. The paper presents experimental results to demonstrate the effectiveness of the method.</p><p>6. The paper discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications.</p><p>Step 2a: Communicate with Agent 4 (the experiment design expert) to develop a clear description of the experiments, analyses, and ablations that would be necessary to verify the paper's claims. 2. The method is based on maximizing flow imbalance between clusters, which is a novel approach.</p><p>3. The paper introduces a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DIGRAC can naturally incorporate node features and does</head><p>not require label supervision.</p><p>5. The paper presents experimental results to demonstrate the effectiveness of the method.</p><p>6. The paper discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications.</p><p>Expected response: Agent 4 should provide a detailed description of the experiments, analyses, and ablations that would be necessary to verify the paper's claims. its performance when additional label information is available.</p><p>In the conclusion and future work section, they mentioned that further work will include additional experiments in the semi-supervised setting, when there exist seed nodes with known cluster labels, or when additional information is available in the form of must-link and cannot-link constraints.</p><p>They also discussed extending the framework to detect the number of clusters, instead of specifying it a-priori. They also mentioned the need for additional preliminary analysis on how many pairwise imbalance scores to consider. They also discussed the need to build a more powerful framework that can automatically detect the value β used in the current model, to select the subset of influential pairs of imbalances.</p><p>Expected response: Agent 0 should confirm receipt of the message and may ask for additional details or clarifications.</p><p>Agent 4 (expert): I do not believe the request is relevant to me, as I do not have a paper chunk. I will stand by for further instructions.</p><p>Agent 0 (leader): However, there are a few discrepancies:</p><p>1. The paper does not seem to have conducted ablation studies to evaluate the individual contributions of the novel components of their method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA).</p><p>2. The paper does not seem to have tested the method's performance when additional label information is available, despite mentioning that the method can naturally incorporate node features and does not require label supervision.</p><p>3. The paper does not seem to have conducted experiments to evaluate the potential improvements they discussed, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications.</p><p>Step 3c: Discuss any discrepancies with the agents to determine if they constitute shortcomings of the paper.</p><p>SEND MESSAGE: Based on the responses and the expectations, I have identified a few discrepancies:</p><p>1. The paper does not seem to have conducted ablation studies to evaluate the individual contributions of the novel components of their method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA).</p><p>2. The paper does not seem to have tested the method's performance when additional label information is available, despite mentioning that the method can naturally incorporate node features and does not require label supervision. Agent 2 (worker): I agree with the identified discrepancies. The authors could have conducted ablation studies to evaluate the individual contributions of the novel components of their method. This would help to understand the impact of each component on the overall performance.</p><p>Regarding the use of additional label information, the authors could have conducted experiments to evaluate the performance of their method when such information is available. This would provide a more comprehensive evaluation of their method.</p><p>As for the potential improvements discussed in the paper, the authors could have conducted experiments to evaluate these improvements. This would provide evidence to support their claims and demonstrate the potential of their method.</p><p>To address these issues, the authors could conduct additional experiments to evaluate the individual contributions of the novel components of their method, the performance of their method when additional label information is available, and the potential improvements they discussed. They could also provide a more detailed discussion on these aspects in their paper.</p><p>Agent 3 (worker): I agree with the identified discrepancies. The authors could have conducted ablation studies to evaluate the individual contributions of the novel components of their method. This would have provided a clearer understanding of the impact of each component on the overall performance of the method.</p><p>Regarding the second point, the authors did mention that they experimented with adding seed nodes with a supervised loss function and found that it enhanced performance. However, they could have provided more details on these experiments and perhaps conducted additional experiments to evaluate the method's performance when more label information is available.</p><p>As for the potential improvements discussed, it would By addressing these issues, the authors can provide a more comprehensive evaluation of their method and its potential improvements.</p><p>Agent 0 (leader):</p><p>Step 3d: Write feedback comments for any identified shortcomings, explaining the issue and suggesting how the authors could resolve it.</p><p>Based on the discussion with the agents, here are the feedback comments: 2. **Incorporation of Label Information**: The paper mentions that the method can naturally incorporate node features and does not require label supervision. However, it would be beneficial to test the method's performance when additional label information is available. This could involve conducting experiments where a small amount of labeled data is used in addition to the unlabeled data, and the performance of the semi-supervised learning approach is compared with the unsupervised learning approach.</p><p>3. **Potential Improvements**: The paper discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. However, it would be beneficial to conduct experiments to evaluate these improvements. This would provide more concrete evidence of the method's potential and areas for future work.</p><p>These comments should be addressed to provide a more comprehensive evaluation of the method and its potential improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MARG-S (clarity)</head><p>Agent 0 (leader): High-level plan:</p><p>1. Ask each agent (Agent 1, Agent 2, Agent 3) to summarize their part of the paper, focusing on the methods, experimental settings, and key concepts.</p><p>2. Compile the summaries into a coherent explanation of the paper.</p><p>3. Explain the paper to Agent 4 and answer any questions they have. imbalance score with values in [0, 1] for interpretability and ease of comparison.</p><p>2. The directed mixed path aggregation scheme, DIMPA, is a method devised for the implementation of the DIGRAC framework. Unfortunately, my part of the paper does not provide detailed information on how this scheme works or the significance of taking a weighted average of information from neighbors within a certain number of hops.</p><p>3. The Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are mentioned as measures of performance in the paper. However, my part of the paper does not provide detailed information on how these are defined and calculated.</p><p>4. The paper discusses the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix when node attributes are not available. Unfortunately, my part of the paper does not provide detailed information on this process or how the value of K is determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The paper mentions the impact of adding seed nodes with a supervised loss function. However, my part of the paper does not provide detailed information on how these seed nodes are selected or their role in the process.</p><p>6. In the future work section, the paper discusses the potential for DIGRAC to be used in a semi-supervised setting.</p><p>However, my part of the paper does not provide detailed information on how this might be implemented.</p><p>7. The term "sparse regime" is not explicitly defined in my part of the paper. However, in the context of graph theory, a sparse graph is one in which the number of edges is close to the minimal number of edges needed for the graph to be connected. Agent 2 (worker): In response to Agent 4's questions:</p><p>1. The novel probabilistic imbalance loss introduced in the paper is designed to optimize the directed flow imbalance for clustering. It is calculated based on the CI vol_sum from a specific equation mentioned in the paper. This loss function is different from others as it specifically targets the imbalance in the flow of information in the network, which is a unique aspect of directed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The Directed Mixed Path Aggregation (DIMPA) scheme is a method to obtain the probability assignment matrix and feed it to the loss function. It involves building node embeddings by capturing local network information and taking a weighted average of information from neighbors within a certain number of hops. This is significant as it allows the model to consider not just the immediate neighbors of a node, but also nodes that are further away, thus capturing more global information about the network.</p><p>3. The Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are standard metrics used in clustering and classification tasks. The paper does not provide specific details on how these are calculated.</p><p>4. When node attributes are not available, the paper suggests using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. The value of K is not specified in my section of the paper.</p><p>5. The paper does mention the impact of adding seed nodes with a supervised loss function, but does not provide specific details on how these seed nodes are selected or their role in the process.</p><p>6. The paper suggests that DIGRAC could potentially be used in a semi-supervised setting in future work, but does not provide specific details on how this might be implemented.</p><p>7. The term "sparse regime" typically refers to situations where the data or network is sparse, i.e., there are fewer connections or interactions between nodes. The paper suggests that further research could be done in this area, but does not provide specific details.</p><p>8. The paper uses both synthetic and real-world data for experiments. The synthetic data is generated using Directed Stochastic Block Models (DSBM), with various parameters such as the number of clusters, the ratio between the largest and smallest cluster size, and the number of nodes.</p><p>The real-world data includes five datasets: Telegram, Blog, Migration, WikiTalk, and Lead-Lag. The paper does not provide specific details on the hyperparameters, equipment, or material specifications used in the experiments.</p><p>I hope this helps answer Agent 4's questions. If there are any further queries, feel free to ask.</p><p>Expected response: Confirmation from Agent 0 that the answers are satisfactory or further questions from Agent 4.</p><p>Agent 3 (worker): I can provide answers to questions 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The paper uses the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix when node attributes are not available. This is a common technique in spectral graph theory, where the eigenvectors of a matrix associated with a graph (in this case, a Hermitian matrix) are used to extract features that capture the structure of the graph.</p><p>The value of K is not specified in my section of the paper, and it may be determined based on the specific requirements of the application or through a process of experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The paper discusses the impact of adding seed nodes with a supervised loss function. These seed nodes are nodes for which the cluster labels are known a priori, and they are used to guide the clustering process. A supervised loss function is applied to these seed nodes, which helps to improve the performance of the clustering algorithm. The specific method for selecting these seed nodes is not detailed in my section of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The Directed Mixed Path Aggregation (DIMPA) scheme is a method to obtain the probability assignment matrix and feed it to the loss function. It involves building node embeddings by capturing local network information and taking a weighted average of information from neighbors within a certain number of hops. This is significant as it allows the model to consider not just the immediate neighbors of a node, but also nodes that are further away, thus capturing more global information about the network.</p><p>3. The Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are standard metrics used in clustering and classification tasks. The paper does not provide specific details on how these are calculated.</p><p>4. When node attributes are not available, the paper suggests using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. This is a common technique in spectral graph theory, where the eigenvectors of a matrix associated with a graph (in this case, a Hermitian matrix) are used to extract features that capture the structure of the graph. The value of K is not specified in the paper, and it may be determined based on the specific requirements of the application or through a process of experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The paper discusses the impact of adding seed nodes with a supervised loss function. These seed nodes are nodes for which the cluster labels are known a priori, and they are used to guide the clustering process. A supervised loss function is applied to these seed nodes, which helps to improve the performance of the clustering algorithm. The specific method for selecting these seed nodes is not detailed in the paper.</p><p>6. The paper suggests that DIGRAC could potentially be used in a semi-supervised setting in future work, but does not provide specific details on how this might be implemented.</p><p>7. The term "sparse regime" typically refers to situations where the data or network is sparse, i.e., there are fewer connections or interactions between nodes. The paper suggests that further research could be done in this area, but does not provide specific details. You mentioned that the paper does not provide specific details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated. This could be a potential issue as these metrics are crucial for evaluating the performance of the method.</p><p>Could the authors provide more details on how these metrics are calculated?</p><p>4. Regarding the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix, you mentioned that the value of K is not specified in the paper.</p><p>This could be a potential issue as the choice of K could significantly impact the performance of the method. Could the authors provide more guidance on how to choose the value of K?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>You mentioned that the paper does not provide specific details on how seed nodes are selected for the supervised loss function. This could be a potential issue as the selection of seed nodes could significantly impact the performance of the method. Could the authors provide more guidance on how to select these seed nodes?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>You mentioned that the paper does not provide specific details on how DIGRAC could potentially be used Agent 2 (worker): Regarding the Directed Mixed Path Aggregation (DIMPA) scheme, the weighted average of information from neighbors within a certain number of hops is calculated by row-normalizing the adjacency matrix to obtain A s. This is then used to capture local network information.</p><p>The weight assigned to each neighbor is determined by the adjacency matrix and the number of hops. The paper does not provide specific details on the calculation of these weights.</p><p>As for the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix when node attributes are not available, the value of K is not specified in the paper. It may be determined based on the specific requirements of the application or through a process of experimentation.</p><p>Expected response: Confirmation of the information provided or further questions on the DIMPA scheme and the use of eigenvectors.</p><p>Agent 3 (worker): In response to Agent 4's questions:</p><p>1. The CI vol_sum is not explicitly defined in my section of the paper. It seems to be related to the loss function used in the DIGRAC model, but without further context or information, I cannot provide a detailed explanation of how it is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The Directed Mixed Path Aggregation (DIMPA) scheme is not detailed in my section. I would need more information to provide a detailed explanation of how the weighted average of information from neighbors is calculated.</p><p>3. The Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are not explained in my section. These are standard metrics in machine learning and their calculation would be based on standard definitions.</p><p>4. The paper suggests using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix when node attributes are not available. The value of K is not specified in my section of the paper. It may be determined based on the specific requirements of the application or through a process of experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The paper discusses the impact of adding seed nodes with a supervised loss function. These seed nodes are nodes for which the cluster labels are known a priori, and they are used to guide the clustering process. The specific method for selecting these seed nodes is not detailed in my section of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>The paper suggests that DIGRAC could potentially be used in a semi-supervised setting in future work, but does not provide specific details on how this might be implemented in my section. 3. The Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are standard metrics in machine learning and their calculation would be based on standard definitions. The paper does not provide specific details on how these are calculated.</p><p>4. When node attributes are not available, the paper suggests using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. The value of K is not specified in the paper. It may be determined based on the specific requirements of the application or through a process of experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The paper discusses the impact of adding seed nodes with a supervised loss function. These seed nodes are nodes for which the cluster labels are known a priori, and they are used to guide the clustering process. The specific method for selecting these seed nodes is not detailed in the paper.</p><p>6. The paper suggests that DIGRAC could potentially be used in a semi-supervised setting in future work, but does not provide specific details on how this might be implemented. 5. The paper should provide more guidance on how to select seed nodes for the supervised loss function.</p><p>6. The paper should provide more details on how DI-GRAC could potentially be used in a semi-supervised setting.</p><p>7. The paper should provide more details on the hyperparameters, equipment, and material specifications used in the experiments to make it easier for other researchers to reproduce the experiments.</p><p>Step 7: Review and finalize the feedback comments.</p><p>The feedback comments provided above are based on the comments from Agent 4 and are aimed at improving the clarity and reproducibility of the paper. These comments are now finalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 MARG-S (impact)</head><p>Agent 0 (leader): High-level plan:</p><p>1. Ask each agent (1, 2, 3) to summarize their part of the paper, focusing on the motivations, goals, and key findings.</p><p>2. Compile the summaries into a coherent explanation of the paper.</p><p>3. Share the explanation with Agent 4 and ask for their questions and potential shortcomings. Step 3: Share the explanation with Agent 4 and ask for their questions and potential shortcomings. Agent 4 (expert): In response to question 3, "ambient nodes" refer to nodes that are not part of a meaningful cluster in the network.</p><p>They are not part of the main directional signal planted in the graph and are considered as background noise.</p><p>In response to question 6, the self-supervised loss function for clustering and the directed mixed path aggregation scheme to build node embeddings are novel contributions of this paper. The loss function is designed to maximize the imbalance flow in the network, which is a unique approach compared to traditional clustering methods that aim to maximize intra-cluster similarity and inter-cluster dissimilarity. The directed mixed path aggregation scheme is used to capture local network information by taking a weighted average of information from neighbors within a certain number of hops. The paper does not discuss any specific assumptions or limitations associated with these methods.</p><p>EXPECTED RESPONSE: Confirmation from Agent 0 that the answers provided are satisfactory or further questions from Agent 4.</p><p>Agent 3 (worker): I can provide some information regarding questions 2 and 4.</p><p>For question 2, my section of the paper discusses the experimental results of DIGRAC on real-world data sets.</p><p>The paper mentions that the node in-and outdegrees may not be identical across clusters in these data sets. The authors used the largest K eigenvectors of the Hermitian matrix to construct an input feature matrix. They experimented with different loss functions and found that the "sort" variant performed best. However, the specific benchmarks or datasets used for these experiments are not mentioned in my section.</p><p>For question 4, the conclusion of my section men- Agent 0 (leader):</p><p>Step 6: Based on the discussion with Agent 4 and the information from the other agents, write a list of feedback comments.</p><p>Feedback comments:</p><p>1. The imbalance score is a novel contribution of this paper. However, since it's a new measure, it would be beneficial if the authors could compare it with other existing measures in the field. This would help to validate its effectiveness and provide a more comprehensive evaluation of DIGRAC's performance.</p><p>2. The paper does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. This is an important aspect to consider, especially when discussing the potential for large scale industrial applications. The authors should provide this information to give a complete picture of DIGRAC's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>It would be interesting to know how DIGRAC handles "ambient nodes". Are they simply ignored or does the algorithm have a specific way of dealing with them?</p><p>4. The authors should provide more details on how DIGRAC can detect alternative patterns in the data, such as meta-graph structures, and how it can include exogenous information. These are important features that could set DIGRAC apart from existing methods, so they should be explained more thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The authors should discuss any potential limitations or assumptions associated with the self-supervised loss function and the directed mixed path aggregation scheme.</p><p>This would help to identify any potential shortcomings and areas for future improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Refinement Stage</head><p>The comment being refined is as follows:</p><p>"Ablation Studies: The paper would benefit from ablation studies to evaluate the individual contributions of the novel components of the method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). This could involve running experiments where one component is removed or modified at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they con-tribute to the overall performance."</p><p>Agent 0 (leader): High-level plan:</p><p>1. Understand the comment: The comment is about the lack of ablation studies in the paper. Ablation studies are used to understand the contribution of each component of a method by removing or modifying them one at a time.</p><p>The comment suggests that the paper would benefit from such studies, particularly for the novel components like the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA).</p><p>2. Share the comment with other agents: I will share the comment with the other agents and ask them to check their parts of the paper to see if there are any ablation studies mentioned.</p><p>3. Gather information: I will ask the agents to provide information about the novel components mentioned in the comment (probabilistic imbalance loss and DIMPA), their role in the method, and any results or discussions related to them in the paper.</p><p>4. Evaluate the comment: Based on the information gathered, I will evaluate whether the comment is valid and important. If the paper already includes ablation studies or if the novel components are sufficiently explained and justified, the comment may not be necessary.</p><p>5. Refine the comment: If the comment is valid, I will refine it to be more specific and helpful. This could involve suggesting specific experiments for the ablation studies or providing more context about why these studies are important.</p><p>6. Finalize the comment: I will finalize the comment and ensure it meets the criteria of being detailed, helpful, and important.</p><p>Now, let's execute the plan.</p><p>Step 1: Understanding the comment</p><p>The comment is suggesting that the paper lacks ablation studies, particularly for the novel components like the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). These studies could help understand the importance of each component and how they contribute to the overall performance.</p><p>Step 2: Sharing the comment with other agents SEND MESSAGE: We have received a review comment suggesting that the paper lacks ablation studies for the novel components of the method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). Please check your parts of the paper to see if there are any ablation studies mentioned. Also, provide information about these novel components, their role in the method, and any results or discussions related to them. Agent 2 (worker): In my part of the paper, the authors discuss the novel components of the method, namely the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA).</p><p>The probabilistic imbalance loss is used to find a partition that maximizes the imbalance flow, capturing groups of nodes that represent clusters in the meta-graph. The normalization by volumes is carried out to penalize partitions that put most nodes into a single cluster. The authors also discuss different variants of this loss function.</p><p>The DIMPA scheme is used to implement DIGRAC.</p><p>It is a digraph extension of the KernelGCN method and is used to build node embeddings by capturing local network information. The authors describe how they row-normalize the adjacency matrix and add a weighted self-loop to each node. They also discuss how they define two feature mapping functions for source and target embeddings.</p><p>However, there is no explicit mention of ablation studies for these components in my part of the paper. The authors do discuss the efficacy of their method in the experiments section, but they do not isolate the impact of these individual components.</p><p>EXPECTED RESPONSE: Agent 0 should confirm receipt of this information and may provide further instructions or questions. They may also ask other agents for additional information if needed.</p><p>Agent 3 (worker): In my part of the paper, there is no explicit mention of ablation studies for the probabilistic imbalance loss or the directed mixed path aggregation scheme (DIMPA). However, there is a discussion about the performance of different loss functions, including a supervised loss function and a variant called L sort vol_sum. The paper suggests that adding L sort vol_sum to the loss could be beneficial and that the current choice "sort" performs best among these variants. There is no mention of a probabilistic imbalance loss or DIMPA in my section.</p><p>Expected response: Agent 0 should confirm receipt of my message and may provide further instructions or questions based on the information I provided. They may also share information from other agents regarding the ablation studies and the novel components of the method.</p><p>Agent 0 (leader):</p><p>Step 3: Gathering information</p><p>The agents have confirmed that there are no explicit ablation studies mentioned in their parts of the paper. They have also provided detailed information about the novel components of the method, the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA).</p><p>Step 4: Evaluating the comment Based on the information provided by the agents, it seems that the comment is valid. The paper introduces novel components but does not conduct ablation studies to evaluate their individual contributions. This could make it difficult for readers to understand the importance of each component and how they contribute to the overall performance.</p><p>Step 5: Refining the comment Now, I will refine the comment to make it more specific and helpful. I will suggest specific experiments for the ablation studies and provide more context about why these studies are important.</p><p>SEND MESSAGE: The paper introduces a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA) as part of the proposed method. However, there are no ablation studies to evaluate their individual contributions.</p><p>It would be beneficial to conduct experiments where the probabilistic imbalance loss and DIMPA are removed or modified one at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance. For example, an ablation study could involve comparing the performance of the method with and without the probabilistic imbalance loss, or with different variants of this loss function. Similarly, the impact of the DIMPA scheme could be evaluated by comparing the performance with other path aggregation schemes.</p><p>Expected response: The agents should confirm receipt of the refined comment and provide any additional feedback or suggestions.</p><p>Step 6: Finalizing the comment Once the agents confirm the refined comment, I will finalize it.</p><p>The final comment should be detailed, helpful, and important, providing clear guidance for the authors on how to improve their paper. Agent 1 (worker): I confirm receipt of the final message.</p><p>I expect a response that acknowledges my confirmation.</p><p>Agent 2 (worker): I confirm receipt of the final message.</p><p>No further action is required from my side at this time.</p><p>Agent 0 (leader): SEND MESSAGE TO AGENT 3:</p><p>Please confirm receipt of the final message. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our multi-agent architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The survey interface. Participants were asked to rate the specificity, accuracy, and overall helpfulness of each comment, and to rate the overall review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Your paper chunk is shown below : ---START PAPER CHUNK ---{ paper_chunk } ---END PAPER CHUNK ---Information about agents : There are { num_agents } agents in the group , including yourself . You are { agent_name }. The other agent (s) are : { other_agent_names }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Your paper chunk is shown below : ---START PAPER CHUNK ---{ paper_chunk } ---END PAPER CHUNK ---Information about agents : There are { num_agents } agents in the group , including yourself . You are { agent_name }. The other agent (s) are : { other_agent_names }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>each agent to summarize the main goals, contributions, and claims from their section of the paper. b. Compile and filter the responses to create a concise list of the paper's main goals, contributions, and claims. 2. Identify expectations for fulfilling the goals and claims. a. Communicate with Agent 4 (the experiment design expert) to develop a clear description of the experiments, analyses, and ablations that would be necessary to verify the paper's claims. 3. Check whether the paper matches the expectations. a. Ask each agent to describe the evaluations and experiments conducted in their section of the paper. b. Compare the actual experiments with the expected experiments and identify any discrepancies. c. Discuss any discrepancies with the agents to determine if they constitute shortcomings of the paper. d. Write feedback comments for any identified shortcomings, explaining the issue and suggesting how the authors could resolve it. Now, let's execute the plan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Agent 2 (</head><label>2</label><figDesc>worker): The main goal of my section of the paper is to describe the method of Directed Graph Clustering (DIGRAC) and its implementation, as well as the experiments conducted to test its efficacy. The paper introduces a self-supervised loss for clustering and a directed mixed path aggregation scheme (DIMPA) to implement DIGRAC. The method aims to find a partition that maximizes the imbalance flow to capture clusters in the meta-graph. The paper also introduces a global probabilistic imbalance score and discusses different variants for selecting pairs of clusters. The DIMPA scheme is used to build node embeddings and obtain the probability assignment matrix. The paper also discusses the experiments conducted to test the efficacy of DIGRAC, using both synthetic and real-world data sets. The performance of DIGRAC is measured using the Adjusted Rand Index (ARI) and accuracy for node clustering, node classification, and link direction prediction. The paper claims that DIGRAC outperforms its competitors, especially when there exist ambient nodes. EXPECTED RESPONSE: Confirmation of receipt and understanding of the summary, and potentially further instructions or questions. Agent 3 (worker): The main goals of my section of the paper are to present the experimental results of the DIGRAC model and to discuss future work and potential improvements. The experimental results show that the DIGRAC model performs well in node clustering tasks, especially when using the "sort" loss function and when additional label information is available. The future work section discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. The paper also suggests that the model could be improved by considering more pairwise imbalance scores and by using regularization in sparse settings. Expected response: Confirmation from Agent 0 that my summary is accurate and complete, or requests for additional information or clarification. Agent 4 (expert): I do not believe the request is relevant to me, as I do not have a paper chunk. I will stand by for further instructions. Agent 0 (leader): Step 1b: Compile and filter the responses to create a concise list of the paper's main goals, contributions, and claims. Based on the responses from the agents, the main goals, contributions, and claims of the paper are: 1. The paper introduces a new Graph Neural Network (GNN) framework, DIGRAC, for node clustering in directed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>SEND MESSAGE: Agent 4, based on the main goals, contributions, and claims of the paper, could you help us develop a clear description of the experiments, analyses, and ablations that would be necessary to verify the paper's claims? Here are the main points: 1. The paper introduces a new Graph Neural Network (GNN) framework, DIGRAC, for node clustering in directed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Agent 4 (</head><label>4</label><figDesc>expert): Based on the main goals, contributions, and claims of the paper, here are the experiments, analyses, and ablations that would be necessary to verify the paper's claims: 1. **Baseline Comparison**: To validate the effectiveness of the DIGRAC framework, it should be compared with existing state-of-the-art methods for node clustering in directed networks. This comparison should include both traditional methods and other GNN-based methods. The comparison should be done on multiple datasets with varying characteristics to ensure the robustness of the results. 2. **Ablation Study**: To understand the contribution of each component of the DIGRAC framework, an ablation study should be conducted. This involves removing or modifying one component at a time (e.g., the probabilistic imbalance loss, the DIMPA scheme) and observing the impact on the performance. This will help to understand the importance of each component and how they contribute to the overall performance. 3. **Feature Importance Analysis**: Since DIGRAC can naturally incorporate node features, an analysis should be conducted to understand the importance of different features. This can be done by using feature importance methods such as permutation importance or SHAP values. 4. **Scalability Analysis**: To validate the claim that DIGRAC can be adapted for large scale applications, experiments should be conducted on large datasets. The scalability can be evaluated in terms of both computational time and memory usage. 5. **Semi-supervised Learning Experiment**: To validate the potential improvement of semi-supervised learning, experiments should be conducted where a small amount of labeled data is used in addition to the unlabeled data. The performance of the semi-supervised learning approach can be compared with the unsupervised learning approach. 6. **Automatic Cluster Detection Experiment**: To validate the potential improvement of automatic detection of the number of clusters, experiments should be conducted where the number of clusters is not provided as input. The performance of the automatic cluster detection approach can be compared with the approach where the number of clusters is provided. The performance of the methods can be evaluated using standard clustering metrics such as Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and F1 score. The statistical significance of the results should be tested using appropriate statistical tests. Agent 0 (leader): Step 3a: Ask each agent to describe the evaluations and experiments conducted in their section of the paper. SEND MESSAGE: Please describe the evaluations and experiments conducted in your section of the paper. Be as specific as possible. Expected response: Each agent should provide a detailed description of the evaluations and experiments conducted in their section of the paper. Agent 1 (worker): My section of the paper does not contain specific details about the evaluations and experiments conducted. It mainly discusses the problem definition, the proposed DIGRAC method, and the self-supervised loss for clustering. However, it does mention that experimental results on synthetic data and real-world data at different scales demonstrate that the method can achieve state-of-the-art performance for a wide range of network densities and topologies. Expected response: Agent 0 should acknowledge the information provided and may ask for further details or clarification if needed. Agent 2 (worker): In my section of the paper, the authors conduct several experiments to evaluate the effectiveness of their proposed method, DIGRAC. Firstly, they use synthetic data generated from Directed Stochastic Block Models (DSBM) with different parameters and structures. They vary the number of clusters,the ratio between the largest and smallest cluster size, the number of nodes, and the direction flip probability. They also consider different meta-graph structures, including "cycle", "path", "complete", and "star", with or without ambient nodes. Secondly, they test DIGRAC on five real-world digraph datasets: Telegram, Blog, Migration, WikiTalk, and Lead-Lag. The number of clusters and values of β are set differently for each dataset. In terms of performance measures, they use the Adjusted Rand Index (ARI) for node clustering, and accuracy for node classification and link direction prediction. They also introduce normalized imbalance scores to evaluate clustering performance based on flow imbalance. Finally, they compare DIGRAC against 11 recent methods from the literature for clustering digraphs, including InfoMap, DGCN, and three variants of DiGCN. The authors also discuss their training setup and hyperparameter selection process. They use 10% of all nodes from each cluster as test nodes, 10% as validation nodes to select the model, and the remaining 80% as training nodes. Hyperparameters are selected via a greedy search. Agent 3 (worker): In my section of the paper, the authors conducted experiments on real-world data sets to evaluate the performance of the DIGRAC model. They used the largest K eigenvectors of the Hermitian matrix to construct an input feature matrix. They tested the model on a DSBM("cycle", T, n = 1000, K = 5, p = 0.02, ρ = 1) model and compared the test ARI performance using three variants of loss functions on the same digraph. They found that the "sort" choice performed best among these variants. They also experimented on adding seeds, with the seed ratio defined as the ratio of the number of seed nodes to the number of training nodes. They found that seed nodes with a supervised loss function enhance performance, and inferred that the model can further boost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Step 3b: Compare the actual experiments with the expected experiments and identify any discrepancies. Based on the responses from the agents and the expectations from Agent 4, the paper seems to have conducted a thorough set of experiments. They have used both synthetic and real-world datasets, varied parameters, and compared their method with several recent methods. They have also considered different meta-graph structures and used multiple performance measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3.</head><label></label><figDesc>The paper does not seem to have conducted experiments to evaluate the potential improvements they discussed, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. Do you agree that these are shortcomings of the paper? If so, how should the authors address these issues?Expected response: Each agent should provide their opinion on whether the identified discrepancies constitute shortcomings of the paper and suggest how the authors could address these issues.Agent 1 (worker): I agree that these are potential shortcomings of the paper. To address these issues, the authors could conduct additional experiments to evaluate the individual contributions of the novel components of their method. This could involve running ablation studies where they remove or modify one component at a time and observe the impact on the results. They could also test the method's performance when additional label information is available to see if this improves the results. Finally, they could conduct experiments to evaluate the potential improvements they discussed, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications.Expected response: Agent 0 should acknowledge the suggestions and may provide further instructions or ask for additional input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>have been beneficial if the authors had conducted experiments to evaluate these. For instance, they could have conducted experiments in a semi-supervised setting, tested the automatic detection of the number of clusters, and evaluated the method's performance in large scale applications. These experiments would have provided more concrete evidence of the method's potential and areas for future work. Expected response: Agent 0 should confirm receipt of the message and may ask for additional details or clarifications. Agent 4 (expert): I agree with the identified discrepancies and consider them as shortcomings of the paper. Here's how the authors could address these issues: 1. **Ablation Studies**: The authors should conduct ablation studies to evaluate the individual contributions of the novel components of their method. This involves removing or modifying one component at a time (e.g., the probabilistic imbalance loss, the DIMPA scheme) and observing the impact on the performance. This will help to understand the importance of each component and how they contribute to the overall performance. 2. **Incorporation of Label Information**: The authors should test the method's performance when additional label information is available. This can be done by conducting experiments where a small amount of labeled data is used in addition to the unlabeled data. The performance of the semi-supervised learning approach can be compared with the unsupervised learning approach. 3. **Potential Improvements**: The authors should conduct experiments to evaluate the potential improvements they discussed. This includes experiments for semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. The performance of these improved approaches can be compared with the original approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Studies**: The paper would benefit from ablation studies to evaluate the individual contributions of the novel components of the method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). This could involve running experiments where one component is removed or modified at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4.</head><label></label><figDesc>scheme, DIMPA, for the implementation of the framework.The method is tested on synthetic and real-world data, demonstrating superior performance in terms of the Adjusted Rand Index (ARI) and imbalance scores. The paper also discusses the applicability of DIGRAC in various fields such as time series analysis, ranking, and anomaly detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8.</head><label></label><figDesc>My part of the paper does not provide detailed information on the experimental settings, the kind of synthetic and real-world data used, or the hyperparameters, equipment, and material specifications used in the experiments. I hope this helps. If you need more detailed information on any of these points, I would recommend reaching out to the other agents who may have access to different parts of the paper. Expected response: Confirmation from Agent 0 that the message has been received and understood, or follow-up questions for further clarification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>I</head><label></label><figDesc>hope this helps. Please let me know if you need further clarification on these points. Expected response: Confirmation from Agent 0 that my answers have been received and are satisfactory, or follow-up questions for further clarification. Agent 0 (leader): Step 5: Pass the answers back to Agent 4. SEND MESSAGE: Here are the answers to your questions: 1. The novel probabilistic imbalance loss introduced in the paper is a self-supervised loss function inspired by Cucuringu et al. (2020a). It aims to cluster the nodes by maximizing a normalized form of cut imbalance across clusters. The imbalance flows are normalized to obtain an imbalance score with values in [0, 1] for interpretability and ease of comparison. This loss function is designed to optimize the directed flow imbalance for clustering. It is calculated based on the CI vol_sum from a specific equation mentioned in the paper. This loss function is different from others as it specifically targets the imbalance in the flow of information in the network, which is a unique aspect of directed networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>8.</head><label></label><figDesc>The paper uses both synthetic and real-world data for experiments. The synthetic data is generated using Directed Stochastic Block Models (DSBM), with various parameters such as the number of clusters, the ratio between the largest and smallest cluster size, and the number of nodes. The real-world data includes five datasets: Telegram, Blog, Migration, WikiTalk, and Lead-Lag. The paper does not provide specific details on the hyperparameters, equipment, or material specifications used in the experiments. Do you have any further questions or comments about the paper? Expected response: Further questions or comments from Agent 4 about the paper, or confirmation that they are finished. Agent 4 (expert): Thank you for the detailed responses, Agent 0. I have a few follow-up questions and comments: 1. You mentioned that the novel probabilistic imbalance loss is calculated based on the CI vol_sum from a specific equation mentioned in the paper. Could you provide more details about this equation? How is the CI vol_sum calculated? 2. Regarding the Directed Mixed Path Aggregation (DIMPA) scheme, you mentioned that it involves taking a weighted average of information from neighbors within a certain number of hops. How is this weighted average calculated? What factors determine the weight assigned to each neighbor? 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>in a semi-supervised setting. This could be a potential issue as it leaves the reader with unanswered questions about the future direction of the research. Could the authors provide more details on this? 7.You mentioned that the paper does not provide specific details on the hyperparameters, equipment, or material specifications used in the experiments. This could be a potential issue as it makes it difficult for other researchers to reproduce the experiments. Could the authors provide more details on these aspects?These details are crucial for understanding the paper's methods, experimental settings, and key concepts, and for determining whether the paper is well-organized and can be easily understood and reproduced.Agent 0 (leader): Step 4 (Continued): Pass any follow-up questions or comments from Agent 4 to the relevant agent(s) for clarification. SEND MESSAGE: Agent 4 has several follow-up questions and comments about the paper. I would appreciate if you could provide further clarification on the following: 1. Could you provide more details about the equation used to calculate the CI vol_sum for the novel probabilistic imbalance loss? How is the CI vol_sum calculated? 2. Regarding the Directed Mixed Path Aggregation (DIMPA) scheme, how is the weighted average of information from neighbors within a certain number of hops calculated? What factors determine the weight assigned to each neighbor? 3. Agent 4 noted that the paper does not provide specific details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated. Could you provide more details on how these metrics are calculated? 4. Regarding the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix, Agent 4 noted that the value of K is not specified in the paper. Could you provide more guidance on how to choose the value of K? 5. Agent 4 noted that the paper does not provide specific details on how seed nodes are selected for the supervised loss function. Could you provide more guidance on how to select these seed nodes? 6. Agent 4 noted that the paper does not provide spe-cific details on how DIGRAC could potentially be used in a semi-supervised setting. Could you provide more details on this? 7. Agent 4 noted that the paper does not provide specific details on the hyperparameters, equipment, or material specifications used in the experiments. Could you provide more details on these aspects? Expected response: Detailed answers to Agent 4's follow-up questions from Agent 1, Agent 2, and Agent 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>7. The paper uses both synthetic and real-world data for experiments. The specific details on the hyperparameters, equipment, or material specifications used in the experiments are not provided in my section of the paper. I expect the response to contain further questions or clarifications from Agent 0 or Agent 4, or instructions for the next steps. Agent 0 (leader): Step 5 (Continued): Pass the answers back to Agent 4. SEND MESSAGE: Here are the further clarifications to your follow-up questions: 1. The CI vol_sum is related to the loss function used in the DIGRAC model, but the agents do not have further context or information to provide a detailed explanation of how it is calculated. 2. The weighted average of information from neighbors within a certain number of hops in the Directed Mixed Path Aggregation (DIMPA) scheme is calculated by row-normalizing the adjacency matrix to obtain A s. This is then used to capture local network information. The weight assigned to each neighbor is determined by the adjacency matrix and the number of hops. The paper does not provide specific details on the calculation of these weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>7. The paper uses both synthetic and real-world data for experiments. The specific details on the hyperparameters, equipment, or material specifications used in the experiments are not provided in the paper. Do you have any further questions or comments about the paper? Expected response: Further questions or comments from Agent 4 about the paper, or confirmation that they are finished. Agent 4 (expert): Thank you for your responses, Agent 0. I have no further questions at this time. However, I would like to summarize the potential issues and suggestions for the paper: 1. The paper should provide more details about the novel probabilistic imbalance loss, specifically how the CI vol_sum is calculated in the equation mentioned. 2. The paper should provide more details about the Directed Mixed Path Aggregation (DIMPA) scheme, specifically how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor. 3. The paper should provide more details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated. 4. The paper should provide more guidance on how to choose the value of K when using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. 5. The paper should provide more guidance on how to select seed nodes for the supervised loss function. 6. The paper should provide more details on how DI-GRAC could potentially be used in a semi-supervised setting. 7. The paper should provide more details on the hyperparameters, equipment, and material specifications used in the experiments to make it easier for other researchers to reproduce the experiments. These suggestions are aimed at improving the clarity and reproducibility of the paper. I am now done with my review. Agent 0 (leader): Step 6: Write feedback based on any points Agent 4 indicates are in need of improvement. Based on Agent 4's comments, here is the list of feedback comments for the paper: 1. The paper should provide more details about the novel probabilistic imbalance loss, specifically how the CI vol_sum is calculated in the equation mentioned. 2. The paper should provide more details about the Directed Mixed Path Aggregation (DIMPA) scheme, specifically how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor. 3. The paper should provide more details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated. 4. The paper should provide more guidance on how to choose the value of K when using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>4.</head><label></label><figDesc>scheme to build node embeddings. The paper also introduces a new measure, the imbalance score, to evaluate clustering performance based on flow imbalance. The key findings presented in the paper are that DIGRAC, which optimizes directed flow imbalance for clustering without requiring label supervision, attains state-of-the-art results on directed graph clustering for a wide range of noise and sparsity levels and graph structures and topologies. The paper also discusses how DIGRAC complements existing approaches and extends its applicability beyond settings where the input data is a digraph. The experimental results section compares DIGRAC against other methods from the literature and shows that DIGRAC performs well across a range of network densities and noise levels, and is particularly effective when there are ambient nodes present. The conclusion and future work sections outline the current capabilities of DIGRAC, such as creating node embeddings and performing directed clustering, and suggest future improvements, including semi-supervised settings, automatic detection of the number of clusters, and adaptations for large scale industrial applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>SEND MESSAGE: Here is a summary of the paper: [insert summary here]. Do you have any questions or potential shortcomings to point out? Expected response: Questions and potential shortcomings from Agent 4. Agent 4 (expert): I'm sorry, but there seems to be a mistake. The summary of the paper was not included in your message. Could you please provide the summary so I can start my review? Agent 0 (leader): Apologies for the oversight. Let's correct that. SEND MESSAGE: Here is a summary of the paper: The paper introduces a new method for node clustering in directed networks, called DIGRAC. The motivation behind this method is the observation that directionality in networks can reveal latent properties of network flows and can be a vital signal for directed clustering, even in the absence of any edge density differences. The goal of DIGRAC is to recover clusters with strong and imbalanced flow among them. The method uses a self-supervised loss function for clustering and a directed mixed path aggregation scheme to build node embeddings. The paper also introduces a new measure, the imbalance score, to evaluate clustering performance based on flow imbalance. The key findings presented in the paper are that DIGRAC, which optimizes directed flow imbalance for clustering without requiring label supervision, attains state-of-the-art results on directed graph clustering for a wide range of noise and sparsity levels and graph structures and topologies. The paper also discusses how DIGRAC complements existing approaches and extends its applicability beyond settings where the input data is a digraph. The experimental results section compares DIGRAC against other methods from the literature and shows that DIGRAC performs well across a range of network densities and noise levels, and is particularly effective when there are ambient nodes present. The conclusion and future work sections outline the current capabilities of DIGRAC, such as creating node embeddings and performing directed clustering, and suggest future improvements, including semi-supervised settings, automatic detection of the number of clusters, and adaptations for large scale industrial applications. Do you have any questions or potential shortcomings to point out? Expected response: Questions and potential shortcomings from Agent 4. Agent 3 (worker): I have a question regarding the "imbalance score" mentioned in the summary. My part of the paper does not provide a definition or explanation for this term. Could you please provide more information about what the imbalance score is and how it is used in the DIGRAC method? I expect the response to provide a detailed explanation of the imbalance score, including its definition and its role in the DIGRAC method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>6. The lack of information on existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters is a missed opportunity. The authors should provide a comparison with these methods to show how DIGRAC could potentially improve upon them. In summary, while the paper introduces some novel concepts and methods, there are several areas where more information or justification is needed. The authors should provide more details on the imbalance score, computational efficiency, handling of ambient nodes, detection of alternative patterns, inclusion of exogenous information, and potential limitations of the proposed methods. They should also compare DIGRAC with existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Expected response: The agents should provide information about whether there are any ablation studies in their parts of the paper and details about the novel components mentioned in the comment. Agent 1 (worker): In my part of the paper, the authors introduce a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA) as part of their proposed method, DIGRAC. The probabilistic imbalance loss is used in a self-supervised manner to act on the digraph induced by all training nodes. The global imbalance score, one minus whom is the self-supervised loss function, is aggregated from pairwise normalized cut imbalances. The method is end-to-end in combining embedding generation and clustering without an intermediate step. The directed mixed path aggregation scheme (DIMPA) is part of the implementation of the framework. However, there is no explicit mention of ablation studies for these components in my part of the paper. Expected response: Confirmation from Agent 0 about the information provided and possibly further instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Agent 0 (</head><label>0</label><figDesc>schemes. Please confirm receipt of this refined comment and provide any additional feedback or suggestions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>scheme (DIMPA) as part of the proposed method. However, there are no ablation studies to evaluate their individual contributions. It would be beneficial to conduct experiments where the probabilistic imbalance loss and DIMPA are removed or modified one at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance. For example, an ablation study could involve comparing the performance of the method with and without the probabilistic imbalance loss, or with different variants of this loss function. Similarly, the impact of the DIMPA scheme could be evaluated by comparing the performance with other path aggregation schemes." }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Automated evaluation results with recall, precision, and Jaccard values, in addition to the average number of comments generated by each method. The proposed MARG-S method outperforms all baselines in terms of recall, but generates more comments than other baselines and thus has lower precision and Jaccard scores.</figDesc><table><row><cell>Method</cell><cell cols="4">Recall Precision Jaccard # comments</cell></row><row><cell>SARG-B</cell><cell>7.43</cell><cell>1.40</cell><cell>1.25</cell><cell>19.7</cell></row><row><cell>SARG-TP</cell><cell>10.62</cell><cell>4.61</cell><cell>3.46</cell><cell>11.6</cell></row><row><cell>MARG-TP</cell><cell>8.49</cell><cell>5.34</cell><cell>3.52</cell><cell>8.5</cell></row><row><cell>LiZCa</cell><cell>9.67</cell><cell>9.96</cell><cell>5.58</cell><cell>4.0</cell></row><row><cell>MARG-S</cell><cell>15.84</cell><cell>4.41</cell><cell>3.53</cell><cell>19.8</cell></row><row><cell>no refinement</cell><cell>11.92</cell><cell>3.32</cell><cell>2.70</cell><cell>18.3</cell></row><row><cell cols="2">experiments-only 4.36</cell><cell>4.83</cell><cell>2.23</cell><cell>4.1</cell></row><row><cell>clarity-only</cell><cell>3.25</cell><cell>2.65</cell><cell>1.46</cell><cell>6.9</cell></row><row><cell>impact-only</cell><cell>8.88</cell><cell>4.75</cell><cell>3.32</cell><cell>8.8</cell></row><row><cell>Human</cell><cell>9.42</cell><cell>12.00</cell><cell>5.45</cell><cell>4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Example comments generated by each method (SARG-TP and MARG-TP omitted for brevity) for the same</cell></row><row><cell>paper. Qualitatively, we find that MARG-S writes relatively long and specific comments, whereas other methods</cell></row><row><cell>tend to write shorter and more generic comments.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Average number of input and generated tokens</cell></row><row><cell>per paper for each method. This includes tokens used</cell></row><row><cell>for internal discussion in multi-agent methods, but not</cell></row><row><cell>tokens used outside of the method (e.g., for measuring</cell></row><row><cell>the alignment metric). MARG-S generates substantially</cell></row><row><cell>more tokens than other methods, and thus is more ex-</cell></row><row><cell>pensive to run.</cell></row><row><cell>rate groups for different comment types, separate</cell></row><row><cell>groups for the refinement stage, and several agents</cell></row><row><cell>communicating at once in each group), and the</cell></row><row><cell>time needed to generate a review could likely be</cell></row><row><cell>reduced by 2-10x depending on the document size.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Average quality ratings for each method. LiZCa and SARG-B are rated similarly, while MARG-S has over twice the fraction of "good" comments compared to the other two methods.</figDesc><table><row><cell></cell><cell cols="2">Bad MARG-S</cell><cell cols="5">Coef Std. err z-value Pr(&gt;|z|) Comment Ratings by Method Neutral 0.19 0.33 0.58 0.56</cell><cell>Good</cell></row><row><cell>SARG-B</cell><cell cols="3">63.4% Inaccuracy (minor) LiZCa</cell><cell>-0.66 -1.33</cell><cell>0.52 0.35</cell><cell>-1.28 -3.83</cell><cell>26.2% 0.20 10 -3</cell><cell>10.3%</cell></row><row><cell>LiZCa</cell><cell cols="3">68.6% Specificity (specific) Inaccuracy (major)</cell><cell>-5.54 1.90</cell><cell>0.58 0.36</cell><cell>-9.62 5.33</cell><cell>22.9% 10 -21 10 -7</cell><cell>8.6%</cell></row><row><cell>MARG-S</cell><cell></cell><cell>47.4%</cell><cell></cell><cell></cell><cell cols="2">31.2%</cell><cell>21.4%</cell></row><row><cell>0</cell><cell></cell><cell>20</cell><cell></cell><cell>40</cell><cell>60</cell><cell></cell><cell>80</cell><cell>100</cell></row><row><cell>MARG-S LiZCa SARG-B Figure 5: 0</cell><cell cols="2">20 38.3% 42.9% 48.3% Major inaccuracy</cell><cell cols="4">40 Comment Accuracy by Method 60 31.2% 31.4% 26.9% Minor inaccuracy</cell><cell>80</cell><cell>30.5% 25.7% 24.8% Accurate</cell><cell>100</cell></row><row><cell></cell><cell cols="2">Very generic</cell><cell cols="4">Comment Specificity by Method Generic Specific</cell><cell>Very specific</cell></row><row><cell>SARG-B</cell><cell></cell><cell>38.6%</cell><cell></cell><cell cols="2">20.7%</cell><cell cols="2">29.0%</cell><cell>11.7%</cell></row><row><cell>LiZCa</cell><cell></cell><cell>48.6%</cell><cell></cell><cell></cell><cell>11.4%</cell><cell cols="2">28.6%</cell><cell>11.4%</cell></row><row><cell>MARG-S</cell><cell>14.3%</cell><cell>14.9%</cell><cell></cell><cell>31.8%</cell><cell></cell><cell></cell><cell>39.0%</cell></row><row><cell>0</cell><cell></cell><cell>20</cell><cell></cell><cell>40</cell><cell>60</cell><cell></cell><cell>80</cell><cell>100</cell></row></table><note><p><p><p><p><p><p>Figure</p>6</p>: Average accuracy ratings for each method. MARG-S has the most fully accurate comments by a small margin, and SARG-B has the most major inaccuracies, but all methods have similar accuracy distributions overall.</p>Figure</p>7</p>: Average specificity ratings for each method. LiZCa and SARG-B have similar proportions of the specific and very-specific comments, but LiZCa has more very generic comments. MARG-S is extremely specific compared to the other two methods; 71% of MARG-S comments are rated specific or very specific, compared to only 40% for LiZCa and SARG-B.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>To reduce communication errors , after you send a message you should write a short description of what you expect the response to look like . If the response you get doesn 't match your expectation , it is not necessarily wrong , but you should review it and potentially ask follow -up questions to ensure that no mistakes or miscommunications have occurred .</figDesc><table><row><cell>addition ,</cell></row><row><cell>if a message or request you receive is unclear or</cell></row><row><cell>does not seem relevant to you , you should explain</cell></row><row><cell>your confusion and request any additional</cell></row><row><cell>clarification needed .</cell></row><row><cell>Communication protocol :</cell></row><row><cell>To send a message to the group leader , write " SEND</cell></row><row><cell>MESSAGE : " and then your message . Include all</cell></row><row><cell>necessary information , but be concise ; do not</cell></row><row><cell>include any extra greetings or commentary .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The expert is { expert_2 }. a . Come up with a clear description of experiments , analyses , and ablations that you would use to verify the paper ' s claims if you were doing the study yourself . Be specific and detailed in your description ; what experiments should be conducted , how should they be set up , and why are they helpful for verifying the claims ? 3. Check whether the paper matches your expectations a . Go through the actual evaluations and experiments in the paper and identify the similarities and differences between them and your experiment description . Make sure to pay careful attention to details . This will require communication with other agents to collect all the necessary information . If agents do not provide all the needed information or if something is ambiguous , you must send additional messages to resolve the communication issues . b . For each way the paper ' s experiments don ' t match your expectations , determine if this constitutes a shortcoming of the paper , or if the paper ' s experiments still fulfill the goals and claims of the paper . It may be helpful to share your thoughts , the claims , the expected experiments , and the real experiments with other agents and get their opinions on whether the paper ' s experiments fall short . b . If the paper ' s experiments are suboptimal or inadequate , write a feedback comment explaining the shortcoming and what the authors should do to resolve the issue . Be detailed and specific in your feedback to make it clear what the authors should do and why the suggestion is important .</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>OpenAI has recently released a vision-enabled version of GPT-4, but this was not available to us at the time of this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We use gpt-4-0613, which has an 8192-token capacity; larger models have been developed but were not available to us while conducting this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Note that while the relative differences are similar to those reported in<ref type="bibr" target="#b19">Liang et al. (2023)</ref>, our absolute recall scores are lower. We conjecture that this is primarily due to differences in the alignment step; in particular, the pairwise filtering makes our approach more conservative.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>While this is a small number of participants, we note that the number of rated comments is much higher (each participant rates many comments per method), and we obtain statistically significant conclusions from mixed-effect analyses in which we control for participant bias as a random effect.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Of course, it is also possible that the kinds of novel suggestions MARG-S makes only appear useful to authors and actually do not improve the paper in ways that reviewers (or readers) care about. Measuring the extent to which this is the case would require a much more sophisticated study of how these comments affect the long-term impact of papers, and we leave this to future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>OpenAI has not publicly released architecture details; the mixture-of-experts claim was made by an AI researcher on a podcast and is consistent with the speed and cost of the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>We would expect to find slightly fewer bad comments in this analysis than in the user study, as the annotator of this analysis is not as familiar with the papers as their respective authors would be. used in the MiniImagenet benchmark, the training setup, and the behavior of BMG and MG with different data and time budgets. ...</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>When you are done talking with the group leader , tell them that you are done with your review , and give them a summary list of any missing information , poorly justified points , or other suggestions that you identified .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Bryan Pardo for helpful comments. This work was supported in part by NSF grant IIS-2006851 and the Tencent AI Lab Rhino-Bird Gift Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in citation recommendation models survey</title>
		<author>
			<persName><forename type="first">Zafar</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Kefalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahadar</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.113790</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">113790</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zafar Ali, Pavlos Kefalas, Khan Muhammad, Bahadar Ali, and Muhammad Imran. 2020. Deep learning in citation recommendation models survey. Expert Systems with Applications, 162:113790.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding</title>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongchang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhidian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2308.14508</idno>
		<idno>ArXiv:2308.14508</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. LongBench: A Bilingual, Mul- titask Benchmark for Long Context Understanding. ArXiv:2308.14508 [cs].</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Quality Assist: A Technology-Assisted Peer Review Based on Citation Functions to Predict the Paper Quality</title>
		<author>
			<persName><forename type="first">Setio</forename><surname>Basuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Tsuchiya</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2022.3225871</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="126815" to="126831" />
			<date type="published" when="2022">2022</date>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Setio Basuki and Masatoshi Tsuchiya. 2022. The Quality Assist: A Technology-Assisted Peer Review Based on Citation Functions to Predict the Paper Quality. IEEE Access, 10:126815-126831. Confer- ence Name: IEEE Access.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rune</forename><surname>Haubo Bojesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Singmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Scheipl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Grothendieck</surname></persName>
		</author>
		<author>
			<persName><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="http://lme4.r-forge.r-project.org" />
		<title level="m">Package &apos;lme4</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Douglas Bates, Martin Maechler, Ben Bolker, Steven Walker, Rune Haubo Bojesen Christensen, Hen- rik Singmann, Bin Dai, Fabian Scheipl, Gabor Grothendieck, Peter Green, and others. 2009. Pack- age &apos;lme4&apos;. URL http://lme4. r-forge. r-project. org.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Writer-Defined AI Personas for On-Demand Feedback Generation</title>
		<author>
			<persName><forename type="first">Karim</forename><surname>Benharrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Zindulka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Heuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<idno>ArXiv:2309.10433</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, and Daniel Buschek. 2023. Writer- Defined AI Personas for On-Demand Feedback Gen- eration. ArXiv:2309.10433 [cs].</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PEERRec: An AIbased approach to automatically generate recommendations and predict decisions in peer review</title>
		<author>
			<persName><forename type="first">Prabhat</forename><surname>Kumar Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tirthankar</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00799-023-00375-0</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prabhat Kumar Bharti, Tirthankar Ghosal, Mayank Agarwal, and Asif Ekbal. 2023. PEERRec: An AI- based approach to automatically generate recommen- dations and predict decisions in peer review. Interna- tional Journal on Digital Libraries.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Package &apos;ordinal</title>
		<author>
			<persName><forename type="first">Rune</forename><surname>Haubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojesen</forename><surname>Christensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Rune Haubo Bojesen Christensen. 2015. Package &apos;ordi- nal&apos;. Stand, 19(2016). Publisher: Citeseer.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aries: A corpus of scientific paper edits made in response to peer reviews</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Arcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailey</forename><surname>Bransom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><surname>Downey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mike D&apos;Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, and Doug Downey. 2023. Aries: A corpus of scientific paper edits made in response to peer reviews.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving Factuality and Reasoning in Language Models through Multiagent Debate</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.14325</idno>
		<idno>ArXiv:2305.14325</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improv- ing Factuality and Reasoning in Language Models through Multiagent Debate. ArXiv:2305.14325 [cs].</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective practices for developing reading comprehension</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><surname>David Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of education</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2009">2009</date>
			<publisher>Publisher: SAGE Publications Sage CA</publisher>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Nell K Duke and P David Pearson. 2009. Effective prac- tices for developing reading comprehension. Journal of education, 189(1-2):107-122. Publisher: SAGE Publications Sage CA: Los Angeles, CA.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How Discussion Questions Influence Children&apos;s Story Understanding</title>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">G</forename><surname>Fielding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><surname>Others</surname></persName>
		</author>
		<idno>. ERIC Number: ED314724</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note type="raw_reference">Linda G. Fielding and And Others. 1990. How Dis- cussion Questions Influence Children&apos;s Story Under- standing. Technical Report No. 490. Technical report. ERIC Number: ED314724.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frederic Gmeiner and Nur Yildirim. 2023. Dimensions for Designing LLM-based Writing Support</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2Writing Workshop at CHI 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>What can&apos;t large language models do? the future of ai-assisted academic writing. In In2Writing Workshop at CHI 2023</note>
	<note type="raw_reference">Raymond Fok and Daniel S Weld. 2023. What can&apos;t large language models do? the future of ai-assisted academic writing. In In2Writing Workshop at CHI 2023. Frederic Gmeiner and Nur Yildirim. 2023. Dimen- sions for Designing LLM-based Writing Support. In In2Writing Workshop at CHI 2023.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv:2308.00352</idno>
		<title level="m">Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</title>
		<imprint/>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. ArXiv:2308.00352 [cs].</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Long-Text Understanding with Short-Text Models</title>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00547</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient Long-Text Understanding with Short-Text Models. Transactions of the Association for Compu- tational Linguistics, 11:284-299.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Screening for Self-Plagiarism in a Subspecialty-versus</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">U</forename><surname>Kalnins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Halm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castillo</surname></persName>
		</author>
		<idno type="DOI">10.3174/ajnr.A4234</idno>
	</analytic>
	<monogr>
		<title level="m">General Imaging Journal Using iThenticate</title>
		<imprint>
			<publisher>Editorial Perspectives</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1034" to="1038" />
		</imprint>
	</monogr>
	<note>American Journal of Neuroradiology</note>
	<note type="raw_reference">A. U. Kalnins, K. Halm, and M. Castillo. 2015. Screen- ing for Self-Plagiarism in a Subspecialty-versus- General Imaging Journal Using iThenticate. Amer- ican Journal of Neuroradiology, 36(6):1034-1038. Publisher: American Journal of Neuroradiology Sec- tion: Editorial Perspectives.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Artificial intelligence to support publishing and peer review: A summary and review</title>
		<author>
			<persName><forename type="first">Kayvan</forename><surname>Kousha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
		<idno type="DOI">10.1002/leap.1570</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/leap.1570" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">_eprint</note>
	<note type="raw_reference">Kayvan Kousha and Mike Thelwall. 2023. Artificial intelligence to support publishing and peer review: A summary and review. Learned Publishing, n/a(n/a). _eprint: https://onlinelibrary.wiley.com/doi/ pdf/10.1002/leap.1570.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CAMEL: Communicative Agents for</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Kader Hammoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hani</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Khizbullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.17760</idno>
		<idno>ArXiv:2303.17760</idno>
	</analytic>
	<monogr>
		<title level="m">Exploration of Large Scale Language Model Society</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. CAMEL: Communicative Agents for &quot;Mind&quot; Ex- ploration of Large Scale Language Model Society. ArXiv:2303.17760 [cs].</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Can large language models provide useful feedback on research papers? A largescale empirical analysis</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hancheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailas</forename><surname>Vodrahalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno>ArXiv:2310.01783</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel McFarland, and James Zou. 2023. Can large language models provide useful feedback on research papers? A large- scale empirical analysis. ArXiv:2310.01783 [cs].</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>ArXiv:2307.03172</idno>
		<title level="m">Lost in the Middle: How Language Models Use Long Contexts</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv:2307.03172 [cs].</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihar</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.00622</idno>
		<idno>ArXiv:2306.00622</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Ryan Liu and Nihar B. Shah. 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. ArXiv:2306.00622 [cs].</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Writing Tools: Looking Back to Look Ahead</title>
		<author>
			<persName><forename type="first">Cerstin</forename><surname>Mahlow</surname></persName>
		</author>
		<idno>ArXiv:2303.17894</idno>
	</analytic>
	<monogr>
		<title level="m">2Writing Workshop at CHI 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Cerstin Mahlow. 2023. Writing Tools: Looking Back to Look Ahead. In In2Writing Workshop at CHI 2023. ArXiv:2303.17894 [cs].</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatically detect statistical reporting inconsistencies to increase reproducibility of metaanalyses</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michèle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">R</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName><surname>Polanin</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1408</idno>
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="574" to="579" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michèle B. Nuijten and Joshua R. Polanin. 2020. &quot;statcheck&quot;: Automatically detect statistical reporting inconsistencies to increase reproducibility of meta- analyses. Research Synthesis Methods, 11(5):574- 579.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">OpenAI. 2023. Gpt-4 technical report.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A review of cooperative multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Oroojlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davood</forename><surname>Hajinezhad</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-022-04105-y</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13677" to="13722" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Afshin Oroojlooy and Davood Hajinezhad. 2022. A review of cooperative multi-agent deep reinforcement learning. Applied Intelligence, 53(11):13677-13722.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
		<idno>ArXiv:2304.03442</idno>
		<title level="m">Generative Agents: Interactive Simulacra of Human Behavior</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Joon Sung Park, Joseph C. O&apos;Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Sim- ulacra of Human Behavior. ArXiv:2304.03442 [cs].</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The NLP Task Effectiveness of Long-Range Transformers</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.eacl-main.273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 17th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3774" to="3790" />
		</imprint>
	</monogr>
	<note type="raw_reference">Guanghui Qin, Yukun Feng, and Benjamin Van Durme. 2023. The NLP Task Effectiveness of Long-Range Transformers. In Proceedings of the 17th Conference of the European Chapter of the Association for Com- putational Linguistics, pages 3774-3790, Dubrovnik, Croatia. Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Beyond summarization: Designing ai support for realworld expository writing tasks</title>
		<author>
			<persName><forename type="first">Zejiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pao</forename><surname>Siangliulue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hammerbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Chee</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In In2Writing Workshop at CHI 2023</note>
	<note type="raw_reference">Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. 2023. Be- yond summarization: Designing ai support for real- world expository writing tasks. In In2Writing Work- shop at CHI 2023.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">2023a. Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title>
		<author>
			<persName><forename type="first">Qingyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv:2308.15022</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. 2023a. Recursively Summarizing Enables Long- Term Dialogue Memory in Large Language Models. ArXiv:2308.15022 [cs].</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Re-viewRobot: Explainable Paper Review Generation based on Knowledge Synthesis</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06119[cs].ArXiv:2010.06119</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, and Nazneen Fatema Rajani. 2020a. Re- viewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. arXiv:2010.06119 [cs]. ArXiv: 2010.06119.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Linformer: Self-Attention with Linear Complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>ArXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
	<note type="raw_reference">Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Self-Attention with Linear Complexity. ArXiv:2006.04768 [cs, stat].</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">2023b. Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</title>
		<author>
			<persName><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoguang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno>ArXiv:2307.05300</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023b. Unleash- ing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self- Collaboration. ArXiv:2307.05300 [cs].</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Number Preference, Precision and Implicit Confidence</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Begg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew Welsh, Daniel Navarro, and Steve Begg. 2011. Number Preference, Precision and Implicit Confi- dence. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33. Issue: 33.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Recursively Summarizing Books with Human Feedback</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<idno>ArXiv:2109.10862</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Sti- ennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively Summarizing Books with Human Feedback. ArXiv:2109.10862 [cs].</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv:2308.08155</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: En- abling Next-Gen LLM Applications via Multi-Agent Conversation Framework. ArXiv:2308.08155 [cs].</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Norman Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delesley</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Memorizing Transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing Trans- formers.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Retrieval meets Long Context Large Language Models</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Bakhturina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>ArXiv:2310.03025</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
	<note type="raw_reference">Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catan- zaro. 2023. Retrieval meets Long Context Large Language Models. ArXiv:2310.03025 [cs].</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-Training</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First MiniCon Conference</title>
		<meeting>the First MiniCon Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weizhe Yuan and Pengfei Liu. 2022. KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-Training. In Proceedings of the First MiniCon Conference.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms</title>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Başar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60990-0_12</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Reinforcement Learning and Control, Studies in Systems, Decision and Control</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Kyriakos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Vamvoudakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frank</forename><forename type="middle">L</forename><surname>Wan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Derya</forename><surname>Lewis</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cansever</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="321" to="384" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. 2021. Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms. In Kyri- akos G. Vamvoudakis, Yan Wan, Frank L. Lewis, and Derya Cansever, editors, Handbook of Reinforcement Learning and Control, Studies in Systems, Decision and Control, pages 321-384. Springer International Publishing, Cham.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
