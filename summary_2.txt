The first part of the paper presents the motivation and contributions of the implicit MAML (iMAML) algorithm in the context of few-shot learning and meta-learning. The authors introduce iMAML as a solution to overcome computational and memory burdens associated with gradient-based meta-learning, particularly the need to differentiate through the inner loop learning process.

The general approach involves learning meta-parameters in an outer loop while adapting task-specific models in the inner loop with minimal data. Traditional methods like MAML require differentiating through the entire optimization path, which is computationally expensive. iMAML, however, leverages implicit differentiation to compute accurate meta-gradients based only on the solution of the inner level optimization, significantly reducing memory requirements and computational costs.

Theoretical contributions of iMAML include proving that it can compute meta-gradients with minimal memory footprint and without increased computational cost. Experimentally, iMAML shows empirical gains on few-shot image recognition benchmarks. The proximal regularization approach within iMAML ensures that task-specific parameters remain closely dependent on meta-parameters, thus avoiding issues like vanishing gradients.

The second part of the paper focuses on solving the bi-level meta-learning problem using an iterative gradient-based algorithm. The gradient descent update is expanded using the chain rule, involving the computation of meta-gradients through automatic differentiation. However, directly propagating derivatives through the iterative process is computationally impractical. Instead, an implicit differentiation approach via Lemma 1 is employed, decoupling meta-gradient computation from the inner optimizer's path. This method computes the meta-gradient efficiently using the Jacobian derived from the final optimization result.

Algorithm 1 describes the iMAML implementation, which involves sampling tasks, computing task-specific meta-gradients, and updating meta-parameters iteratively. The algorithm starts by sampling a mini-batch of tasks, computing the task meta-gradient using the Implicit-Meta-Gradient function, and updating the meta-parameters using gradient descent or Adam optimizer. Algorithm 2 outlines the steps for implicit meta-gradient computation, which includes obtaining task parameters through an iterative optimization solver, computing partial outer-level gradients, and approximating the meta-gradient using an iterative solver like conjugate gradient (CG).

The theoretical analysis in Section 3.2 examines the impact of approximations on meta-gradient computation, showing that iMAML offers substantial memory savings without increasing computational costs compared to traditional methods. Theorem 1 establishes that iMAML can compute approximate meta-gradients with controlled errors and efficient memory usage, making it suitable for large-scale deep learning applications. Theoretical findings are presented in terms of computational and memory complexities, indicating that iMAML matches the minimax computational complexity of traditional methods but with reduced memory requirements. This is supported by proving that the algorithm's memory requirement is equivalent to the memory needed for Hessian-vector products, which is a small constant factor over the memory required for gradient computations.